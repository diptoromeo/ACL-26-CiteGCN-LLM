# ACL-26-CiteGCN-LLM
**Citation-Aware Research Paper Classification via Graph Convolutional Networks and Large Language Models**

This repository contains the official implementation of **CiteGCN-LLM**, a unified framework for research paper classification and recommendation that jointly models citation structure and textual semantics using Graph Convolutional Networks (GCNs) and Large Language Models (LLMs).

The framework constructs heterogeneous citation graphs and integrates them with transformer-based textual representations to achieve state-of-the-art performance on scholarly datasets.

## ğŸ“Œ **Overview**

Scholarly documents are inherently interconnected through citations, authorship, and shared terminology. Purely text-based models fail to capture these relational signals, while graph-only methods lack deep semantic understanding.

**CiteGCN-LLM bridges this gap by:**

  - Modeling citation topology using GCNs

  - Extracting contextual semantics from abstracts using pretrained LLMs

  - ointly optimizing both branches under a unified multi-label objective

## ğŸ§  **Key Contributions**

- **Two novel heterogeneous citation graphs**

  - RPCG-1: Paperâ€“Wordâ€“Citation graph (TF-IDF + PMI + citations)

  - RPCG-2: Paperâ€“Authorâ€“Citation graph (co-authorship + citations)

- **Unified GCNâ€“LLM architecture**

  - Structural logits from GCN

  - Semantic logits from transformer encoder

  - Joint training with BCEWithLogits loss

- **Top-K TF-IDF based labeling**

  - Interpretable, personalized multi-label supervision

- **Extensive evaluation**

  - arXiv, DBLP, Elsevier, and PubMed

  - Significant gains over GCNs, Graph Transformers, and LLM-only baselines
 

## ğŸ—ï¸ **Framework Architecture**

```text
Paper Abstracts  -->  LLM Encoder  -->  Semantic Logits
        |                                
        v                                
Citation Graph  -->  GCN Layers  -->  Structural Logits
        |
        +---------------- Fusion ----------------> Final Prediction
```

  - GCN captures citation and relational structure

  - LLM captures deep semantic meaning

  - Outputs are combined at the logit level


## ğŸ“Š **Graph Construction**
**RPCG-1: Paperâ€“Wordâ€“Citation Graph**

  - Paperâ€“Word edges weighted by TF-IDF

  - Wordâ€“Word edges weighted by Positive PMI

  - Paperâ€“Paper edges weighted by citation counts

**RPCG-2: Paperâ€“Authorâ€“Citation Graph**

  - Paperâ€“Author membership edges

  - Authorâ€“Author edges via co-authorship (PMI)

  - Paperâ€“Paper citation edges

Both graphs are normalized and processed using standard GCN message passing.

## ğŸ·ï¸ Labeling Strategy

- Extract Top-K globally salient terms from paper titles using TF-IDF

- Assign each paper a multi-hot label vector

- Enables personalized and interpretable classification


## âš™ï¸ Training Objective

CiteGCN-LLM is trained end-to-end with a **joint binary cross-entropy loss**:

![loss](https://latex.codecogs.com/svg.image?\mathcal{L}=\mathcal{L}_{\mathrm{GCN}}+\mathcal{L}_{\mathrm{LLM}})

- Uses BCEWithLogitsLoss

- Supports independent multi-label predictions

- Single backpropagation step updates both branches


## ğŸ“ˆ Experimental Results

- Consistently outperforms:

	- GCN, GAT, GraphSAGE, GIN

	- Graph Transformers (Graph-BERT, Graphormer)

	- LLM-augmented GNN baselines

- Achieves 93%+ accuracy on arXiv and DBLP

- RPCG-2 shows strong gains on socially connected datasets


## ğŸ—‚ï¸ Repository Structure

```text
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ arxiv/
â”‚   â”œâ”€â”€ dblp/
â”‚   â”œâ”€â”€ elsevier/
â”‚   â””â”€â”€ pubmed/
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ build_rpcg1.py
â”‚   â””â”€â”€ build_rpcg2.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gcn.py
â”‚   â”œâ”€â”€ llm_encoder.py
â”‚   â””â”€â”€ citegcn_llm.py
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ utils/
â””â”€â”€ README.md

```

## ğŸ”§ Requirements

- Python â‰¥ 3.9

- PyTorch

- PyTorch Geometric

- HuggingFace Transformers

- NumPy, SciPy, scikit-learn

- NLTK


## ğŸš€ Usage (High-Level)

Preprocess datasets and construct citation graphs

1. Extract LLM embeddings from abstracts

2. Train CiteGCN-LLM jointly on graph and text

3. Evaluate classification accuracy and F1 scores

_(Detailed scripts and configurations are provided in the repository.)_



## Citation

```bibtex
@inproceedings{citegcnllm2025,
  title     = {CiteGCN-LLM: Citation-Aware Research Paper Classification via Graph Convolutional Networks and Large Language Models},
  author    = {Anonymous},
  booktitle = {Proceedings of the ACL},
  year      = {2026}
}
```

