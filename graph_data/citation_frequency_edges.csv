row,column,weight
"Cite_1_1: Rotating industrial machinery underpins manufacturing and infrastructure, yet robust fault diagnosis remains crucial for optimizing efficiency and minimizing downtime. This review comprehensively analyzes the transformative impact of Deep Learning (DL) on fault diagnosis in this critical domain. Drawing insights from eight thematic tables, we unveil key findings, explore future implications, and offer practical recommendations. Our review highlights the significant advantages of DL over traditional methods, showcasing its superior accuracy and versatility across diverse applications. We emphasize the paramount role of data quality, diversity, and representativeness in building robust models, identifying specific data types influencing optimal DL architectures. By delving into the strengths and limitations of common techniques, we provide a valuable roadmap for practitioners to adapt models for specific needs. Finally, we call for collaborative efforts between researchers and practitioners to accelerate the development and application of these game-changing solutions. This comprehensive review promises for further advancements in DL-based fault diagnosis, fostering a future where industrial machinery operates with unprecedented reliability and efficiency.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_2: Recent advancements in sensing, measurement, and computing technologies have significantly expanded the potential for signal-based applications, leveraging the synergy between signal processing and Machine Learning (ML) to improve both performance and reliability. This fusion represents a critical point in the evolution of signal-based systems, highlighting the need to bridge the existing knowledge gap between these two interdisciplinary fields. Despite many attempts in the existing literature to bridge this gap, most are limited to specific applications and focus mainly on feature extraction, often assuming extensive prior knowledge in signal processing. This assumption creates a significant obstacle for a wide range of readers. To address these challenges, this paper takes an integrated article approach. It begins with a detailed tutorial on the fundamentals of signal processing, providing the reader with the necessary background knowledge. Following this, it explores the key stages of a standard signal processing-based ML pipeline, offering an in-depth review of feature extraction techniques, their inherent challenges, and solutions. Differing from existing literature, this work offers an application-independent review and introduces a novel classification taxonomy for feature extraction techniques. Furthermore, it aims at linking theoretical concepts with practical applications, and demonstrates this through two specific use cases: a spectral-based method for condition monitoring of rolling bearings and a wavelet energy analysis for epilepsy detection using EEG signals. In addition to theoretical contributions, this work promotes a collaborative research culture by providing a public repository of relevant Python and MATLAB signal processing codes. This effort is intended to support collaborative research efforts and ensure the reproducibility of the results presented.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_3: Classical Predictive Maintenance (PdM) methods of rolling bearings rely on supervised learning, which involves training a classifier on a labeled dataset to classify the current state of the bearings. In practical situations, labeled data may be limited in terms of its size and type, which can make it difficult to use for training predictive maintenance models. The proposed Similarity-Based PdM (SB-PdM) software addresses the challenge of limited labeled data by approaching the classification task as a similarity measure to a reference sample, rather than using supervised classification. Experimental results have confirmed the effectiveness of the proposed SB-PdM software in achieving very high accuracy, while requiring only moderate computational resources compared to traditional machine learning (ML)-based methods.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_4: Effective management of water distribution networks (WDNs) is critical for conserving water and reducing financial losses. This study addresses the problem of false alarms in WDNs’ acoustic loggers, often triggered by electrical transformer noise. We propose a false alarm prevention framework that features a transformer noise-based frequency selection (TNFS) method, utilizing domain knowledge of the system. TNFS provides a uniform feature set without the iterative sampling or sample dependence typical of other methods. Its rapid processing and low computational needs make it exceptionally suitable for WDNs’ acoustic loggers with constrained computing resources. In addition, we introduce the generative adversarial network-enhanced synthetic minority over-sampling technique (SMOTE), designed to tackle the mode collapse issue in generative adversarial networks (GANs). Our system, validated with real-world data spanning 17 months, dramatically reduces false alarms from electrical noise by 99.6%, highlighting the importance of domain-specific knowledge in the application of machine learning to industrial sensor networks.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_5: Machines operating in a production environment inevitably undergo changes throughout their usage life span, especially due to environmental influences, modifications or wear and tear. This introduces complexities in leveraging machine learning to monitor their condition for maintenance purposes. Depending on the significance of the changes, the performance of the condition monitoring system could degrade drastically. Especially, if changes occur frequently, retraining the system from scratch is not feasible. Therefore, we investigated active learning and self-training as adaptation strategies to enhance the robustness of the condition monitoring process. We evaluated both strategies using a demonstrator consisting of several electric motors and a single vibration sensor. The classification task is to identify, which motors are running based on their superimposed vibration data. We emulated different changes occurring in typical production environments and rated them as small, medium and large scale changes. For a comparative analysis of both strategies, we trained a benchmark model based on a convolutional neural network and evaluated the performance of active learning and self-training for the different changes in relation to this reference model. The results indicate that employing both active learning and self-training in a production environment to adapt the model can enhance its robustness. Self-training is the preferred option for small changes, as it adapts the model without the need for user interaction. For medium and large scale changes, on the other hand, self-training can fail, while active learning is a sensible strategy, despite the operator having to label some of the data.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_6: Vibration-Based Condition Monitoring (VBCM) is commonly utilized in Prognostics and Health Management (PHM) due to its non-destructive nature and inherent advantages over alternative forms of condition monitoring. Furthermore, the rapid evolution of sensor fabrication and the rise of the Internet of Things (IoT) have facilitated large-scale VBCM systems across diverse domains, including industry, transportation, healthcare, agriculture, and wildlife monitoring. The recent advancements in computing technologies have significantly expanded the potential for VBCM by leveraging the synergy between signal processing and Machine Learning (ML). Accordingly, data-driven VBCM has emerged as a paradigm shift, improving the performance and reliability of VBCM systems. To this end, addressing various attributes of data-driven VBCM becomes increasingly important since it represents the core of current and future VBCM systems. The work presented in this thesis addresses the main aspects of VBCM, including signal processing fundamentals, feature extraction, availability of labeled data, computational complexity, and power efficiency. The methods employed in this thesis span the fields of Digital Signal Processing (DSP) and ML techniques (supervised, Deep Learning (DL)), including signal preprocessing, signal denoising, signal frequency-domain analysis, signal time-frequency domain analysis, feature extraction, signal companding (compression-expansion), and 1-dimensional (1D) convolutional reconstruction autoencoders. These methods address extraction of effective condition-related features, limited availability of labeled data, noise removal, complexity considerations in VBCM systems, and power efficiency of power-constrained sensor nodes in remote VBCM. By addressing the aforementioned problems, the end-to-end performance of VBCM systems can be improved in terms of the size of training data, the reliability of the monitoring process, system delay, memory requirements, and power consumption. To ensure the explainability of the extracted features, the developed methods for the extraction of condition-related features are based on signal processing since feature engineering using signal processing creates explainable features that link meaningfully to ii signal conditions or classes compared to DL-based features. The thesis also contributes to the VBCM literature by providing a comprehensive tutorial on signal processing fundamentals, an overview of a typical signal-based ML pipeline, and an application-independent review of feature extraction techniques. The work presented in this thesis presents efficient solutions to the main challenges that face the practical deployment of real-world VBCM systems","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_7: Asenkron motorlar endüstride iş gücünün sağlanması açısından birçok uygulamada kullanılmaktadır. Asenkron motorlarda oluşan arızalar mil yatağı, stator ve rotor bileşenleri ile ilgilidir. Bu bileşenlerden mil yatağı arızaları en çok karşılaşılan problemlerden biridir. Bu arızaların teşhisi için genellikle titreşim sinyalleri kullanılmaktadır. Endüstriyel ortamda çalışan motor ile aynı özelliklerde bir motor bulmak zor olduğundan karşılaştırma yapılarak arızaların tespiti yapılamamaktadır. Bu çalışmada titreşim sinyallerinin zaman frekans görüntüleri oluşabilecek mil yatağı arızaları için toplanarak transfer öğrenme tabanlı bir model ile eğitilmiştir. Daha sonra endüstriyel mil arızası olan bir motordan aynı şartlarda ve benzer bir konumda alınan sinyaller kullanılarak endüstrideki büyük güçlü motordaki arıza belirlenmiştir. Yapılan testler sonucunda endüstrideki motorda oluşan kusurların %95’in üzerinde doğru bir şekilde tespit edildiği ispatlanmıştır.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_8: Having accurate prediction on the health of machines in manufacturing can lead to a profitable organization if the operations and maintenance decisions are appropriately performed. This hinges on making well-informed operational and maintenance decisions. Incorporating condition monitoring and predictive maintenance strategies can significantly contribute to achieving this goal. By continuously monitoring the real-time condition of machines, organizations can gather valuable data that offers insights into the performance and health of the equipment. However, dealing with a scarce dataset, which is common in real world applications, makes any prognostics on the maintenance system intricate. This is further exacerbated by the unavailability of failure data within the system which makes degradation model is best suited for the said situation. Since there is no extensive study discussing computational time under similar settings of two different networks for the degradation model in estimating RUL, this study investigates a simple Long Short-Term Model (LSTM) method for prognostics, which is compared to a two-dimensional Convolutional Neural Network (CNN) under the same training options. The networks are trained using the popular Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset from the National Aeronautics and Space Administration (NASA). The aim of this study is to estimate the remaining useful life (RUL) of a turbofan engine in the most effective way. With carefully designed and defined network architectures, better performance can be attained, enabling proper foreseen of the RUL of an engine as soon as it is more likely to be close to failure. Based on the comparison, it is noted that the simple LSTM method for RUL prediction outperforms the two-dimensional CNN with better RUL prediction, Root Mean Square Error (RMSE), and computational time. For future improvement, this study can be further explored for a more sophisticated hybrid model that might produce better prediction in various sectors such as manufacturing, automotive, and military applications.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
"Cite_1_9: In structural testing, modal analysis of the response data reflects the specifications of a structure. This paper proposes a data-driven anomaly detection method using the universal representation of modal testing response data. High-frequency time series data are preprocessed and transformed into truncated frequency response signals. Subsequently, the universal representation learning of frequency response signals is realized through contrastive learning based on TS2Vec, and distance-based anomaly detection is performed using the K-nearest neighbor algorithm with semi-supervised learning. For a limited excitation experimental dataset consisting of 32 samples, the proposed method achieves a detection rate of 80.0%. This result demonstrates the validity of the universal representation of modal testing response data.","Main_1:   Within smart manufacturing, data driven techniques are commonly adopted for
condition monitoring and fault diagnosis of rotating machinery. Classical
approaches use supervised learning where a classifier is trained on labeled
data to predict or classify different operational states of the machine.
However, in most industrial applications, labeled data is limited in terms of
its size and type. Hence, it cannot serve the training purpose. In this paper,
this problem is tackled by addressing the classification task as a similarity
measure to a reference sample rather than a supervised classification task.
Similarity-based approaches require a limited amount of labeled data and hence,
meet the requirements of real-world industrial applications. Accordingly, the
paper introduces a similarity-based framework for predictive maintenance (PdM)
of rotating machinery. For each operational state of the machine, a reference
vibration signal is generated and labeled according to the machine's
operational condition. Consequentially, statistical time analysis, fast Fourier
transform (FFT), and short-time Fourier transform (STFT) are used to extract
features from the captured vibration signals. For each feature type, three
similarity metrics, namely structural similarity measure (SSM), cosine
similarity, and Euclidean distance are used to measure the similarity between
test signals and reference signals in the feature space. Hence, nine settings
in terms of feature type-similarity measure combinations are evaluated.
Experimental results confirm the effectiveness of similarity-based approaches
in achieving very high accuracy with moderate computational requirements
compared to machine learning (ML)-based methods. Further, the results indicate
that using FFT features with cosine similarity would lead to better performance
compared to the other settings.
",0.45
Cite_2_1: We consider the Kinetic Fokker–Planck (FKP) equation in a domain with Maxwell reflection condition on the boundary. We establish the ultracontractivity of the associated semigroup and the hypocoercivity of the associated operator. We deduce the convergence with the constructive rate of the solution to the KFP equation towards the stationary state with the same mass as the initial datum.,"Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_2: We consider weak solutions of the inhomogeneous non-cutoff Boltzmann equation in a bounded domain with any of the usual physical boundary conditions: in-flow, bounce-back, specular-reflection and diffuse-reflection. When the mass, energy and entropy densities are bounded above, and the mass density is bounded away from a vacuum, we obtain an estimate of the norm of the solution depending on the macroscopic bounds on these hydrodynamic quantities only. This is a regularization effect in the sense that the initial data is not required to be bounded. We present a proof based on variational ideas, which is fundamentally different to the proof that was previously known for the equation in periodic spatial domains.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_3: We establish the boundedness of weak subsolutions for a class of degenerate Kolmogorov equations of hypoelliptic type, compatible with a homogeneous Lie group structure, within bounded product domains using the De Giorgi iteration. We employ the renormalization formula to handle boundary values and provide energy estimates. An L^1-L^p type embedding estimate derived from the fundamental solution is utilized to incorporate lower-order divergence terms. This work naturally extends the boundedness theory for uniformly parabolic equations, with matching exponents for the coefficients.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_4: This work is devoted to the study of the obstacle problem associated to the Kolmogorov–Fokker–Planck operator with rough coefficients through a variational approach. In particular, after the introduction of a proper anisotropic Sobolev space and related properties, we prove the existence and uniqueness of a weak solution for the obstacle problem by adapting a classical perturbation argument to the convex functional associated to the case of our interest. Finally, we conclude this work by providing a one-sided associated variational inequality, alongside with an overview on related open problems.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_5: In this paper, we prove a kinetic Nash type inequality and adapt it to a new functional inequality for functions in a kinetic Sobolev space with absorbing boundary conditions on the half-space. As an application, we address the boundary behavior of the kinetic Fokker-Planck equations in the half-space. Our main result is the sharp regularity of the solution at the absorbing boundary and grazing set.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_6: After carrying out an overview on the non Euclidean geometrical setting suitable for the study of Kolmogorov operators with rough coefficients, we list some properties of the functional space, mirroring the classical theory for uniformly elliptic operators. Then we provide the reader with the proof of a new Sobolev embedding for functions in . Additionally, after reviewing recent results regarding weak regularity theory, we discuss some of their latest applications to real life problems arising both in Physics and in Economics. Finally, we conclude our analysis stating some recent results regarding the study of fractional kinetic operators.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_7: In this work, we provide a comprehensive gradient regularity theory for a broad class of nonlinear kinetic Fokker-Planck equations. We achieve this by establishing precise pointwise estimates in terms of the data in the spirit of nonlinear potential theory, leading to fine gradient regularity results under borderline assumptions on the data. Notably, our gradient estimates are novel already in the absence of forcing terms and even for linear kinetic Fokker-Planck equations in divergence form.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_8: We prove the local-in-time well-posedness of the relativistic Vlasov–Maxwell–Landau system in a bounded domain with the specular reflection condition. Our result covers the case when is a nonconvex domain, e.g., solid torus. To the best of our knowledge, this is the first local well-posedness result for a nonlinear kinetic model with a self-consistent magnetic effect in a three-dimensional bounded domain.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_9: We study the obstacle problem associated with the Kolmogorov operator , which arises from the theory of optimal control in Asian-American options pricing models. Our first main contribution is to improve the known regularity of solutions, from  to . The previous result in the literature, which has been called optimal, corresponds to  regularity with respect to the Kolmogorov distance. This is the expected regularity for solutions to obstacle problems. Our unexpected improvement of regularity in the  variable is obtained using Bernstein's technique and an approach drawing on ideas from Evans-Krylov theory. We then use this improvement in regularity of the solution to prove the first known free boundary regularity result. We show that under a standard thickness condition, the free boundary is a  regular surface. This result constitutes the first step in the program of free boundary regularity. Critically, our arguments rely on a new monotonicity formula and a commutator estimate that are only made possible by the solution's enhanced regularity in .","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_2_10: In this thesis we study some regularity problems for kinetic-type partial differential equations. These equations are characterized by the fact that their second order part is fully degenerate, but the presence of a first order operator restores good properties for the solution. In the first part of the thesis we consider a class of Backward Kolmogorov equations with rough coefficients, namely coefficients that are measurable in time and Hölder continuous in space. We prove optimal regularity for the fundamental solution and Schauder estimates for the Cauchy problem. In the second part we study boundary regularity for a kinetic Fokker-Planck equation with constant coefficients. We also prove a Nash inequality for kinetic Sobolev spaces.","Main_2:   We obtain local Holder continuity estimates up to the boundary for a kinetic
Fokker-Planck equation with rough coefficients, with the prescribed influx
boundary condition. Our result extends some recent developments that
incorporate De Giorgi methods to kinetic Fokker-Planck equations. We also
obtain higher order asymptotic estimates near the incoming part of the
boundary. In particular, when the equation has a zero boundary conditions and
no source term, we prove that the solution vanishes at infinite order on the
incoming part of the boundary.
",0.5
"Cite_4_1: This paper introduces a new fixed effects estimator for linear panel data models with clustered time patterns of unobserved heterogeneity. The method avoids non-convex and combinatorial optimization by combining a preliminary consistent estimator of the slope coefficient, an agglomerative pairwise-differencing clustering of cross-sectional units, and a pooled ordinary least squares regression. Asymptotic guarantees are established in a framework where can grow at any power of , as both and approach infinity. Unlike most existing approaches, the proposed estimator is computationally straightforward and does not require a known upper bound on the number of groups. As existing approaches, this method leads to a consistent estimation of well-separated groups and an estimator of common parameters asymptotically equivalent to the infeasible regression controlling for the true groups. An application revisits the statistical association between income and democracy.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_2: Approximating time-varying unobserved heterogeneity by discrete types has become increasingly popular in economics. Yet, provably valid post-clustering inference for target parameters in models that do not impose an exact group structure is still lacking. This paper fills this gap in the leading case of a linear panel data model with nonseparable two-way unobserved heterogeneity. Building on insights from the double machine learning literature, we propose a simple inference procedure based on a bias-reducing moment. Asymptotic theory and simulations suggest excellent performance. In the application on fiscal policy we revisit, the novel approach yields conclusions in line with economic theory.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_3: Our confidence set quantifies the statistical uncertainty from data-driven group assignments in grouped panel models. It covers the true group memberships jointly for all units with pre-specified probability and is constructed by inverting many simultaneous unit-specific one-sided tests for group membership. We justify our approach under N,T → ∞ asymptotics using tools from high-dimensional statistics, some of which we extend in this paper. We provide Monte Carlo evidence that the confidence set has adequate coverage in finite samples. An empirical application illustrates the use of our confidence set.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_4: Environmental policy is increasingly concerned with measuring emissions resulting from local changes to electricity consumption. These marginal emissions are challenging to measure because electricity grids encompass multiple locations and the information available to identify the effect of each location’s consumption on grid-wide emissions is limited. We formalize this as a high-dimensional aggregation problem: The effect of electricity consumption on emissions can be estimated precisely for each electricity generating unit (generator), but not precisely enough to obtain reliable estimates of marginal emissions by summing these effects across all generators in a grid. We study how two economic constraints can address this problem: electricity supply equals demand and an assumption of monotonicity. We show that these constraints can be used to formulate a ‘naturally regularized’ estimator, which implements an implicit penalization that does not need to be directly tuned. Under an additional assumption of sparsity, we show that our new estimator solves the high-dimensional aggregation problem, i.e., it is consistent for marginal emissions where the usual regression estimator would not be. We also develop an asymptotically valid method for inference to accompany our estimator. When applied to the U.S. electricity grid with 13 separate consumption regions, our method yields plausible patterns of marginal generation across fuel types and geographic location. Our estimates of region-level marginal emissions are precise, account for imports/exports between regions, and allow for all fuel types to potentially be on the margin.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_5: Consider a panel data setting where repeated observations on individuals are available. Often it is reasonable to assume that there exist groups of individuals that share similar effects of observed characteristics, but the grouping is typically unknown in advance. We first conduct a local analysis which reveals that the variances of the individual coefficient estimates contain useful information for the estimation of group structure. We then propose a method to estimate unobserved groupings for general panel data models that explicitly accounts for the variance information. Our proposed method remains computationally feasible with a large number of individuals and/or repeated measurements on each individual. The developed ideas can also be applied even when individual-level data are not available and only parameter estimates together with some quantification of estimation uncertainty are given to the researcher. A thorough simulation study demonstrates superior performance of our method than existing methods and we apply the method to two empirical applications.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_6: This paper provides a unified framework for the selection of valid moment conditions and detection of latent group structures based on the moment condition validity in general nonlinear generalized method of moments (GMM) panel data models. It accommodates a diverging number of moment conditions and group-specific heterogeneous validity of moment conditions across agents. The proposed method integrates the pairwise adaptive fused Lasso and the adaptive Lasso regularization into the GMM estimation. The estimator is shown to be consistent and simultaneously achieves classification and moment selection consistency. The asymptotic distribution of a post-regularization estimator is derived, and its oracle properties are established. The finite-sample performance of the proposed method is evaluated through a Monte Carlo simulation experiment. The method is applied to empirically investigate the impact of agricultural productivity shocks on rural-to-urban migration in China.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_4_7: Latent group structures in panel data models are a new and powerful approach to deal with unobserved heterogeneity in a parsimonious way. This review, with a special focus on grouped structure in unobservable traits, first analyzes the limits and opportunities of Bonhomme and Manresa (2015a)’s Grouped Fixed Effects (GFE) estimator, also discussing the literature it contributed to create. A rich selection of models enhancing clustered heterogeneity at a slope level, starting from Su et al. (2016a), is then presented. A short section investigates how the applied literature has employed in practice the GFE. Finally, the GFE of Bonhomme et al. (2022) is presented in detail together with its limits and advantages.","Main_4:   In this paper, we develop spectral and post-spectral estimators for grouped
panel data models. Both estimators are consistent in the asymptotics where the
number of observations $N$ and the number of time periods $T$ simultaneously
grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent
and asymptotically normal with mean zero under the assumption of well-separated
groups even if $T$ is growing much slower than $N$. The post-spectral estimator
has, therefore, theoretical properties that are comparable to those of the
grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In
contrast to the grouped fixed-effect estimator, however, our post-spectral
estimator is computationally straightforward.
",0.35
"Cite_6_1: This paper studies a class of convex Finite-sum Coupled Compositional Optimization (cFCCO) problems with applications including group distributionally robust optimization (GDRO) and learning with imbalanced data. To better address these problems, we introduce an efficient single-loop primal-dual block-coordinate stochastic algorithm called ALEXR. The algorithm employs block-coordinate stochastic mirror ascent with extrapolation for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we derive lower complexity bounds, demonstrating the (near-)optimality of ALEXR within a broad class of stochastic algorithms for cFCCO. Experimental results on GDRO and partial Area Under the ROC Curve (pAUC) maximization demonstrate the promising performance of our algorithm.","Main_6:   Variance reduction techniques such as SPIDER/SARAH/STORM have been
extensively studied to improve the convergence rates of stochastic non-convex
optimization, which usually maintain and update a sequence of estimators for a
single function across iterations. What if we need to track multiple functional
mappings across iterations but only with access to stochastic samples of
$\mathcal{O}(1)$ functional mappings at each iteration? There is an important
application in solving an emerging family of coupled compositional optimization
problems in the form of $\sum_{i=1}^m f_i(g_i(\mathbf{w}))$, where $g_i$ is
accessible through a stochastic oracle. The key issue is to track and estimate
a sequence of $\mathbf g(\mathbf{w})=(g_1(\mathbf{w}), \ldots,
g_m(\mathbf{w}))$ across iterations, where $\mathbf g(\mathbf{w})$ has $m$
blocks and it is only allowed to probe $\mathcal{O}(1)$ blocks to attain their
stochastic values and Jacobians. To improve the complexity for solving these
problems, we propose a novel stochastic method named Multi-block-Single-probe
Variance Reduced (MSVR) estimator to track the sequence of $\mathbf
g(\mathbf{w})$. It is inspired by STORM but introduces a customized error
correction term to alleviate the noise not only in stochastic samples for the
selected blocks but also in those blocks that are not sampled. With the help of
the MSVR estimator, we develop several algorithms for solving the
aforementioned compositional problems with improved complexities across a
spectrum of settings with non-convex/convex/strongly
convex/Polyak-{\L}ojasiewicz (PL) objectives. Our results improve upon prior
ones in several aspects, including the order of sample complexities and
dependence on the strong convexity parameter. Empirical studies on multi-task
deep AUC maximization demonstrate the better performance of using the new
estimator.
",0.2
"Cite_6_2: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its coupled compositional objective structure, emerges as an important optimization paradigm for addressing a wide range of machine learning problems. In this paper, we focus on a challenging class of non-convex non-smooth FCCO, where the outer functions are non-smooth weakly convex or convex and the inner functions are smooth or weakly convex. Existing state-of-the-art result face two key limitations: (1) a high iteration complexity of O(1/epsilon^6) under the assumption that the stochastic inner functions are Lipschitz continuous in expectation; (2) reliance on vanilla SGD-type updates, which are not suitable for deep learning applications. Our main contributions are two fold: (i) We propose stochastic momentum methods tailored for non-smooth FCCO that come with provable convergence guarantees; (ii) We establish a new state-of-the-art iteration complexity of O(1/epsilon^5). Moreover, we apply our algorithms to multiple inequality constrained non-convex optimization problems involving smooth or weakly convex functional inequality constraints. By optimizing a smoothed hinge penalty based formulation, we achieve a new state-of-the-art complexity of O(1/epsilon^5) for finding an (nearly) epsilon-level KKT solution. Experiments on three tasks demonstrate the effectiveness of the proposed algorithms.","Main_6:   Variance reduction techniques such as SPIDER/SARAH/STORM have been
extensively studied to improve the convergence rates of stochastic non-convex
optimization, which usually maintain and update a sequence of estimators for a
single function across iterations. What if we need to track multiple functional
mappings across iterations but only with access to stochastic samples of
$\mathcal{O}(1)$ functional mappings at each iteration? There is an important
application in solving an emerging family of coupled compositional optimization
problems in the form of $\sum_{i=1}^m f_i(g_i(\mathbf{w}))$, where $g_i$ is
accessible through a stochastic oracle. The key issue is to track and estimate
a sequence of $\mathbf g(\mathbf{w})=(g_1(\mathbf{w}), \ldots,
g_m(\mathbf{w}))$ across iterations, where $\mathbf g(\mathbf{w})$ has $m$
blocks and it is only allowed to probe $\mathcal{O}(1)$ blocks to attain their
stochastic values and Jacobians. To improve the complexity for solving these
problems, we propose a novel stochastic method named Multi-block-Single-probe
Variance Reduced (MSVR) estimator to track the sequence of $\mathbf
g(\mathbf{w})$. It is inspired by STORM but introduces a customized error
correction term to alleviate the noise not only in stochastic samples for the
selected blocks but also in those blocks that are not sampled. With the help of
the MSVR estimator, we develop several algorithms for solving the
aforementioned compositional problems with improved complexities across a
spectrum of settings with non-convex/convex/strongly
convex/Polyak-{\L}ojasiewicz (PL) objectives. Our results improve upon prior
ones in several aspects, including the order of sample complexities and
dependence on the strong convexity parameter. Empirical studies on multi-task
deep AUC maximization demonstrate the better performance of using the new
estimator.
",0.2
"Cite_6_3: This paper explores stochastic multi-level compositional optimization, where the objective function is a composition of multiple smooth functions. Traditional methods for solving this problem suffer from either sub-optimal sample complexities or require huge batch sizes. To address these limitations, we introduce the Stochastic Multi-level Variance Reduction (SMVR) method. In the expectation case, our SMVR method attains the optimal sample complexity of O(1/ϵ3) to find an ϵ-stationary point for non-convex objectives. When the function satisfies convexity or the Polyak-Łojasiewicz (PL) condition, we propose a stage-wise SMVR variant. This variant improves the sample complexity to O(1/ϵ2) for convex functions and O(1/(μϵ)) for functions meeting the μ-PL condition or μ-strong convexity. These complexities match the lower bounds not only in terms of ϵ but also in terms of μ (for PL or strongly convex functions), without relying on large batch sizes in each iteration. Furthermore, in the finite-sum case, we develop the SMVR-FS algorithm, which can achieve a complexity of O(n−−√/ϵ2) for non-convex objectives, O(n−−√/ϵlog(1/ϵ)) for convex functions and O(n−−√/μlog(1/ϵ)) for objectives satisfying the μ-PL condition, where n denotes the number of functions in each level. To make use of adaptive learning rates, we propose the Adaptive SMVR method, which maintains the same complexities while demonstrating faster convergence in practice.","Main_6:   Variance reduction techniques such as SPIDER/SARAH/STORM have been
extensively studied to improve the convergence rates of stochastic non-convex
optimization, which usually maintain and update a sequence of estimators for a
single function across iterations. What if we need to track multiple functional
mappings across iterations but only with access to stochastic samples of
$\mathcal{O}(1)$ functional mappings at each iteration? There is an important
application in solving an emerging family of coupled compositional optimization
problems in the form of $\sum_{i=1}^m f_i(g_i(\mathbf{w}))$, where $g_i$ is
accessible through a stochastic oracle. The key issue is to track and estimate
a sequence of $\mathbf g(\mathbf{w})=(g_1(\mathbf{w}), \ldots,
g_m(\mathbf{w}))$ across iterations, where $\mathbf g(\mathbf{w})$ has $m$
blocks and it is only allowed to probe $\mathcal{O}(1)$ blocks to attain their
stochastic values and Jacobians. To improve the complexity for solving these
problems, we propose a novel stochastic method named Multi-block-Single-probe
Variance Reduced (MSVR) estimator to track the sequence of $\mathbf
g(\mathbf{w})$. It is inspired by STORM but introduces a customized error
correction term to alleviate the noise not only in stochastic samples for the
selected blocks but also in those blocks that are not sampled. With the help of
the MSVR estimator, we develop several algorithms for solving the
aforementioned compositional problems with improved complexities across a
spectrum of settings with non-convex/convex/strongly
convex/Polyak-{\L}ojasiewicz (PL) objectives. Our results improve upon prior
ones in several aspects, including the order of sample complexities and
dependence on the strong convexity parameter. Empirical studies on multi-task
deep AUC maximization demonstrate the better performance of using the new
estimator.
",0.2
"Cite_6_4: In this paper, we introduce principled stochastic algorithms to efficiently optimize Normalized Discounted Cumulative Gain (NDCG) and its top-K variant for deep models. To this end, we first propose novel compositional and bilevel compositional objectives for optimizing NDCG and top-K NDCG, respectively. We then develop two stochastic algorithms to tackle these non-convex objectives, achieving an iteration complexity of  for reaching an-stationary point. Our methods employ moving average estimators to track the crucial inner functions for gradient computation, effectively reducing approximation errors. Besides, we introduce practical strategies such as initial warm-up and stop-gradient techniques to enhance performance in deep learning. Despite the advancements, the iteration complexity of these two algorithms does not meet the optimal for smooth non-convex optimization. To address this issue, we incorporate variance reduction techniques in our framework to more finely estimate the key functions, design new algorithmic mechanisms for solving multiple lower-level problems with parallel speed-up, and propose two types of algorithms. The first type directly tracks these functions with the variance reduced estimators, while the second treats these functions as solutions to minimization problems and employs variance reduced estimators to construct gradient estimators for solving these problems. We manage to establish the optimal complexity for both types of algorithms. It is important to highlight that our algorithmic frameworks are versatile and can optimize a wide spectrum of metrics, including Precision@K/Recall@K, Average Precision (AP), mean Average Precision (mAP), and their top-K variants. We further present efficient stochastic algorithms for optimizing these metrics with convergence guarantees. We conduct comprehensive experiments on multiple ranking tasks to verify the effectiveness of our proposed algorithms, which consistently surpass existing strong baselines.","Main_6:   Variance reduction techniques such as SPIDER/SARAH/STORM have been
extensively studied to improve the convergence rates of stochastic non-convex
optimization, which usually maintain and update a sequence of estimators for a
single function across iterations. What if we need to track multiple functional
mappings across iterations but only with access to stochastic samples of
$\mathcal{O}(1)$ functional mappings at each iteration? There is an important
application in solving an emerging family of coupled compositional optimization
problems in the form of $\sum_{i=1}^m f_i(g_i(\mathbf{w}))$, where $g_i$ is
accessible through a stochastic oracle. The key issue is to track and estimate
a sequence of $\mathbf g(\mathbf{w})=(g_1(\mathbf{w}), \ldots,
g_m(\mathbf{w}))$ across iterations, where $\mathbf g(\mathbf{w})$ has $m$
blocks and it is only allowed to probe $\mathcal{O}(1)$ blocks to attain their
stochastic values and Jacobians. To improve the complexity for solving these
problems, we propose a novel stochastic method named Multi-block-Single-probe
Variance Reduced (MSVR) estimator to track the sequence of $\mathbf
g(\mathbf{w})$. It is inspired by STORM but introduces a customized error
correction term to alleviate the noise not only in stochastic samples for the
selected blocks but also in those blocks that are not sampled. With the help of
the MSVR estimator, we develop several algorithms for solving the
aforementioned compositional problems with improved complexities across a
spectrum of settings with non-convex/convex/strongly
convex/Polyak-{\L}ojasiewicz (PL) objectives. Our results improve upon prior
ones in several aspects, including the order of sample complexities and
dependence on the strong convexity parameter. Empirical studies on multi-task
deep AUC maximization demonstrate the better performance of using the new
estimator.
",0.2
"Cite_7_1: The mutual mapping between microstructures and material properties is fundamental to material design. To enable the inverse design of random porous materials with targeted stress-strain responses, this study introduces a latent space-driven, attention-enhanced deep learning framework. Nearly 20,000 random porous materials were generated through level-cut of Gaussian random fields, and their elastoplastic responses were simulated via finite element analysis. To reduce computational cost, the microstructure was projected into latent low-dimensional representational space using a variational autoencoder (VAE). Furthermore, based on the connotation of time consistency, the task of predicting stress-strain responses was reformulated as a temporal prediction problem, which was then addressed using a sequence transformer model based on the attention mechanism. Finally, a VAE2SeqT model was developed to map microstructure data to sequence-based representations. Additionally, an adaptive weight differential-enhanced loss function was proposed to capture the time-dependent nature of stress-strain curves. In the reverse process, a multi-objective optimization algorithm based on NSGA-III explores the Pareto optimal solutions closest to the target in the latent search domain, which are then mapped back to the original pixel space. The results demonstrate that latent space-driven deep learning frameworks perform well in both forward prediction and inverse design, offering promising pathways for the targeted design of complex materials in different fields.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_2: Multiscale simulations of the mechanical behaviors of composites always have high computational costs. In this study, the concept of an element genome is introduced and incorporated with finite element (FE) simulation to enhance computational efficiency. The element genome can be regarded as a coarse mesh for the finite element method that also exhibits the mechanical characteristics of several fine meshes. This enables the FE simulation to be implemented with high efficiency by using coarse meshes, while the more detailed mechanical responses of the fine meshes are simultaneously computed from the element genome to ensure computational efficiency and accuracy. The database of the element genome is constructed using FE simulation. A machine learning technology is then employed to accurately compute the mechanical behaviors of a coarse mesh comprised of any combination of fine meshes with different mechanical properties. The proposed element genome–based FE simulation is then adopted to simulate the effective moduli of various composites, including a particle-reinforced composite, a fiber-reinforced composite, a 3D woven composite, and a random pixel particle-reinforced composite. The findings reveal that this method can save at least 93% of the computational cost for predicting the effective behaviors of different composites while showing good computational accuracy. Subsequently, a method of stress refinement is proposed to improve the computational accuracy of the stress fields of the composites. The results show that the element genome–based FE simulation with stress refinement can accurately approximate the stress fields of composites.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_3: The influence of the internal structure at micrometer length scales on the deformation of polycrystalline materials can be effectively captured using crystal plasticity finite element methods (CPFEM). However, the complexity and nonlinearity of the deformation equations CPFEM solves demand significant computational power and resources to achieve accurate predictions, limiting its broader application. To address this challenge, we have identified a reduced-order representation of the complex data in order to establish a computationally efficient reduced-order models (ROM) and drastically reduce the computational expense of CPFEM. Specifically, in this work, we developed a parametric, data-driven, and non-intrusive ROM framework for CPFEM using proper orthogonal decomposition (POD) and sparse variational Gaussian process (SVGP) regression for single-crystal microstructures under tensile loading conditions. The developed protocol enables one to compress field into a latent/low-dimensional space described by principal component analysis (PCA) via the singular value decomposition (SVD) algorithm. As a result, the high-dimensional data are reduced to a significantly smaller amount of dimensions with POD bases and POD coefficients. Furthermore, we deployed an ensemble of SVGPs—extended from the classical Gaussian process (GP) regression for scalability and handling big data—in a massively parallel manner to train and predict latent POD coefficients using known POD bases from a set of previously obtained simulations results. Lastly, using the predicted POD coefficients, we reconstructed the full-field results and showed reasonable agreement compared with the true values obtained from running CPFEM. The developed framework is validated with a set of CPFEM simulations of a single embedded void in single-crystal aluminum alloy. While the framework is broadly applicable, this work specifically focuses on single-crystal microstructures, a single load case (e.g., tensile), and a specific void geometry (spherical).","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_4: Design for manufacturing is essential to fully exploit the potential of emerging materials and processing technologies. However, traditional trial-and-error optimisation often exhibits inferior performance in manufacturability-driven problems, particularly when handling complex shapes. Surrogate modelling and optimisation have been widely investigated for efficiently predicting simulation results and enhancing manufacturability. Nevertheless, existing methods are mostly constrained by fixed shape parameterisation schemes, limiting their flexibility and effectiveness. To overcome this limitation, this research develops a non-parametric optimisation framework, validated on a sheet metal forming case study, specifically the blank shape optimisation of an automotive hot-stamped B-pillar. The framework integrates an auto-decoder, serving as a differentiable blank shape generator, a convolutional neural network (CNN)-powered surrogate model for manufacturability evaluation, and an Adam optimiser for automated shape optimisation. The surrogate model predicts thickness distributions from the signed distance fields (SDFs) of blank shapes, which are generated by the auto-decoder from latent vectors; based on the predictions, the optimiser iteratively updates the latent vectors to acquire a blank shape with optimised manufacturability. The proposed framework demonstrates superior performance in terms of the accuracy of thickness distribution prediction, the fidelity of blank shape generation, and the efficiency of blank shape optimisation.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_5: Predicting the damage behavior of 3D woven composites subjected to impact loading has long been a key focus of research. Finite element (FE) simulation is commonly employed to address such issue. However, it demands significant computational resources and is also time-consuming. To facilitate rapid prediction of the damage morphology in 3D angle-interlock woven composites (3DAWCs) under impact loading, this study implements FE simulation methods to generate a dataset of damage morphology and develops a predictor-decoder deep learning model that incorporates attention mechanism. This model is capable of providing accurate, real-time predictions of meso-scale damage and constituent damage. The successful development of this model demonstrates the feasibility of using deep learning frameworks for damage prediction in woven composites. It also offers a potential solution to the challenge of damage prediction for related textile materials and supports real-time material monitoring and structural design optimization.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_6: Finite element analysis (FEA) of microstructural representative volume elements (RVEs) is a key component of modern research in composite materials with randomly distributed inclusions. The advent of machine learning techniques utilizing such FEA results have been instrumental in deriving new insights on microstructure-property relationships of stochastic composites. Training these ML models require a large number of diverse microstructures, but current algorithms face difficulties in generating RVEs with high volume fractions and complex inclusion morphologies. Here, we present a novel algorithm, Mesh Assisted Placement (MEAP), which employs a dynamic mesh grid within the bounding box to track the available space and accelerate the positioning of any custom-shaped inclusions. The inclusion boundaries are generated with splines, and an intersection-counting algorithm is used to prevent overlaps. Inclusion area and distribution were regulated with Gaussian integration and a propagation-based method respectively, to ensure adherence to user inputs. MEAP was assessed and found to give highly random inclusion distributions based on nearest neighbor orientation, Ripley’s K function, and the pair distribution function. It can produce RVEs up to a maximum volume fraction of > 90 %, with computational time up to 4 orders of magnitude faster than existing algorithms. Validation of MEAP was carried out by modelling the microstructures of heterogeneous materials from the literature (particulate metal-matrix composites, fiber-based CFRP and irregular carbides in low alloy steel), subjecting these microstructural models to FEA and comparing the simulated results with previously reported experimental results. In all cases, excellent agreement between the FEA and experimental results were obtained.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_7: Residual stresses, which remain within a component after processing, can deteriorate performance. Accurately determining their full-field distributions is essential for optimizing the structural integrity and longevity. However, the experimental effort required for full-field characterization is impractical. Given these challenges, this work proposes a machine learning (ML) based Residual Stress Generator (RSG) to infer full-field stresses from limited measurements. An extensive dataset was initially constructed by performing numerous process simulations with a diverse parameter set. A ML model based on U-Net architecture was then trained to learn the underlying structure through systematic hyperparameter tuning. Then, the model's ability to generate simulated stresses was evaluated, and it was ultimately tested on actual characterization data to validate its effectiveness. The model's prediction of simulated stresses shows that it achieved excellent predictive accuracy and exhibited a significant degree of generalization, indicating that it successfully learnt the latent structure of residual stress distribution. The RSG's performance in predicting experimentally characterized data highlights the feasibility of the proposed approach in providing a comprehensive understanding of residual stress distributions from limited measurements, thereby significantly reducing experimental efforts.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_8: From characterizing the speed of a thermal system's response to computing natural modes of vibration, eigenvalue analysis is ubiquitous in engineering. In spite of this, eigenvalue problems have received relatively little treatment compared to standard forward and inverse problems in the physics-informed machine learning literature. In particular, neural network discretizations of solutions to eigenvalue problems have seen only a handful of studies. Owing to their nonlinearity, neural network discretizations prevent the conversion of the continuous eigenvalue differential equation into a standard discrete eigenvalue problem. In this setting, eigenvalue analysis requires more specialized techniques. Using a neural network discretization of the eigenfunction, we show that a variational form of the eigenvalue problem called the 'Rayleigh quotient' in tandem with a Gram-Schmidt orthogonalization procedure is a particularly simple and robust approach to find the eigenvalues and their corresponding eigenfunctions. This method is shown to be useful for finding sets of harmonic functions on irregular domains, parametric and nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss the utility of harmonic functions as a spectral basis for approximating solutions to partial differential equations. Through various examples from engineering mechanics, the combination of the Rayleigh quotient objective, Gram-Schmidt procedure, and the neural network discretization of the eigenfunction is shown to offer unique advantages for handling continuous eigenvalue problems.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_9: Accurate analysis of mechanical fields, such as stresses, strains, and deformation fields, is essential in the design and repair of composite materials. Compared to the time-consuming computational micromechanics methods, there is a high demand for developing a fast, efficient and accurate surrogate model to conduct mechanical fields analysis. This paper introduces a CNN-based U-Net model trained on physically meaningful data, mapping the stiffness distribution of unidirectional fibre-reinforced polymer (UD FRP) composite laminae to their corresponding linear and nonlinear stress fields. The model is based on a dataset generated through Discrete Element Method (DEM) simulations, which includes the simulation data from 50 Representative Volume Elements (RVEs) with 60% fibre volume fraction and 50 RVEs with 40% fibre volume fraction for training, and 25 RVEs for each fibre volume fraction (60%, 50%, 40% and 30%) for testing. In comparison to the existing studies, the proposed U-Net model demonstrates superior accuracy in predicting both linear and nonlinear stress fields of laminae, including laminae that have different fibre volume fractions to those in the training data, and without the need for a large training dataset.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_7_10: The increasing adoption of machine learning (ML) in fiber-reinforced polymer (FRP) composite design has led to a reliance on black-box models, which achieve high predictive accuracy but lack interpretability. Python symbolic regression (PySR) offers a solution by deriving explicit equations that reveal the governing mechanics of composite structures. This study focuses on hybrid FRP bolted connections, which are rapidly adopted in the industry but remain insufficiently addressed in academic research. To address this gap, a framework was developed to identify key design parameters and predict damage initiation loads by integrating experimental testing, finite element modeling (FEM), and ML. Feature selection and ML models analyzed the dataset, providing insights that guided PySR in deriving interpretable equations. Hybrid L-joint specimens were fabricated and tested to determine damage initiation loads, with results validating FEM models in ABAQUS. A design of experiments approach structured the dataset, and feature selection identified key factors influencing joint performance. ML models assessed dataset quality, with Huber regression emerging as the best-performing model. Based on insights from feature analysis and ML models, PySR derived a compact, interpretable equation that provided greater accuracy and deeper physical insights than the Huber model. This equation aids hybrid L-joint design by improving the understanding of damage initiation mechanics. Beyond predictive accuracy, the findings highlight the model’s scalability to different bolt sizes, equally spaced row of bolts, and stacking sequences. This study demonstrates the potential of interpretable ML in structural engineering applications, particularly for hybrid composite-metal joints, where transparent models are essential for design optimization and predictive accuracy.","Main_7:   Computational stress analysis is an important step in the design of material
systems. Finite element method (FEM) is a standard approach of performing
stress analysis of complex material systems. A way to accelerate stress
analysis is to replace FEM with a data-driven machine learning based stress
analysis approach. In this study, we consider a fiber-reinforced matrix
composite material system and we use deep learning tools to find an alternative
to the FEM approach for stress field prediction. We first try to predict stress
field maps for composite material systems of fixed number of fibers with
varying spatial configurations. Specifically, we try to find a mapping between
the spatial arrangement of the fibers in the composite material and the
corresponding von Mises stress field. This is achieved by using a convolutional
neural network (CNN), specifically a U-Net architecture, using true stress maps
of systems with same number of fibers as training data. U-Net is a
encoder-decoder network which in this study takes in the composite material
image as an input and outputs the stress field image which is of the same size
as the input image. We perform a robustness analysis by taking different
initializations of the training samples to find the sensitivity of the
prediction accuracy to the small number of training samples. When the number of
fibers in the composite material system is increased for the same volume
fraction, a finer finite element mesh discretization is required to represent
the geometry accurately. This leads to an increase in the computational cost.
Thus, the secondary goal here is to predict the stress field for systems with
larger number of fibers with varying spatial configurations using information
from the true stress maps of relatively cheaper systems of smaller fiber
number.
",0.5
"Cite_8_1: We study the complexity (that is, the weight of the multiplication table) of the elliptic normal bases introduced by Couveignes and Lercier. We give an upper bound on the complexity of these elliptic normal bases, and we analyze the weight of some specific vectors related to the multiplication table of those bases. This analysis leads us to some perspectives on the search for low complexity normal bases from elliptic periods.","Main_8:   We study the complexity of multiplication of two elements in a finite field
extension given by their coordinates in a normal basis. We show how to control
this complexity using the arithmetic and geometry of algebraic curves.
",0.1
"Cite_8_2: Abstract. Let K be a finite field, X and Y two curves over K, and Y → X an unramified abelian cover with Galois group G. Let D be a divisor on X and E its pullback on Y. Under mild conditions the linear space associated with E is a free K[G]-module. We study the algorithmic aspects and applications of these modules.","Main_8:   We study the complexity of multiplication of two elements in a finite field
extension given by their coordinates in a normal basis. We show how to control
this complexity using the arithmetic and geometry of algebraic curves.
",0.1
"Cite_9_1: The localization of UAVs (unmanned aerial vehicles) is crucial for their safe, efficient, and accurate operation. Current developments are mostly based on GNSS/GPS signals for outdoor navigation. However, these technologies are vulnerable under the scenarios where the signals are interrupted or unavailable. This paper presents a GNSS-denied UAV localization technique using aerial and satellite image matching. In our proposed end-to-end network, the CLIP model is utilized to extract and correlate the geolocation of UAV acquired images and an existing satellite map. The image orientation is then incorporated for feature similarity computation to derive the flight heading information. To validate the model performance, an aerial image dataset is collected from the UAV flying at the altitude of 100 meters above the sea level. The evaluation conducted in a 2.23km2 region with the location and heading errors of 39.2m and 15.9◦shows the feasibility of the proposed GNSS-denied UAV localization method. The codes and dataset are available at https://github.com/codebra721/CLIP-UAV-localization.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_2: Vision-based localization techniques are effective UAV localization solutions for GNSS-denied conditions, however they depend on costly, complex, and seasonally variable satellite images or 3D maps, whereas humans can determine location using vector maps. Inspired by human navigation strategies, we propose VecMapLocNet , which uses vector maps to determine UAV 3-DoF poses (latitude, longitude, and yaw) through cross-modal matching. Three key modules are designed to improve the matching between UAV images and vector maps. The UAV feature extraction module is low-latency and adaptable to various flight altitudes, ensuring it is suitable for airborne deployment. The vector map feature extraction module employs a weighted representation of different map elements, ensuring robustness against changes in visual appearance. Inspired by Fourier transforms, the feature matching module for 3-DoF pose estimation is parameter-free, computationally efficient, and invariant to cross-modal differences. To evaluate VecMapLocNet, we introduce a comprehensive dataset that presents challenges, encompassing seven cities worldwide. Through rigorous experimentation, VecMapLocNet has demonstrated competitive performance compared to existing methods in localization accuracy (84.45% Recall@5 m), yaw estimation (88.61% Recall@5°), and computational efficiency (25.23ms latency on onboard device Jetson Orin). Furthermore, we validated VecMapLocNet’s performance in real-world scenarios, with experimental results confirming its generalization ability, achieving a localization error of 16.7 m and an orientation error of 3.1°. ","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_3: Autonomous vehicles, such as Unmanned Aerial Vehicles (UAVs), have the potential to completely reshape various industries such as parcel delivery, agriculture, surveillance, and search-and-rescue missions. Consequently, the demand for safe, cost-effective, and intelligent navigation systems is crucial to ensure reliable performance in complex and dynamic environments. In this study, we propose a novel vision-based UAV navigation method that integrates depthmap estimation with a Vision-Language Model (VLM) for efficient obstacle avoidance and path planning. The system processes RGB images captured by the UAV, transforming them into depth maps using DepthAnythingV2, a powerful zero-shot depth estimator. These depth maps are then analyzed by the VLM, which detects nearby obstacles and plans avoidance maneuvers. We have explored Gemini-flash and GPT-4o model as VLM in our study. A fully connected network integrates the VLM output with the UAV’s relative heading angle to predict the optimal course of action, enabling the UAV to dynamically navigate complex environments toward its target. The system’s effectiveness is validated through simulations in AirSim using Blocks and Downtown West environment. The UAV consistently reaches its destination, avoiding obstacles and achieving a near perfect task completion rate of 0.98. By eliminating the need for costly sensors such as LiDAR and operating without pre-existing maps, our solution provides a cost-efficient, and generalizable approach to real-time UAV navigation, especially in unfamiliar or dynamic settings and highlights emerging trends in autonomous systems research utilizing VLMs.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_4: In recent years, deep learning has been extensively deployed on unmanned aerial vehicles (UAVs), particularly for object detection. As the cornerstone of UAV-based object detection, deep neural networks are susceptible to adversarial attacks, with adversarial patches being a relatively straightforward method to implement. However, current research on adversarial patches, especially those targeting UAV object detection, is limited. This scarcity is notable given the complex and dynamically changing environment inherent in UAV image acquisition, which necessitates the development of more robust adversarial patches to achieve effective attacks. To address the challenge of adversarial attacks in UAV high-altitude reconnaissance, this paper presents a robust adversarial patch generation framework. Firstly, the dataset is reconstructed by considering various environmental factors that UAVs may encounter during image collection, and the influences of reflections and shadows during photography are integrated into patch training. Additionally, a nested optimization method is employed to enhance the continuity of attacks across different altitudes. Experimental results demonstrate that the adversarial patches generated by the proposed method exhibit greater robustness in complex environments and have better transferability among similar models.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_5: Accurate geolocation using Global Navigation Satellite Systems (GNSS) is essential for safe and long-range unmanned aerial vehicles (UAVs) flights. However, GNSS systems are susceptible to blockages, jamming, and spoofing attacks. Localization using onboard cameras and satellite images provides a promising solution for UAVs operating in GNSS-denied environments. In this paper, we developed a novel UAV visual localization system for GNSS-denied situations, both day and night, that integrates image matching, visual odometry (VO), and terrain-weighted constraint optimization. First, an effective map management strategy is designed for satellite image chunking, real-time scheduling, and merging. Then, a 2D–3D geo-registration method, combining Bidirectional Homologous Points Search, is introduced to obtain accurate 3D virtual control points for UAV absolute localization. Lastly, a position estimation and optimization method, integrating the sliding window with terrain weighting constraints, is proposed to control position error accumulation and reduce position drift. Twenty experiments were conducted in typical and complex scenarios to validate our system’s resilience to altitude changes, trajectory variations, and rolling terrain. Our system demonstrated drift-free and viewpoint-robust, maintaining stability even in feature-poor environments and seasonal variations. It does not require loop closure, allowing for re-localization after positioning failures. Additionally, we utilized thermal infrared images to demonstrate the system’s performance in night-time conditions. With a Mean Absolute Error of less than 7 m, it can be a powerful complement to GNSS in the event of GNSS-Denied environments.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_6: In this work, we propose a solution that leverages geospatial data to initialize the monocular visual-inertial navigation system. For Visual-Inertial Navigation Systems (VINS) operating on UAVs, the ability to perform initialization and relocalization in mid-air is essential. However, degenerate motion can cause VINS to lose scale, making traditional initialization algorithms less reliable. To address this issue, we fuse geographic information in the initialization process, and utilize a learning-based feature matching algorithm to associate the information with inertial states. The proposed approach demonstrates adaptability to the degenerate motions of UAVs and significantly surpasses the estimation accuracy of conventional VINS initialization algorithms. Compared to methods that assist initialization by using a laser-range-finder (LRF), the proposed method solely relies on low-cost satellite imagery and elevation information. We evaluate the proposed approach on a large-scale UAV dataset, and compare with existing methods. The results demonstrate the superior effectiveness of the proposed method.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_7: Visual localization plays an essential role in the autonomous flight of Unmanned Aerial Vehicles (UAVs) especially for the Global Navigation Satellite System (GNSS) denied environments. Existing aerial-based visual localization methods mainly focus on eliminating image variance between database map and captured frames. However, these is a lack of public dataset and baseline for method comparisons, which impedes the development of aerial-based visual localization. To address this issue, we construct AerialVL, a large-scale dataset, which is collected using UAV flying at different altitudes, along various routes, and during diverse time periods. AerialVL consists of 11 image sequences covering approximately 70 km of trajectory and includes a reference satellite image database corresponding to the flight area. Leveraging AerialVL, we perform thorough evaluations on various mainstream solutions designed for aerial-based visual localization for the first time. This evaluation encompasses visual place recognition, visual alignment localization and visual odometry, serving as comparison baselines. Furthermore, we present a general aerial-based visual localization framework, which unifies various methods and integrates them into a modular architecture. We note that across all flight trajectories, the proposed framework achieves higher localization accuracy and robustness against the existing methods.","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_9_8: Object detection techniques for autonomous Un-manned Aerial Vehicles (UAV) are built upon Deep Neural Networks (DNN), which are known to be vulnerable to adversarial patch perturbation attacks that lead to object detection evasion. Yet, current adversarial patch generation schemes are not designed for UAV imagery settings. This paper proposes a new robust adversarial patch generation attack against object detection with UAVs. We build adversarial patches considering UAV-specific settings such as the UAV camera perspective, viewing angle, distance, and brightness changes. As a result, built patches can also degrade the accuracy of object detector models implemented with different initializations and architectures. Experiments conducted on the VisDrone dataset have shown the proposal's feasibility, achieving an attack success rate of up to 80% in a white-box setting. In addition, we also transfer the patch against DNN models with different initializations and different architectures, reaching attack success rates of up to 75% and 78%, respectively, in a gray-box setting. ","Main_9:   Precise geolocalization is crucial for unmanned aerial vehicles (UAVs).
However, most current deployed UAVs rely on the global navigation satellite
systems (GNSS) or high precision inertial navigation systems (INS) for
geolocalization. In this paper, we propose to use a lightweight visual-inertial
system with a 2D georeference map to obtain accurate and consecutive geodetic
positions for UAVs. The proposed system firstly integrates a micro inertial
measurement unit (MIMU) and a monocular camera as odometry to consecutively
estimate the navigation states and reconstruct the 3D position of the observed
visual features in the local world frame. To obtain the geolocation, the visual
features tracked by the odometry are further registered to the 2D georeferenced
map. While most conventional methods perform image-level aerial image
registration, we propose to align the reconstructed points to the map points in
the geodetic frame; this helps to filter out the large portion of outliers and
decouples the negative effects from the horizontal angles. The registered
points are then used to relocalize the vehicle in the geodetic frame. Finally,
a pose graph is deployed to fuse the geolocation from the aerial image
registration and the local navigation result from the visual-inertial odometry
(VIO) to achieve consecutive and drift-free geolocalization performance. We
have validated the proposed method by installing the sensors to a UAV body
rigidly and have conducted two flights in different environments with unknown
initials. The results show that the proposed method can achieve less than 4m
position error in flight at 100m high and less than 9m position error in flight
about 300m high.
",0.4
"Cite_10_1: We present a comprehensive survey on dynamics of the motion of particles with noncommutative Poisson structure. We use Souriau’s method of orbit to study this exotic mechanics on the tangent bundle of the configuration space or velocity phase space. We consider Feynman-Dyson’s proof of Maxwell’s equations using Jacobi identity on the velocity phase space. In this review we generalize the Feynman-Dyson’s scheme by incorporating the non-commutativity between various spatial coordinates along with the velocity coordinates. This allows us to study a generalized class of Hamiltonian systems. We explore various dynamical flows associated to the Souriau form associated to this generalized Feynman-Dyson’s scheme. Moreover, using the Souriau form we show that these new classes of generalized systems are volume preserving mechanical systems.","Main_10:   We consider canonical/Weyl-Moyal type noncommutative (NC) spaces with
rectilinear coordinates. Motivated by the analogy of the formalism of the
quantum mechanical harmonic oscillator problem in quantum phase-space with that
of the canonical-type NC 2-D space, and noting that the square of length in the
latter case is analogous to the Hamiltonian in the former case, we arrive at
the conclusion that the length and area are quantized in such an NC space, if
the area is expressed entirely in terms of length. We extend our analysis to
3-D case and formulate a ladder operator approach to the quantization of length
in 3-D space. However, our method does not lend itself to the quantization of
spacetime length in 1+1 and 2+1 Minkowski spacetimes if the noncommutativity
between time and space is considered. If time is taken to commute with spatial
coordinates and the noncommutativity is maintained only among the spatial
coordinates in 2+1 and 3+1 dimensional spacetime, then the quantization of
spatial length is possible in our approach.
",0.15
"Cite_10_2: In this paper, we present a novel approach to quantizing the length in noncommutative spaces with positional-dependent noncommutativity. The method involves constructing ladder operators that change the length not only along a plane but also along the third direction due to a noncommutative parameter that is a combination of canonical/Weyl–Moyal-type and Lie algebraic-type. The primary quantization of length in canonical-type noncommutative space takes place only on a plane, while in the present case, it happens in all three directions. We establish an operator algebra that allows for the raising or lowering of eigenvalues of the operator corresponding to the square of the length. We also attempt to determine how the obtained ladder operators act on different states and work out the eigenvalues of the square of the length operator in terms of eigenvalues corresponding to the ladder operators. We conclude by discussing the results obtained.","Main_10:   We consider canonical/Weyl-Moyal type noncommutative (NC) spaces with
rectilinear coordinates. Motivated by the analogy of the formalism of the
quantum mechanical harmonic oscillator problem in quantum phase-space with that
of the canonical-type NC 2-D space, and noting that the square of length in the
latter case is analogous to the Hamiltonian in the former case, we arrive at
the conclusion that the length and area are quantized in such an NC space, if
the area is expressed entirely in terms of length. We extend our analysis to
3-D case and formulate a ladder operator approach to the quantization of length
in 3-D space. However, our method does not lend itself to the quantization of
spacetime length in 1+1 and 2+1 Minkowski spacetimes if the noncommutativity
between time and space is considered. If time is taken to commute with spatial
coordinates and the noncommutativity is maintained only among the spatial
coordinates in 2+1 and 3+1 dimensional spacetime, then the quantization of
spatial length is possible in our approach.
",0.15
"Cite_10_3: It is more than a century-old concept that the Minkowski spacetime is flat. From the pure geometric point of view, we explicitly address the issue of whether a noncommutative Minkowski spacetime is flat or not. In the framework of the twisted-diffeomorphism approach to noncommutative gravity with canonical type noncommutative (NC) coordinate structure, one important result that we get is that the NC Minkowski spacetime parametrized either with spherical polar coordinates or with parabolic coordinates has nontrivial NC corrections to Riemann curvature tensor, Ricci tensor and curvature scalars. Another crucial result is that the curvature scalars have singular behavior at certain points, and these singularities are not coordinate singularities. The nature of these singularities clearly points towards the idea of high-energetic probes turning into black holes. The absence of any such noncommutative corrections, and thus any such singularities, in the cases of Cartesian coordinates and cylindrical coordinates leads to the conclusion that high-energetic probes do not turn into black holes when the canonical NC structure is considered for these coordinates.","Main_10:   We consider canonical/Weyl-Moyal type noncommutative (NC) spaces with
rectilinear coordinates. Motivated by the analogy of the formalism of the
quantum mechanical harmonic oscillator problem in quantum phase-space with that
of the canonical-type NC 2-D space, and noting that the square of length in the
latter case is analogous to the Hamiltonian in the former case, we arrive at
the conclusion that the length and area are quantized in such an NC space, if
the area is expressed entirely in terms of length. We extend our analysis to
3-D case and formulate a ladder operator approach to the quantization of length
in 3-D space. However, our method does not lend itself to the quantization of
spacetime length in 1+1 and 2+1 Minkowski spacetimes if the noncommutativity
between time and space is considered. If time is taken to commute with spatial
coordinates and the noncommutativity is maintained only among the spatial
coordinates in 2+1 and 3+1 dimensional spacetime, then the quantization of
spatial length is possible in our approach.
",0.15
"Cite_11_1: We study the large-time behavior of an ensemble of entities obeying replicator-like stochastic dynamics with mean-field interactions as a model for a primordial ecology. We prove the propagation-of-chaos property and establish conditions for the strong persistence of the N-replicator system and the existence of invariant distributions for a class of associated McKean–Vlasov dynamics. In particular, our results show that, unlike typical models of neutral ecology, fitness equivalence does not need to be assumed but emerges as a condition for the persistence of the system. Further, neutrality is associated with a unique Dirichlet invariant probability measure. We illustrate our findings with some simple case studies, provide numerical results, and discuss our conclusions in the light of Neutral Theory in ecology.","Main_11: Pathwise uniqueness for multi-dimensional stochastic McKean--Vlasov equation
is established under moderate regularity conditions on the drift and diffusion
coefficients. Both drift and diffusion depend on the marginal measure of the
solution. For pathwise uniqueness, the drift is assumed to be Dini-continuous
in the state variable, while the diffusion must be Lipschitz, continuous in
time and uniformly nondegenerate. The setting is classical McKean--Vlasov, that
is, coefficients of the equation are represented as integrals over the marginal
distributions of the process.
",0.05
"Cite_12_1: Covers for a set of functional dependencies (FDs) are fundamental for many areas of data management, such as integrity maintenance, query optimization, database design, and data cleaning. When declaring integrity constraints, keys enjoy native support in database systems while FDs need to be enforced by triggers or at application level. Consequently, maximizing the use of keys will provide the best support. We propose the new notion of mixed cover for a set of FDs, comprising the set of minimal keys together with a cover for the set of non-key FDs implied by the FD set. We establish sequential and parallel algorithms for computing mixed covers from a given set of FDs, and illustrate that they complement each other in terms of their performance. Even though FD covers are typically smaller in number or size than their corresponding mixed cover, the latter generate orders of magnitude lower overheads during integrity maintenance. We also quantify how mixed covers improve the performance of query, refresh and insert operations on the TPC-H benchmark under different constraint workloads and scaling factors. Finally, we illustrate the growth of performance improvement for optimal over minimal-reduced covers, justifying the significant time required for computing the former.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_2: Covers for a set of functional dependencies (FDs) are fundamental for many areas of data management, such as integrity maintenance, query optimization, database design, and data cleaning. When declaring integrity constraints, keys enjoy native support in database systems while FDs need to be enforced by triggers or at application level. Consequently, maximizing the use of keys will provide the best support. We propose the new notion of mixed cover for a set of FDs, comprising the set of minimal keys together with a cover for the set of non-key FDs implied by the FD set. We establish sequential and parallel algorithms for computing mixed covers from a given set of FDs, and illustrate that they complement each other in terms of their performance. Even though FD covers are typically smaller in number or size than their corresponding mixed cover, the latter generate orders of magnitude lower overheads during integrity maintenance. We also quantify how mixed covers improve the performance of query, refresh and insert operations on the TPC-H benchmark under different constraint workloads.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_3: To safeguard sensitive user data, web developers typically rely on implicit access-control policies, which they implement using access checks and query filters. This ad hoc approach is error-prone as these scattered checks and filters are easy to misplace or misspecify, and the lack of an explicit policy precludes external access-control enforcement. More critically, it is difficult for humans to discern what policy is embedded in application code and what data the application may access -- an issue that worsens as development teams evolve. This paper tackles policy extraction: the task of extracting the access-control policy embedded in an application by summarizing its data queries. An extracted policy, once vetted for errors, can stand alone as a specification for the application's data access, and can be enforced to ensure compliance as code changes over time. We introduce Ote, a policy extractor for Ruby-on-Rails web applications. Ote uses concolic execution to explore execution paths through the application, generating traces of SQL queries and conditions that trigger them. It then merges and simplifies these traces into a final policy that aligns with the observed behaviors. We applied Ote to three real-world applications and compared extracted policies to handwritten ones, revealing several errors in the latter.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_4: Broken object-level authorization (BOLA) vulnerabilities are among the most critical security risks facing database-backed applications. However, there is still a significant gap in our systematic understanding of these vulnerabilities. To bridge this gap, we conducted an in-depth study of 101 real-world BOLA vulnerabilities from opensource applications. Our study revealed the four most common object-level authorization models in database-backed application. The insights gained from our study inspired the development of a new tool called BolaRay. This tool employs a combination of SQL and static analysis to automatically infer the distinct types of object-level authorization models, and subsequently verify whether existing implementations enforce appropriate checks for these models. We evaluated BolaRay using 25 popular database-backed applications, which led to the identification of 193 true vulnerabilities, including 178 vulnerabilities that have never been reported before, at a false positive rate of 21.86%. We reported all newly identified vulnerabilities to the corresponding maintainers. To date, 155 vulnerabilities have been confirmed, with 52 CVE IDs granted.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_5: A FinTech system is a cluster of FinTech applications that intensively interact with databases containing a large quantity of user data. To ensure data consistency, it is a common practice to specify data constraints to validate data at runtime. However, data constraints often evolve according to changes in business requirements. Meanwhile, the developers can hardly keep up with the latest requirements during the development cycle. Such an information barrier increases the communication burden and prevents FinTech applications from being updated in time, impeding the development cycle significantly. In this paper, we present a comprehensive empirical study on data constraints in FinTech systems, investigating how they evolve and affect the development process. Our results show that developers find it hard to update their code timely because no mapping from data constraint changes to code is provided. Inspired by the findings from code updates respecting data constraint changes, we propose DCLINK, a traceability link analysis for linking each data constraint change to target methods demanding the code update in the FinTech application. We extensively evaluate DCLINK upon real-world change cases in Ant Group. The results show that DCLINK can effectively and efficiently localize the target methods.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_6: Data dependency-based query optimization techniques can considerably improve database system performance: we apply three such optimization techniques to five database management systems (DBMSs) and observe throughput improvements between 5 % and 33 %. We address two key challenges to achieve these results: (i) efficiently identifying and extracting relevant dependencies from the data, and (ii) making use of the dependencies through SQL rewrites or as transformation rules in the optimizer. First, the schema does not provide all relevant dependencies. We present a workload-driven dependency discovery approach to find additional dependencies within milliseconds. Second, the throughput improvement of a state-of-the-art DBMS is 13 % using only SQL rewrites, but 20 % when we integrate dependency-based optimization into the optimizer and execution engine, e. g., by employing dependency propagation and subquery handling. Using all relevant dependencies, the runtime of four standard benchmarks improves by up to 10 % compared to using only primary and foreign keys, and up to 22 % compared to not using dependencies. The dependency discovery overhead amortizes after a single workload execution.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_7: After decades of progress, database management systems (DBMSs) are now the backbones of many data applications that we interact with on a daily basis. Yet, with the emergence of new data types and hardware, building and optimizing new data systems remain as difficult as the heyday of relational databases. In this paper, we summarize our work towards automating the building and optimization of data systems. Drawing from our own experience, we further argue that any automation technique must address three aspects: user specification, code generation, and result validation. We conclude by discussing a case study using videos data processing, along with opportunities for future research towards designing data systems that are automatically generated.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_12_8: Evaluating query workload on relational database is an essential task for many developers and researchers, but it is challenging to acquire relational data due to data privacy and confidentiality reasons. Query-aware synthetic data generation for database management system (DBMS) becomes crucial for benchmark testing. In order to ensure data fidelity, the synthetic data has to conform query cardinality constraints as well as properties of the database schema. Unfortunately, prior work for data generation either made simple assumptions about queries and database schema or fail to scale with large query workloads. In this paper, we propose ezGen, a synthetic data generator for web application frameworks. ezGen decomposes complicates queries, especially subqueries, into cardinality constraints as data generator’s input, then generating data using a probability approximation model. ezGen leverages a heuristic rule-based method to translate and decouple query-based cardinality into attribute-based cardinality. In addition, different from prior work, we aim to generate synthetic data for real-world database-backed web application testing by exploiting integrity data constraints extracted from application source code to further ensure the generated data fidelity.","Main_12:   Exploiting the relationships among data is a classical query optimization
technique. As persistent data is increasingly being created and maintained
programmatically, prior work that infers data relationships from data
statistics misses an important opportunity. We present ConstrOpt, the first
tool that identifies data relationships by analyzing database-backed
applications. Once identified, ConstrOpt leverages the constraints to optimize
the application's physical design and query execution. Instead of developing a
fixed set of predefined rewriting rules, ConstrOpt employs an
enumerate-test-verify technique to automatically exploit the discovered data
constraints to improve query execution. Each resulting rewrite is provably
equivalent to the original query. Using 14 real-world web applications, our
experiments show that ConstrOpt can discover numerous data constraints from
code analysis and improve real-world application performance significantly.
",0.4
"Cite_13_1: Graph neural networks (GNNs), as topology/structure-aware models within deep learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By directly operating on molecular graphs, GNNs offer an intuitive and expressive framework for learning the complex topological and geometric features of drug-like molecules, cementing their role in modern molecular modeling. This review provides a comprehensive overview of the methodological foundations and representative applications of GNNs in drug discovery, spanning tasks such as molecular property prediction, virtual screening, molecular generation, biomedical knowledge graph construction, and synthesis planning. Particular attention is given to recent methodological advances, including geometric GNNs, interpretable models, uncertainty quantification, scalable graph architectures, and graph generative frameworks. We also discuss how these models integrate with modern deep learning approaches, such as self-supervised learning, multi-task learning, meta-learning and pre-training. Throughout this review, we highlight the practical challenges and methodological bottlenecks encountered when applying GNNs to real-world drug discovery pipelines, and conclude with a discussion on future directions.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_2: Artificial intelligence, particularly language models (LMs), is reshaping research paradigms across scientific domains. In the fields of chemistry and pharmacy, chemical language models (CLMs) have achieved remarkable success in two-dimensional (2D) molecular modeling tasks by leveraging one-dimensional (1D) representations of molecules, such as SMILES and SELFIES. However, extending these successes to three-dimensional (3D) molecular modeling remains a significant challenge, largely due to the absence of effective 1D representations for capturing 3D molecular structures. To address this gap, we introduce ConfSeq, a novel molecular conformation description language that integrates SMILES with internal coordinates including dihedral angles, bond angles, and pseudo-chirality. This design naturally ensures SE(3) invariance, while preserving the human readability and conciseness characteristic of SMILES. ConfSeq enables the reformulation of a range of 3D molecular modeling tasks, such as molecular conformation prediction, 3D molecular generation, and 3D molecular representation, into sequence modeling problems. Then, by simply employing a standard Transformer architecture, we achieve state-of-the-art performance on various benchmark sets. Furthermore, compared to widely used diffusion-based approaches in 3D molecular modeling, the ConfSeq-based method offers unique advantages in inference efficiency, generation controllability, and enables scoring of generated molecules. We believe that ConfSeq can serve as a foundational tool, advancing the development of sequence-based 3D molecular modeling methods.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_3: Three-dimensional molecular generation is critical in drug design. However, current methods often rely on point clouds or oversimplified interaction models, limiting their ability to accurately represent molecular structures. To address these challenges, this paper proposes the multiscale graph equivariant diffusion model for 3D molecule design (MD3MD). MD3MD partitions molecular conformations into multiscale graphs, assigning different weights to capture atomic interactions across scales. This framework guides the diffusion process, enabling high-quality 3D molecular generation. Experimental results demonstrate that MD3MD excels in both unconditional and conditional generation tasks, producing diverse, stable, and innovative molecules that meet specified conditions. Visualization highlights MD3MD’s ability to learn domain-specific patterns and generate molecules distinct from existing datasets while maintaining distributional consistency. By effectively exploring chemical space, MD3MD surpasses previous methods in generating innovative and chemically diverse molecules, offering a notable advancement in the field of molecular design.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_4: Molecular pretrained representations (MPR) has emerged as a powerful approach for addressing the challenge of limited supervised data in applications such as drug discovery and material design. While early MPR methods relied on 1D sequences and 2D graphs, recent advancements have incorporated 3D conformational information to capture rich atomic interactions. However, these prior models treat molecules merely as discrete atom sets, overlooking the space surrounding them. We argue from a physical perspective that only modeling these discrete points is insufficient. We first present a simple yet insightful observation: naively adding randomly sampled virtual points beyond atoms can surprisingly enhance MPR performance. In light of this, we propose a principled framework that incorporates the entire 3D space spanned by molecules. We implement the framework via a novel Transformer-based architecture, dubbed SpaceFormer, with three key components: (1) grid-based space discretization; (2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive experiments show that SpaceFormer significantly outperforms previous 3D MPR models across various downstream tasks with limited data, validating the benefit of leveraging the additional 3D space beyond atoms in MPR models.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_5: Unified generation of sequence and structure for scientific data (e.g., materials, molecules, proteins) is a critical task. Existing approaches primarily rely on either autoregressive sequence models or diffusion models, each offering distinct advantages and facing notable limitations. Autoregressive models, such as GPT, Llama, and Phi-4, have demonstrated remarkable success in natural language generation and have been extended to multimodal tasks (e.g., image, video, and audio) using advanced encoders like VQ-VAE to represent complex modalities as discrete sequences. However, their direct application to scientific domains is challenging due to the high precision requirements and the diverse nature of scientific data. On the other hand, diffusion models excel at generating high-dimensional scientific data, such as protein, molecule, and material structures, with remarkable accuracy. Yet, their inability to effectively model sequences limits their potential as general-purpose multimodal foundation models. To address these challenges, we propose UniGenX, a unified framework that combines autoregressive next-token prediction with conditional diffusion models. This integration leverages the strengths of autoregressive models to ease the training of conditional diffusion models, while diffusion-based generative heads enhance the precision of autoregressive predictions. We validate the effectiveness of UniGenX on material and small molecule generation tasks, achieving a significant leap in state-of-the-art performance for material crystal structure prediction and establishing new state-of-the-art results for small molecule structure prediction, de novo design, and conditional generation. Notably, UniGenX demonstrates significant improvements, especially in handling long sequences for complex structures, showcasing its efficacy as a versatile tool for scientific data generation.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_6: Recent advances in diffusion-based methods have shown promising results for molecular conformer generation, yet their performance remains constrained by training data scarcity---particularly for structurally complex molecules. In this work, we present Fragment-Augmented Diffusion (FragDiff), a data-centric augmentation strategy that incorporates chemical fragmentation techniques into the pre-training phase of modern diffusion-based generative models. Our key innovation lies in decomposing molecules into chemically meaningful fragments that serve as building blocks for systematic data augmentation, enabling the diffusion model to learn enhanced local geometry while maintaining global molecular topology. Unlike existing approaches that focus on complex architectural modifications, FragDiff adopts a data-centric paradigm orthogonal to model design. Comprehensive benchmarks show FragDiff's superior performance, especially in data-scarce scenarios. Notably, it achieves 12.2--13.4% performance improvement on molecules 3x beyond training scale through pretraining on fragments. Overall, we establish a new paradigm integrating chemical fragmentations with diffusion models, advancing computational chemistry workflows.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_7: This study introduces a modified score matching method aimed at generating molecular structures with high energy accuracy. The denoising process of score matching or diffusion models mirrors molecular structure optimization, where scores act like physical force fields that guide particles toward equilibrium states. To achieve energetically accurate structures, it can be advantageous to have the score closely approximate the gradient of the actual potential energy surface. Unlike conventional methods that simply design the target score based on structural differences in Euclidean space, we propose a Riemannian score matching approach. This method represents molecular structures on a manifold defined by physics-informed internal coordinates to efficiently mimic the energy landscape, and performs noising and denoising within this space. Our method has been evaluated by refining several types of starting structures on the QM9 and GEOM datasets, demonstrating that the proposed Riemannian score matching method significantly improves the accuracy of the generated molecular structures, attaining chemical accuracy. The implications of this study extend to various applications in computational chemistry, offering a robust tool for accurate molecular structure prediction.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_8: The diffusion generative model has achieved remarkable performance across various research fields. In this study, we propose a transferable graph attention diffusion model, GADIFF, for a molecular conformation generation task. With adopting multiple equivariant networks in the Markov chain, GADIFF adds GIN (Graph Isomorphism Network) to acquire local information of subgraphs with different edge types (atomic bonds, bond angle interactions, torsion angle interactions, long-range interactions) and applies MSA (Multi-head Self-attention) as noise attention mechanism to capture global molecular information, which improves the representative of features. In addition, we utilize MSA to calculate dynamic noise weights to boost molecular conformation noise prediction. Upon the improvements, GADIFF achieves competitive performance compared with recently reported state-of-the-art models in terms of generation diversity(COV-R, COV-P), accuracy (MAT-R, MAT-P), and property prediction for GEOM-QM9 and GEOM-Drugs datasets. In particular, on the GEOM-Drugs dataset, the average COV-R is improved by 3.75% compared with the best baseline model at a threshold (1.25 Å). Furthermore, a transfer model named GADIFF-NCI based on GADIFF is developed to generate conformations for noncovalent interaction (NCI) molecular systems. It takes GADIFF with GEOM-QM9 dataset as a pre-trained model, and incorporates a graph encoder for learning molecular vectors at the NCI molecular level. The resulting NCI molecular conformations are reasonable, as assessed by the evaluation of conformation and property predictions. This suggests that the proposed transferable model may hold noteworthy value for the study of multi-molecular conformations.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_13_9: Accurately predicting the diverse bound-state conformations of small molecules is crucial for successful drug discovery and design, particularly when detailed protein–ligand interactions are unknown. Established tools exist, but efficiently exploring the vast conformational space remains challenging. This work introduces Moltiverse, a novel protocol using enhanced sampling molecular dynamics (MD) simulations for conformer generation. The extended adaptive biasing force (eABF) algorithm combined with metadynamics, guided by a single collective variable (radius of gyration, RDGYR), efficiently samples the conformational landscape of a small molecule. Moltiverse demonstrates comparable accuracy and, in some cases, superior quality when benchmarked against established software like RDKit, CONFORGE, Balloon, iCon, and Conformator in the Platinum Diverse Data set for drug-like small molecules and the Prime data set for macrocycles. We present multiple quantitative metrics and statistical analysis for robust conformer generation algorithm comparisons and provide recommendations for their improvement based on our findings. Our extensive evaluation shows that Moltiverse is particularly effective for challenging systems with high conformational flexibility, such as macrocycles, where it achieves the highest accuracy among the tested algorithms. The physics-based approach employed by Moltiverse effectively handles a wide range of molecular complexities, positioning it as a valuable tool for computational drug discovery workflows requiring accurate representation of molecular flexibility.","Main_13: Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG
",0.45
"Cite_14_1: We study the consistency of minimum-norm interpolation in reproducing kernel Hilbert spaces corresponding to bounded kernels. Our main result give lower bounds for the generalization error of the kernel interpolation measured in a continuous scale of norms that interpolate between L^2 and the hypothesis space. These lower bounds imply that kernel interpolation is always inconsistent, when the smoothness index of the norm is larger than a constant that depends only on the embedding index of the hypothesis space and the decay rate of the eigenvalues.","Main_14:  It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown -variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H"" older space with smoothness  or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
",0.25
"Cite_14_2: We explore the approximation capabilities of Transformer networks for Hölder and Sobolev functions, and apply these results to address nonparametric regression estimation with dependent observations. First, we establish novel upper bounds for standard Transformer networks approximating sequence-to-sequence mappings whose component functions are Hölder continuous with smoothness index γ P p0,1s. To achieve an approximation error ε under the Lp-norm for p P r1,8s, it suffices to use a fixed-depth Transformer network whose total number of parameters scales as ε´dxn{γ. This result not only extends existing findings to include the case p “ 8, but also matches the best known upper bounds on number of parameters previously obtained for fixed-depth FNNs and RNNs. Similar bounds are also derived for Sobolev functions. Second, we derive explicit convergence rates for the nonparametric regression problem under various β-mixing data assumptions, which allow the dependence between observations to weaken over time. Our bounds on the sample complexity impose no constraints on weight magnitudes. Lastly, we propose a novel proof strategy to establish approximation bounds, inspired by the Kolmogorov-Arnold representation theorem. We show that if the self-attention layer in a Transformer can perform column averaging, the network can approximate sequence-to-sequence Hölder functions, offering new insights into the interpretability of self-attention mechanisms.","Main_14:  It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown -variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H"" older space with smoothness  or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
",0.25
"Cite_14_3: In recent years, there has been growing interest in the field of functional neural networks. They have been proposed and studied with the aim of approximating continuous functionals defined on sets of functions on Euclidean domains. In this paper, we consider functionals defined on sets of functions on spheres. The approximation ability of deep ReLU neural networks is investigated by novel spherical analysis using an encoder-decoder framework. An encoder comes up first to accommodate the infinite-dimensional nature of the domain of functionals. It utilizes spherical harmonics to help us extract the latent finite-dimensional information of functions, which in turn facilitates in the next step of approximation analysis using fully connected neural networks. Moreover, real-world objects are frequently sampled discretely and are often corrupted by noise. Therefore, encoders with discrete input and those with discrete and random noise input are constructed, respectively. The approximation rates with different encoder structures are provided therein.","Main_14:  It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown -variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H"" older space with smoothness  or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
",0.25
"Cite_14_4: We propose nonuniform data-driven parameter distributions for neural network initialization based on derivative data of the function to be approximated. These parameter distributions are developed in the context of non-parametric regression models based on shallow neural networks, and compare favorably to well-established uniform random feature models based on conventional weight initialization. We address the cases of Heaviside and ReLU activation functions, and their smooth approximations (sigmoid and softplus), and use recent results on the harmonic analysis and sparse representation of neural networks resulting from fully trained optimal networks. Extending analytic results that give exact representation, we obtain densities that concentrate in regions of the parameter space corresponding to neurons that are well suited to model the local derivatives of the unknown function. Based on these results, we suggest simplifications of these exact densities based on approximate derivative data in the input points that allow for very efficient sampling and lead to performance of random feature models close to optimal networks in several scenarios.","Main_14:  It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown -variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H"" older space with smoothness  or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
",0.25
"Cite_14_5: Let Ω ⊂ Rd be a bounded domain. We consider the problem of how efficiently shallow neural networks with the ReLUk activation function can approximate functions from Sobolev spaces Ws(Lp(Ω)) with error measured in the Lq(Ω)-norm. Utilizing the Radon transform and recent results from discrepancy theory, we provide a simple proof of nearly optimal approximation rates in a variety of cases, including when q ≤ p, p≥2, and s≤k+(d+1)/2. Therates wederiveareoptimal up to logarithmic factors, and significantly generalize existing results. An interesting consequence is that the adaptivity of shallow ReLUk neural networks enables them to obtain optimal approximation rates for smoothness up to order s = k+(d +1)/2, even though they represent piecewise polynomials of fixed degree k.","Main_14:  It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown -variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H"" older space with smoothness  or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
",0.25
"Cite_15_1: Industry 4.0 has emerged as an important era for process monitoring and improvement. Our expository paper provides a historical perspective on research and practice of statistical process monitoring (SPM) from the 1920s to the present to bring a high-level view of current practice and research directions. We focus on the Industry 4.0 era, which began around 2011 with the introduction of cyber-physical systems and the growth of the Internet of Things. These technological changes have brought tremendous challenges and opportunities to SPM that can only be met with new paradigms for the problems we aim to solve and the approaches we use to evaluate SPM methodology. We provide our perspective on these challenges, primarily focusing on industrial applications. We give recommendations on the evaluation and comparison of monitoring methods to improve the usefulness of research in this area.","Main_15:   One of the significant challenges in monitoring the quality of products today
is the high dimensionality of quality characteristics. In this paper, we
address Phase I analysis of high-dimensional processes with individual
observations when the available number of samples collected over time is
limited. Using a new charting statistic, we propose a robust procedure for
parameter estimation in Phase I. This robust procedure is efficient in
parameter estimation in the presence of outliers or contamination in the data.
A consistent estimator is proposed for parameter estimation and a finite sample
correction coefficient is derived and evaluated through simulation. We assess
the statistical performance of the proposed method in Phase I in terms of the
probability of signal criterion. This assessment is carried out in the absence
and presence of outliers. We show that, in both phases, the proposed control
chart scheme effectively detects various kinds of shifts in the process mean.
Besides, we present two real-world examples to illustrate the applicability of
our proposed method.
",0.2
"Cite_15_2: While researchers and practitioners are seamlessly trying to develop methods for minimizing the effect of outliers in control charts, detecting and screening these outliers continue to pose serious challenges. Keeping in view, the researchers rely on robust estimators to modify the detection limits structure so that the chart can be more sensitive against outliers. In this study, we propose a robust control chart based on and  estimators, whilst the process parameter is estimated from Phase-I. Through intensive Monte-Carlo simulations, the study presents how the estimation of parameter(s) and presence of outliers affect the efficacy of the chart, and then how the proposed outlier detectors bring the chart back to normalcy by restoring its efficacy and sensitivity. Average properties are used as the performance measures. The properties establish the superiority of the proposed scheme over and Tukey's outlier detectors. The applicability of the study includes the effectiveness of the proposed detectors in industrial data set but is not limited to manufacturing industries.","Main_15:   One of the significant challenges in monitoring the quality of products today
is the high dimensionality of quality characteristics. In this paper, we
address Phase I analysis of high-dimensional processes with individual
observations when the available number of samples collected over time is
limited. Using a new charting statistic, we propose a robust procedure for
parameter estimation in Phase I. This robust procedure is efficient in
parameter estimation in the presence of outliers or contamination in the data.
A consistent estimator is proposed for parameter estimation and a finite sample
correction coefficient is derived and evaluated through simulation. We assess
the statistical performance of the proposed method in Phase I in terms of the
probability of signal criterion. This assessment is carried out in the absence
and presence of outliers. We show that, in both phases, the proposed control
chart scheme effectively detects various kinds of shifts in the process mean.
Besides, we present two real-world examples to illustrate the applicability of
our proposed method.
",0.2
"Cite_15_3: Identifying key quality characteristics (KQCs) in manufacturing processes plays an important role in quality control. This paper proposes a method called GSCV-RF-RFE, which combines Grid Search Cross-Validation (GSCV), Random Forest (RF), and Recursive Feature Elimination (RFE), to identify KQCs in imbalanced production data. In the proposed method, the Multiple Imputation by Chained Equations (MICE) algorithm is employed for multiple interpolation of the dataset, while the parameters of the RF algorithm are optimized using the GS method. Comparative analysis demonstrates that the GSCV -RF model outperforms six algorithms, and achieves higher model accuracy than Multinomial Naive Bayes (MNB). Furthermore, the optimized RF algorithm is combined with a RFE algorithm, and adaptive synthetic sampling is employed to balance the input samples. Spearman correlation analysis is conducted to eliminate strong correlations. Experimental result reveal that the proposed combined strategy achieves a lower Class II error rate and a higher accuracy rate in quality feature identification, indicating its excellent overall performance.","Main_15:   One of the significant challenges in monitoring the quality of products today
is the high dimensionality of quality characteristics. In this paper, we
address Phase I analysis of high-dimensional processes with individual
observations when the available number of samples collected over time is
limited. Using a new charting statistic, we propose a robust procedure for
parameter estimation in Phase I. This robust procedure is efficient in
parameter estimation in the presence of outliers or contamination in the data.
A consistent estimator is proposed for parameter estimation and a finite sample
correction coefficient is derived and evaluated through simulation. We assess
the statistical performance of the proposed method in Phase I in terms of the
probability of signal criterion. This assessment is carried out in the absence
and presence of outliers. We show that, in both phases, the proposed control
chart scheme effectively detects various kinds of shifts in the process mean.
Besides, we present two real-world examples to illustrate the applicability of
our proposed method.
",0.2
"Cite_15_4: Modern data collecting methods and computation tools have made it possible to monitor high-dimensional processes. In this article, we investigate phase II monitoring of high-dimensional processes when the available number of samples collected in phase I is limited in comparison to the number of variables. A new charting statistic for high-dimensional multivariate processes based on the diagonal elements of the underlying covariance matrix is introduced and we propose a unified procedure for phases I and II by employing a self-starting control chart. To remedy the effect of outliers, we adopt a robust procedure for parameter estimation in phase I and introduce the appropriate consistent estimators. The statistical performance of the proposed method is evaluated in phase II using the average run length (ARL) criterion in the absence and presence of outliers. Results show that the proposed control chart scheme effectively detects various kinds of shifts in the process mean vector. Finally, we illustrate the applicability of our proposed method via a manufacturing application.","Main_15:   One of the significant challenges in monitoring the quality of products today
is the high dimensionality of quality characteristics. In this paper, we
address Phase I analysis of high-dimensional processes with individual
observations when the available number of samples collected over time is
limited. Using a new charting statistic, we propose a robust procedure for
parameter estimation in Phase I. This robust procedure is efficient in
parameter estimation in the presence of outliers or contamination in the data.
A consistent estimator is proposed for parameter estimation and a finite sample
correction coefficient is derived and evaluated through simulation. We assess
the statistical performance of the proposed method in Phase I in terms of the
probability of signal criterion. This assessment is carried out in the absence
and presence of outliers. We show that, in both phases, the proposed control
chart scheme effectively detects various kinds of shifts in the process mean.
Besides, we present two real-world examples to illustrate the applicability of
our proposed method.
",0.2
"Cite_16_1: Federated Averaging (FedAvg) and its variants are the most popular optimization algorithms in federated learning (FL). Previous convergence analyses of FedAvg either assume full client participation or partial client participation where the clients can be uniformly sampled. However, in practical cross-device FL systems, only a subset of clients that satisfy local criteria such as battery status, network connectivity, and maximum participation frequency requirements (to ensure privacy) are available for training at a given time. As a result, client availability follows a natural cyclic pattern. We provide (to our knowledge) the first theoretical framework to analyze the convergence of FedAvg with cyclic client participation with several different client optimizers such as GD, SGD, and shuffled SGD. Our analysis discovers that cyclic client participation can achieve a faster asymptotic convergence rate than vanilla FedAvg with uniform client participation under suitable conditions, providing valuable insights into the design of client sampling protocols.","Main_16:  A major bottleneck of distributed learning under parameter-server (PS)
framework is communication cost due to frequent bidirectional transmissions
between the PS and workers. To address this issue, local stochastic gradient
descent (SGD) and worker selection have been exploited by reducing the
communication frequency and the number of participating workers at each round,
respectively. However, partial participation can be detrimental to convergence
rate, especially for heterogeneous local datasets. In this paper, to improve
communication efficiency and speed up the training process, we develop a novel
worker selection strategy named AgeSel. The key enabler of AgeSel is
utilization of the ages of workers to balance their participation frequencies.
The convergence of local SGD with the proposed age-based partial worker
participation is rigorously established. Simulation results demonstrate that
the proposed AgeSel strategy can significantly reduce the number of training
rounds needed to achieve a targeted accuracy, as well as the communication
cost. The influence of the algorithm hyper-parameter is also explored to
manifest the benefit of age-based worker selection.
",0.05
"Cite_17_1: Inspired by the observation of a robust 𝑑-wave superconducting phase driven by tuning the next-nearest-neighbor (NNN) electron hopping in recent density matrix renormalization group (DMRG) studies of six- and eight-leg 𝑡−𝐽 model, we systematically study the phase diagram of the two-leg 𝑡−𝐽 ladder with the NNN couplings (including NNN hopping and spin interaction) in a large region of doping level, by means of the DMRG calculations. Upon doping from half filling, we identify the Luther-Emery liquid (LEL) phase, which can be distinguished as the pairing-dominant and charge density-dominant regime by comparing the Luttinger parameter 𝐾𝜌. With the growing NNN couplings, pairing correlations are enhanced and correspondingly 𝐾𝜌 increases, driving the system from the density-dominant to the pairing-dominant regime. In the Tomonaga-Luttinger liquid (TLL) phase in the larger doping region, we identify two TLL regimes with different features of charge density correlation. At the quarter filling (1/2 doping level), we find that the strong dimer orders of bond energy in the open system actually decay algebraically and thus do not indicate a spontaneous translational symmetry breaking. Our results show that in the LEL phase of the two-leg ladder, the NNN couplings seem to play the similar role as that on the wider 𝑡−𝐽 cylinder, and studies on this more accessible system can be helpful toward understanding the emergence of the remarkable 𝑑-wave superconducting phase on the wider system.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_2: We present a deterministic algorithm for the efficient evaluation of imaginary-time diagrams based on the recently introduced discrete Lehmann representation (DLR) of imaginary-time Green’s functions. In addition to the efficient discretization of diagrammatic integrals afforded by its approximation properties, the DLR basis is separable in imaginary-time, allowing us to decompose diagrams into linear combinations of nested sequences of one-dimensional products and convolutions. Focusing on the strong-coupling bold-line expansion of generalized Anderson impurity models, we show that our strategy reduces the computational complexity of evaluating an 𝑀th-order diagram at inverse temperature 𝛽 and spectral width 𝜔max from 𝒪⁡((𝛽⁢𝜔max)2⁢𝑀−1) for a direct quadrature to 𝒪⁡(𝑀⁢(log⁡(𝛽⁢𝜔max))𝑀+1), with controllable high-order accuracy. We benchmark our algorithm using third-order expansions for multiband impurity problems with off-diagonal hybridization and spin-orbit coupling, presenting comparisons with exact diagonalization and quantum Monte Carlo approaches. In particular, we perform a self-consistent dynamical mean-field theory calculation for a three-band Hubbard model with strong spin-orbit coupling representing a minimal model of Ca2⁢RuO4, demonstrating the promise of the method for modeling realistic strongly correlated multiband materials. For both strong and weak coupling expansions of low and intermediate order, in which diagrams can be enumerated, our method provides an efficient, straightforward, and robust blackbox evaluation procedure. In this sense, it fills a gap between diagrammatic approximations of the lowest order, which are simple and inexpensive but inaccurate, and those based on Monte Carlo sampling of high-order diagrams.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_3: We present a generalization of the discrete Lehmann representation (DLR) to three-point correlation and vertex functions in imaginary time and Matsubara frequency. The representation takes the form of a linear combination of judiciously chosen exponentials in imaginary time, and products of simple poles in Matsubara frequency, which are universal for a given temperature and energy cutoff. We present a systematic algorithm to generate compact sampling grids, from which the coefficients of such an expansion can be obtained by solving a linear system. We show that the explicit form of the representation can be used to evaluate diagrammatic expressions involving infinite Matsubara sums, such as polarization functions or self-energies, with controllable, high-order accuracy. This collection of techniques establishes a framework through which methods involving three-point objects can be implemented robustly, with a substantially reduced computational cost and memory footprint.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_4: We propose a simple and efficient method to calculate the electronic self-energy in dynamical mean-field theory (DMFT), addressing a numerical instability often encountered when solving the Dyson equation. Our approach formulates the Dyson equation as a constrained optimization problem with a simple quadratic objective. The constraints on the self-energy are obtained via direct measurement of the leading order terms of its asymptotic expansion within a continuous time quantum Monte Carlo framework and the use of the compact discrete Lehmann representation of the self-energy yields an optimization problem in a modest number of unknowns. We benchmark our method for the noninteracting Bethe lattice, as well as DMFT calculations for both model systems and ab initio applications.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_5: Two-dimensional (2D) metals can host gapless plasmonic excitations that strongly couple to electrons and thus may significantly affect superconductivity. To investigate the dynamical interplay of the electron–electron and electron–phonon interactions in the theory of 2D superconductivity, we apply a full momentum- and frequency-dependent one-loop theory treating electron–phonon, electron–plasmon, and phonon–plasmon coupling with the same accuracy. We tune the strength of the Coulomb interaction by varying the external screening to the layered superconductor and find three distinct regions. At weak screening, superconductivity is mediated by plasmons. In the opposite limit conventional electron–phonon interactions dominate. In between, we find a suppressed superconducting state. Our results show that even in conventional electron–phonon coupled layered materials, superconductivity can be significantly enhanced by the electron–plasmon coupling in a manner that can be controlled by the external screening.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_6: The recent observation of superconductivity in twisted bilayer WSe_2 raises intriguing questions concerning the origin and the properties of superconducting states realized in bands with non-trivial topological properties and repulsive electron-electron interactions. Using a continuum band structure model, we analyze a mechanism for Coulomb interaction-driven superconductivity in twisted bilayers of WSe_2. We discuss the symmetries and the phenomenological properties of the resulting superconducting phases and their evolution with interlayer potential difference, tunable via an out of plane electric field. The pairing strength is a non-monotonic function of interlayer potential, being larger at intermediate values due to mixing of singlet and triplet pairing. In contrast, at larger interlayer potential, the pairing tendency is suppressed due to enhanced Coulomb repulsion. The superconducting state is chiral in a large regime of parameters and undergoes a transition to a nodal nematic superconductor at a critical potential difference. The chiral state, characterized by an intervalley-symmetric superposition of triplet and singlet pairs, is classified as a topological superconductor within the Altland-Zirnbauer class C. At zero interlayer potential difference, the superconducting state is instead of class D, which hosts Majorana zero modes, making it a promising candidate for applications in quantum computation.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_7: Several recent works have introduced highly compact representations of single-particle Green's functions in the imaginary time and Matsubara frequency domains, as well as efficient interpolation grids used to recover the representations. In particular, the intermediate representation with sparse sampling and the discrete Lehmann representation (DLR) make use of low rank compression techniques to obtain optimal approximations with controllable accuracy. We consider the use of the DLR in dynamical mean-field theory (DMFT) calculations, and in particular show that the standard full Matsubara frequency grid can be replaced by the compact grid of DLR Matsubara frequency nodes. We test the performance of the method for a DMFT calculation of Sr2⁢RuO4 at temperature 50K using a continuous-time quantum Monte Carlo impurity solver, and demonstrate that Matsubara frequency quantities can be represented on a grid of only 36 nodes with no reduction in accuracy, or increase in the number of self-consistent iterations, despite the presence of significant Monte Carlo noise.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_8: Based on a nonperturbative scheme to determine the self-energy Σ⁡(𝒌,𝑖⁢𝜔𝑛) with automatically satisfying the Ward identity and the total-momentum conservation law, a fully self-consistent calculation is done in the electron gas at various temperatures 𝑇 to obtain 𝐺⁡(𝒌,𝑖⁢𝜔𝑛) the one-particle Green's function with fulfilling all known conservation laws, sum rules, and correct asymptotic behaviors; here, 𝑇 is taken unprecedentedly low, namely, 𝑘B⁢𝑇/ɛF down to 10−4 with ɛF the Fermi energy, and tiny mesh Δ⁢𝑘 as small as 10−4⁢𝑘F is chosen near the Fermi surface in 𝒌 space with 𝑘F the Fermi momentum. By analytically continuing 𝐺⁡(𝒌,𝑖⁢𝜔𝑛) to the retarded function 𝐺𝑅⁡(𝒌,𝜔), we find a novel low-energy peak, in addition to the quasiparticle (QP) peak and one- and two-plasmon high-energy satellites, in the spectral function 𝐴⁡(𝒌,𝜔)⁢[=−Im⁡𝐺𝑅⁡(𝒌,𝜔)/𝜋] for 𝑘B⁢𝑇≲10−3⁢ɛF in the simple-metal density region (2<𝑟𝑠<6 with 𝑟𝑠 the dimensionless density parameter). This new peak is attributed to the effect of excitonic attraction on Σ⁡(𝒌,𝑖⁢𝜔𝑛) arising from multiple excitations of tightly bound electron-hole pairs in the polarization function Π⁡(𝒒,𝑖⁢𝜔𝑞) for |𝒒|≈2⁢𝑘F and |𝜔𝑞⁢|≪⁢ɛF and thus it is dubbed “excitron.” Although this excitron peak height is only about a one-hundredth of that of QP, its excitation energy is about a half of that of QP for |𝒌|≈𝑘F, seemingly in contradiction to the Landau's hypothesis as to the one-to-one correspondence of low-energy excitations between a free Fermi gas and an interacting normal Fermi liquid. As for the QP properties, our results of both the effective mass 𝑚* and the renormalization factor 𝑧* are in good agreement with those provided by recent quantum Monte Carlo simulations and available experiments.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_17_9: We propose a computational graph representation of high-order Feynman diagrams in Quantum Field Theory (QFT), applicable to any combination of spatial, temporal, momentum, and frequency domains. Utilizing the Dyson-Schwinger and parquet equations, our approach effectively organizes these diagrams into a fractal structure of tensor operations, significantly reducing computational redundancy. This approach not only streamlines the evaluation of complex diagrams but also facilitates an efficient implementation of the field-theoretic renormalization scheme, crucial for enhancing perturbative QFT calculations. Key to this advancement is the integration of Taylor-mode automatic differentiation, a key technique employed in machine learning packages to compute higher-order derivatives efficiently on computational graphs. To operationalize these concepts, we develop a Feynman diagram compiler that optimizes diagrams for various computational platforms, utilizing machine learning frameworks. Demonstrating this methodology's effectiveness, we apply it to the three-dimensional uniform electron gas problem, achieving unprecedented accuracy in calculating the quasiparticle effective mass at metal density. Our work demonstrates the synergy between QFT and machine learning, establishing a new avenue for applying AI techniques to complex quantum many-body problems.","Main_17:   We study the Cooper instability in jellium model in the controlled regime of
small to intermediate values of the Coulomb parameter $r_s \leq 2$. We confirm
that superconductivity naturally emerges from purely repulsive interactions
described by the Kukkonen-Overhauser vertex function. By employing the implicit
renormalization approach we reveal that even in the small-$r_s$ limit, the
dominant mechanism behind Cooper instability is based on dynamic screening of
the Coulomb interaction--accurately captured by the random phase approximation,
whereas the Kohn-Luttinger contribution is negligibly small and, thus, not
relevant.
",0.45
"Cite_18_1: While large language models (LLMs) have demonstrated remarkable capabilities in language modeling, recent studies reveal that they often fail on out-of-distribution (OOD) samples due to spurious correlations acquired during pre-training. Here, we aim to mitigate such spurious correlations through causality-aware post-training (CAPT). By decomposing a biased prediction into two unbiased steps, known as 	extit{event estimation} and 	extit{event intervention}, we reduce LLMs' pre-training biases without incurring additional fine-tuning biases, thus enhancing the model's generalization ability. Experiments on the formal causal inference benchmark CLadder and the logical reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with CAPT can outperform both traditional SFT and larger LLMs on in-distribution (ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the effectiveness and sample efficiency of CAPT.","Main_18:   Recent empirical studies on domain generalization (DG) have shown that DG
algorithms that perform well on some distribution shifts fail on others, and no
state-of-the-art DG algorithm performs consistently well on all shifts.
Moreover, real-world data often has multiple distribution shifts over different
attributes; hence we introduce multi-attribute distribution shift datasets and
find that the accuracy of existing DG algorithms falls even further. To explain
these results, we provide a formal characterization of generalization under
multi-attribute shifts using a canonical causal graph. Based on the
relationship between spurious attributes and the classification label, we
obtain realizations of the canonical causal graph that characterize common
distribution shifts and show that each shift entails different independence
constraints over observed variables. As a result, we prove that any algorithm
based on a single, fixed constraint cannot work well across all shifts,
providing theoretical evidence for mixed empirical results on DG algorithms.
Based on this insight, we develop Causally Adaptive Constraint Minimization
(CACM), an algorithm that uses knowledge about the data-generating process to
adaptively identify and apply the correct independence constraints for
regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds
datasets, covering binary and multi-valued attributes and labels, show that
adaptive dataset-dependent constraints lead to the highest accuracy on unseen
domains whereas incorrect constraints fail to do so. Our results demonstrate
the importance of modeling the causal relationships inherent in the
data-generating process.
",0.25
"Cite_18_2: Inferring the graph structure from observed data is a key task in graph machine learning to capture the intrinsic relationship between data entities. While significant advancements have been made in learning the structure of homogeneous graphs, many real-world graphs exhibit heterogeneous patterns where nodes and edges have multiple types. This paper fills this gap by introducing the first approach for heterogeneous graph structure learning (HGSL). To this end, we first propose a novel statistical model for the data-generating process (DGP) of heterogeneous graph data, namely hidden Markov networks for heterogeneous graphs (H2MN). Then we formalize HGSL as a maximum a-posterior estimation problem parameterized by such DGP and derive an alternating optimization method to obtain a solution together with a theoretical justification of the optimization conditions. Finally, we conduct extensive experiments on both synthetic and real-world datasets to demonstrate that our proposed method excels in learning structure on heterogeneous graphs in terms of edge type identification and edge weight recovery.","Main_18:   Recent empirical studies on domain generalization (DG) have shown that DG
algorithms that perform well on some distribution shifts fail on others, and no
state-of-the-art DG algorithm performs consistently well on all shifts.
Moreover, real-world data often has multiple distribution shifts over different
attributes; hence we introduce multi-attribute distribution shift datasets and
find that the accuracy of existing DG algorithms falls even further. To explain
these results, we provide a formal characterization of generalization under
multi-attribute shifts using a canonical causal graph. Based on the
relationship between spurious attributes and the classification label, we
obtain realizations of the canonical causal graph that characterize common
distribution shifts and show that each shift entails different independence
constraints over observed variables. As a result, we prove that any algorithm
based on a single, fixed constraint cannot work well across all shifts,
providing theoretical evidence for mixed empirical results on DG algorithms.
Based on this insight, we develop Causally Adaptive Constraint Minimization
(CACM), an algorithm that uses knowledge about the data-generating process to
adaptively identify and apply the correct independence constraints for
regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds
datasets, covering binary and multi-valued attributes and labels, show that
adaptive dataset-dependent constraints lead to the highest accuracy on unseen
domains whereas incorrect constraints fail to do so. Our results demonstrate
the importance of modeling the causal relationships inherent in the
data-generating process.
",0.25
"Cite_18_3: Economies are fundamentally complex and becoming more so, but the new discipline of data science-which combines programming, statistics, and domain knowledge-can help cut through that complexity, potentially with productivity benefits to boot. This chapter looks at examples of where innovations from data science are cutting through the complexities faced by policymakers in measurement, allocating resources, monitoring the natural world, making predictions, and more. These examples show the promise and potential of data science to aid policymakers, and point to where actions may be taken that would support further progress in this space.","Main_18:   Recent empirical studies on domain generalization (DG) have shown that DG
algorithms that perform well on some distribution shifts fail on others, and no
state-of-the-art DG algorithm performs consistently well on all shifts.
Moreover, real-world data often has multiple distribution shifts over different
attributes; hence we introduce multi-attribute distribution shift datasets and
find that the accuracy of existing DG algorithms falls even further. To explain
these results, we provide a formal characterization of generalization under
multi-attribute shifts using a canonical causal graph. Based on the
relationship between spurious attributes and the classification label, we
obtain realizations of the canonical causal graph that characterize common
distribution shifts and show that each shift entails different independence
constraints over observed variables. As a result, we prove that any algorithm
based on a single, fixed constraint cannot work well across all shifts,
providing theoretical evidence for mixed empirical results on DG algorithms.
Based on this insight, we develop Causally Adaptive Constraint Minimization
(CACM), an algorithm that uses knowledge about the data-generating process to
adaptively identify and apply the correct independence constraints for
regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds
datasets, covering binary and multi-valued attributes and labels, show that
adaptive dataset-dependent constraints lead to the highest accuracy on unseen
domains whereas incorrect constraints fail to do so. Our results demonstrate
the importance of modeling the causal relationships inherent in the
data-generating process.
",0.25
"Cite_18_4: Weconsider the task of out-of-distribution (OOD) generalization, where the distribution shift is due to an unobserved confounder (Z) affecting both the covariates (X) and the labels (Y ). In this setting, traditional assumptions of covariate and label shift are unsuitable due to the confounding, which introduces heterogeneity in the predictor, i.e., ˆ Y = fZ(X). OOD generalization differs from traditional domain adaptation by not assuming access to the covariate distribution (Xte) of the test samples during training. These conditions create a challenging scenario for OOD robustness: (a) Ztr is an unobserved confounder during training, (b) Pte(Z)= Ptr(Z), (c) Xte is unavailable during training, and (d) the posterior predictive distribution depends on Pte(Z), i.e., ˆ Y = EPte(Z)[fZ(X)]. In general, accurate predictions are unattainable in this scenario, and existing literature has proposed complex predictors based on identifiability assumptions that require multiple additional variables. Our work investigates a set of identifiability assumptions that tremendously simplify the predictor, whose resulting elegant simplicity outperforms existing approaches.","Main_18:   Recent empirical studies on domain generalization (DG) have shown that DG
algorithms that perform well on some distribution shifts fail on others, and no
state-of-the-art DG algorithm performs consistently well on all shifts.
Moreover, real-world data often has multiple distribution shifts over different
attributes; hence we introduce multi-attribute distribution shift datasets and
find that the accuracy of existing DG algorithms falls even further. To explain
these results, we provide a formal characterization of generalization under
multi-attribute shifts using a canonical causal graph. Based on the
relationship between spurious attributes and the classification label, we
obtain realizations of the canonical causal graph that characterize common
distribution shifts and show that each shift entails different independence
constraints over observed variables. As a result, we prove that any algorithm
based on a single, fixed constraint cannot work well across all shifts,
providing theoretical evidence for mixed empirical results on DG algorithms.
Based on this insight, we develop Causally Adaptive Constraint Minimization
(CACM), an algorithm that uses knowledge about the data-generating process to
adaptively identify and apply the correct independence constraints for
regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds
datasets, covering binary and multi-valued attributes and labels, show that
adaptive dataset-dependent constraints lead to the highest accuracy on unseen
domains whereas incorrect constraints fail to do so. Our results demonstrate
the importance of modeling the causal relationships inherent in the
data-generating process.
",0.25
"Cite_18_5: During prediction tasks, models can use any signal they receive to come up with the final answer - including signals that are causally irrelevant. When predicting objects from images, for example, the lighting conditions could be correlated to different targets through selection bias, and an oblivious model might use these signals as shortcuts to discern between various objects. A predictor that uses lighting conditions instead of real object-specific details is obviously undesirable. To address this challenge, we introduce a standard anti-causal prediction model (SAM) that creates a causal framework for analyzing the information pathways influencing our predictor in anti-causal settings. We demonstrate that a classifier satisfying a specific conditional independence criterion will focus solely on the direct causal path from label to image, being counterfactually invariant to the remaining variables. Finally, we propose DISCO, a novel regularization strategy that uses conditional distance correlation to optimize for conditional independence in regression tasks. We can show that DISCO achieves competitive results in different bias mitigation experiments, deeming it a valid alternative to classical kernel-based methods.","Main_18:   Recent empirical studies on domain generalization (DG) have shown that DG
algorithms that perform well on some distribution shifts fail on others, and no
state-of-the-art DG algorithm performs consistently well on all shifts.
Moreover, real-world data often has multiple distribution shifts over different
attributes; hence we introduce multi-attribute distribution shift datasets and
find that the accuracy of existing DG algorithms falls even further. To explain
these results, we provide a formal characterization of generalization under
multi-attribute shifts using a canonical causal graph. Based on the
relationship between spurious attributes and the classification label, we
obtain realizations of the canonical causal graph that characterize common
distribution shifts and show that each shift entails different independence
constraints over observed variables. As a result, we prove that any algorithm
based on a single, fixed constraint cannot work well across all shifts,
providing theoretical evidence for mixed empirical results on DG algorithms.
Based on this insight, we develop Causally Adaptive Constraint Minimization
(CACM), an algorithm that uses knowledge about the data-generating process to
adaptively identify and apply the correct independence constraints for
regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds
datasets, covering binary and multi-valued attributes and labels, show that
adaptive dataset-dependent constraints lead to the highest accuracy on unseen
domains whereas incorrect constraints fail to do so. Our results demonstrate
the importance of modeling the causal relationships inherent in the
data-generating process.
",0.25
"Cite_19_1: Let r(kAn) denote the number of representations of n as a sum of k elements of a set A N. In 2002, Dombi conjectured that if A is co-in nite, then the sequence (r(kAn))n 0 cannot be strictly increasing. Using tools from automata theory and logic, we give an explicit counterexample where N A has positive lower density.","Main_19: We disprove a 2002 conjecture of Dombi from additive number theory. More
precisely, we find examples of sets $A \subset \mathbb{N}$ with the property
that $\mathbb{N} \setminus A$ is infinite, but the sequence $n \rightarrow |\{
(a,b,c) \, : \, n=a+b+c \text{ and } a,b,c \in A \}|$, counting the number of
$3$-compositions using elements of $A$ only, is strictly increasing.
",0.25
"Cite_19_2: et  k≥2 be an integer and let A be a set of nonnegative integers. The representation function  RA,k(n) for the set A is the number of representations of a nonnegative integer n as the sum of k terms from A. Let  A(n) denote the counting function of A. Bell and Shallit [‘Counterexamples to a conjecture of Dombi in additive number theory’, Acta Math. Hung., to appear] recently gave a counterexample for a conjecture of Dombi and proved that if  A(n)=o(n(k−2)/k−ϵ) for some  ϵ>0, then  RN∖A,k(n) is eventually strictly increasing. We improve this result to  A(n)=O(n(k−2)/(k−1)). We also give an example to show that this bound is best possible.","Main_19: We disprove a 2002 conjecture of Dombi from additive number theory. More
precisely, we find examples of sets $A \subset \mathbb{N}$ with the property
that $\mathbb{N} \setminus A$ is infinite, but the sequence $n \rightarrow |\{
(a,b,c) \, : \, n=a+b+c \text{ and } a,b,c \in A \}|$, counting the number of
$3$-compositions using elements of $A$ only, is strictly increasing.
",0.25
"Cite_19_3: We show how to “automatically” prove results about sequences in the On-Line Encyclopedia of Integer Sequences (OEIS) using a free software tool called Walnut, and illustrate it with a number of examples chosen from the OEIS.","Main_19: We disprove a 2002 conjecture of Dombi from additive number theory. More
precisely, we find examples of sets $A \subset \mathbb{N}$ with the property
that $\mathbb{N} \setminus A$ is infinite, but the sequence $n \rightarrow |\{
(a,b,c) \, : \, n=a+b+c \text{ and } a,b,c \in A \}|$, counting the number of
$3$-compositions using elements of $A$ only, is strictly increasing.
",0.25
"Cite_19_4: Let t (t(n))n 0 be the Thue–Morse sequence in 0,1. J.-P. Allouche and J. Shallit asked in 2003 whetherthesubwordcomplexityofthesubsequence(t(n2))n 0 attainsthemaximalvalue.Thisproblemwas solved positively by Y. Moshe in 2007. Indeed Y. Moshe had shown that for all H Q[T] with H(N) N and degH 2,allthesubsequences(t(H(n)))n 0 attainthemaximalsubwordcomplexity.Thenheaskedwh","Main_19: We disprove a 2002 conjecture of Dombi from additive number theory. More
precisely, we find examples of sets $A \subset \mathbb{N}$ with the property
that $\mathbb{N} \setminus A$ is infinite, but the sequence $n \rightarrow |\{
(a,b,c) \, : \, n=a+b+c \text{ and } a,b,c \in A \}|$, counting the number of
$3$-compositions using elements of $A$ only, is strictly increasing.
",0.25
"Cite_19_5: Let r(kAn) denote the number of representations of n as a sum of k elements of a set A N. In 2002, Dombi conjectured that if A is co-innite, then the sequence (r(kAn))n 0 cannot be strictly increasing. Using tools from automata theory and logic, we give an explicit counterexample where N A has positive lower density.","Main_19: We disprove a 2002 conjecture of Dombi from additive number theory. More
precisely, we find examples of sets $A \subset \mathbb{N}$ with the property
that $\mathbb{N} \setminus A$ is infinite, but the sequence $n \rightarrow |\{
(a,b,c) \, : \, n=a+b+c \text{ and } a,b,c \in A \}|$, counting the number of
$3$-compositions using elements of $A$ only, is strictly increasing.
",0.25
"Cite_20_1: Measuring the merger rate density history of binary neutron stars (BNS) can greatly aid in understanding the history of heavy element formation in the Universe. Currently, second-generation gravitational wave (GW) detectors can only measure the BNS merger rate density history at low redshifts (z  0.1). Short gamma-ray bursts (sGRBs) may trace the BNS merger to higher redshifts (z  3). However, not all BNS mergers result in sGRBs, and it is not certain that all sGRBs originate from BNS mergers. In this study, we simultaneously utilize simulated BNS merger GW signals detected by the advanced Laser Interferometer Gravitational-Wave Observatory (LIGO) design and sGRB signals detected by Fermi/GBM (Gamma-ray Burst Monitor) to constrain the BNS merger rate density history up to z  3. The results indicate that with 8 GWs and 571 sGRBs, the BNS merger rate density can be measured with an accuracy of about 50 per cent through to. The ratio of the jet opening angle-corrected sGRB event rate density to the BNS merger rate density, denoted as, can be constrained to a relative uncertainty of 45 per cent. With 21 GWs and 761 sGRBs, the BNS merger rate density can be measured to approximately 35 per cent and 40 per cent at and⁠, respectively. Meanwhile, can be constrained to a relative uncertainty of 28 per cent. Additionally, in our parametrized simulation, we find that at least approximately 550 sGRBs are needed to constrain the characteristic delay time in the star formation rate model, given a relative error of 50 per cent in the estimated redshift.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_2: Future gravitational-wave (GW) detectors are expected to detect tens of thousands of compact binary coalescences (CBC) per year, depending also on the final detectors layout. For this reason, it is essential to have a fast, reliable tool for forecasting how different detector layouts will affect parameter estimation for these events. The Fisher Information Matrix (FIM) is a common tool for tackling this problem. In this paper, we present a new open source code GWJulia to perform FIM analysis of CBC parameters, i.e., stellar black-hole binaries (BBH), neutron star binaries (BNS), and neutron star-black hole binaries (NSBH). The code is purely written in Julia, making it fast while maintaining a high level of accuracy. We consider a set of case studies to compare different Einstein Telescope (ET) designs. We compare a 10km triangular configuration with two 15km L-shaped detectors with different orientations and temperatures. We discuss also the accuracy of combinations of parameters, which is very informative for cosmology or population studies. Finally, we focus on the detection of golden events and explore how the FIM can guide posterior sampling of GW signals using a novel Hamiltonian Monte Carlo (HMC) sampler.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_3: Binary black hole systems are typically assumed to evolve in vacuum. However, the environment surrounding the binary components can influence their properties, such as their tidal deformability, affecting the gravitational waveform produced by the binary and its interpretation in gravitational wave data analysis. In this work we focus on next-generation experiments, such as the Einstein Telescope and LISA, and we quantify the systematic biases in gravitational wave observations that arise when tidally deformed binaries are interpreted as occurring in vacuum. We consider binaries over a range of masses and we compare different phenomenological models for the dynamical evolution of the tidal deformability. We find that systematic biases could significantly affect the measurability of the binary parameters if tidal effects are not carefully modeled.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_4: Taiji, a Chinese space-based gravitational wave detection project, aims to explore the millihertz gravitational wave universe with unprecedented sensitivity, targeting astrophysical and cosmological sources including Galactic binaries, massive black hole binaries, extreme mass-ratio inspirals, and stochastic gravitational wave backgrounds, etc. These observations are expected to provide transformative insights into astrophysics, cosmology, and fundamental physics. However, Taiji's data analysis faces unique challenges distinct from ground-based detectors like LIGO-Virgo-KAGRA, such as the overlap of numerous signals, extended data durations, more rigorous accuracy requirements for the waveform templates, non-negligible subdominant waveform complexities, incompletely characterized noise spectra, non-stationary noises, and various data anomalies. This paper presents the second round of Taiji Data Challenge, a collection of simulation datasets designed as a shared platform for resolving these critical data analysis problems. The current platform distinguishes from previous works by the systematic integration of orbital dynamics based on the full drag-free and attitude control simulation, extended noise sources, more sophisticated and overlapping gravitational wave signals, second-generation time-delay interferometry and the coupling effect of time-varying armlengths, etc. Concurrently released is the open-source toolkit Triangle (available at this https URL), which offers the capabilities for customized simulation of signals, noises and other instrumental effects. By taking a step further towards realistic detection, Taiji Data Challenge II and Triangle altogether serve as a new testbed, supporting the development of Taiji's global analysis and end-to-end pipelines, and ultimately bridging the gaps between observation and scientific objectives.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_5: There has been a longstanding interest among statisticians and analysts for parametric probability distributions (e.g., the Pearson or the Johnson families) flexible enough to match arbitrarily shaped, non-normal data sets. The metalog family emerged in 2016 as a new alternative and has been widely adopted for its tractability and flexibility. Metalogs can closely approximate any continuous quantile function, creating adaptable data-driven probability distributions. However, key theoretical properties such as the number and location of modes, explicit expressions for the moments, and feasibility guarantees have remained largely unexplored. In this paper, we define the metalog 2.0, which uniquely assigns coefficients to avoid degeneracy. We establish the possible number of modes, compute their location, and derive explicit formulas for moments and partial expectations, enabling more precision in applications. A major challenge when fitting data is that the metalog function may fail to be monotonic, rendering it invalid. We provide a mathematically exact feasibility test and introduce an algorithm to find the best feasible metalog fit with arbitrary precision.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_6: As next-generation gravitational-wave (GW) observatories approach unprecedented sensitivities, the need for robust methods to analyze increasingly complex, overlapping signals becomes ever more pressing. Existing matched-filtering approaches and deep-learning techniques can typically handle only one or two concurrent signals, offering limited adaptability to more varied and intricate superimposed waveforms. To overcome these constraints, we present the UnMixFormer, an attention-based architecture that not only identifies the unknown number of concurrent compact binary coalescence GW events but also disentangles their individual waveforms through a multidecoder architecture, even when confronted with five overlapping signals. Our UnMixFormer is capable of capturing both short- and long-range dependencies by modeling them in a dual-path manner, while also enhancing periodic feature representation by incorporating Fourier analysis networks. Our approach adeptly processes binary black hole, binary neutron star, and neutron star–black hole systems over extended time series data (16,384 samples). When evaluating on synthetic data with signal-to-noise ratios (SNR) ranging from 10 to 50, our method achieves 99.89% counting accuracy, a mean overlap of 0.9831 between separated waveforms and templates, and robust generalization ability to waveforms with spin precession, orbital eccentricity, and higher modes, marking a substantial advance in the precision and versatility of GW data analysis.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_7: Gravitational waves (GWs) from coalescing binary black holes (BBHs) can come from different environments. GWs interact gravitationally with astrophysical objects, which makes it possible to use gravitational lensing by nearby objects (self-lensing) to learn about their environments. We quantify the probability of self-lensing through the optical depth τ for the main channels of detectable GWs at frequencies fGW ∼ (1 − 103)Hz. We then analyze the detectability of the lensing effect (imprint). In star clusters, the probability of self-lensing by stellar-mass black holes (BHs) is low (τ ≃ 10−7), even when taking into account nearby BHs in resonant interactions (τ ≃ 10−5). Additionally, the lensing imprint of a stellar-mass lens (diffraction and interference) is too marginal to be detectable by the LIGO-Virgo-KAGRA detectors and most Einstein Telescope signals. For a massive BH lens in the center of a cluster, the probability can reach τ ≃ 10−4 either via von Zeipel-Lidov-Kozai induced mergers of BBHs orbiting a central massive BH or BBHs formed as GW captures in single-single interactions in the Bahcall-Wolf cusp of a nuclear cluster, likely eccentric. For self-lensing by a supermassive BH for BBHs in the migration trap of an AGN (active galactic nucleus) disk is τ ≃ 10−2. The imprint of these massive lenses are two images that are easily detectable already in current detectors. Moreover, AGN disk merger signals have a distinct linear h+ polarization. The probability depends on the extent of the detectability through the threshold impact parameter ymax, which can increase for future detectors. We conclude that constraining the environment of BBHs is possible by combining self-lensing imprints with other waveform signatures such as eccentricity and polarization.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_8: We study the angular power spectrum of gravitational–wave and galaxy catalogs in tomographic redshift and distance bins as a probe of late–time cosmology, focusing specifically on next–generation ground–based interferometers in combination with the Euclid photometric survey. We assess the potential of this technique to constrain the Hubble constant and the matter energy density. Our analysis incorporates realistic gravitational–wave source populations, error modeling calibrated on recent detector designs, and accounts for nuisance parameters. We show that the tomographic angular cross–correlation could determine the Hubble constant to percent or sub-percent precision depending on the binning choice, configuration and operation time of gravitational–wave observatories. This conclusion holds even when marginalizing over the unknown tracer biases, primordial power–spectrum parameters and baryon density. In particular, we show that the combination of the galaxy auto–correlation spectra and the cross–correlation of gravitational waves and galaxy surveys can lead to an improvement of up to a factor ∼10 in constraining power over either of the two probes taken individually. However, this prospect crucially relies on the presence of multiple gravitational–wave interferometers able to yield precise sky localization. We also discuss the use of a spectroscopic redshift catalog, as well as the detectability of the clustering bias of gravitational–wave sources.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_9: Gravitational waves (GWs) provide independent distance measurements when used as standard sirens, but require additional redshift information in order to operate as cosmological probes; this type of information can be obtained from real galaxy catalogs via statistical association with potential hosts. The limitations of such catalogs in different sky regions, however, must be properly accounted for to ensure robust and unbiased results. This work focuses on the development of the necessary updates to the Bayesian framework in the CHIMERA pipeline to correctly model the incompleteness of a real galaxy catalog (GLADE+). The developed pipeline, together with galaxy luminosity weighting, are tested on a selection of events taken from the GWTC-3 catalog: two significant GW events are used to produce statistical inferences for the Hubble constant (H0), and a sample of 42 Binary Black Hole events is analyzed through a Markov Chain Monte Carlo simulation: this allows for a joint estimation of cosmological and astrophysical parameters. The Thesis highlights the need to account for galaxy catalog incompleteness in standard siren analyses and demonstrates the potential of the updated CHIMERA pipeline to perform cosmological analyses on real data, especially in anticipation of upcoming GW data from the LVK detector network.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_10: Gravitational waves (GWs) from compact binary mergers have emerged as one of the most promising probes of cosmology and General Relativity (GR). However, a major challenge in fully exploiting GWs as standard sirens with current and future GW observatories is developing efficient and robust codes capable of analyzing the increasing data volumes that are, and will be, acquired. We present here CHIMERA 2.0, an advanced computational framework for hierarchical Bayesian inference of cosmological, modified gravity, and population hyperparameters using standard sirens and galaxy catalogs. This upgrade introduces novel GPU-accelerated algorithms to estimate the hierarchical likelihood, enabling the analysis of thousands of events - crucial for next-generation experiments - and includes the two-parameter (Xi_0-n) modified GW propagation model. Using texttt{CHIMERA} 2.0, we forecast cosmological and modified GW propagation constraints for the future LIGO-Virgo-KAGRA O5 run. We analyze three binary black hole populations of 300 events at SNR>20, each with a different value of Xi_0: 0.6, 1 (corresponding to GR), and 1.8. Multiple analyses were performed each catalog, comprising a population of approximately 5000 events, thanks to texttt{CHIMERA} 2.0, which is 10-1000 times faster depending on the settings and catalog size. We jointly infer cosmological, modified GW propagation, and population hyperparameters. With spectroscopic galaxy catalogs, the fiducial Xi_0 is recovered with a precision of 22%, 7.5%, and 10% for Xi_0 = 0.6, 1, and 1.8, respectively; while the precision on H_0 is 2-7 times worse than when Xi_0 is not inferred. Finally,in the case of photometric redshifts the constraints degrade on average by 3.5 times in all cases, underscoring the importance of future spectroscopic surveys in maximizing the constraining power of standard sirens.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_11: The stochastic gravitational-wave background (SGWB) generated by the inspiral and merger of binary neutron stars is traditionally modelled assuming that the inspiral is promptly followed by the collapse of the merger remnant to a rotating black hole. While this is reasonable for the most massive binaries, it is not what is expected in general, as the remnant may survive for up to hundreds of milliseconds and radiate an amount of energy that is significantly larger than that lost during the whole inspiral. To account for this additional contribution to the SGWB, we consider a waveform model that includes both the inspiral and the postmerger emission. We show for the first time that for a large set of parameterized equations of state (EOSs) compatible with observational constraints, there is considerable spectral power in the 1-2,{rm kHz} range, distinct from that associated with the inspiral and leading to a dimensionless GW energy density Omega_{rm GW} simeq 10^{-10}-10^{-9}. We discuss the enhanced detectability of the SGWB by third-generation detectors such as the Einstein Telescope and Cosmic Explorer, and show how it depends on the signal-to-noise ratio of foreground binaries and on the remnant lifetime. Interestingly, even a non-detection of the high-frequency part of the SGWB could provide valuable constraints on the remnant lifetime, offering novel insights into the postmerger dynamics and the EOS of nuclear matter.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_12: We present SEOBNRv5THM, an accurate and fast gravitational-waveform model for quasi-circular, spinning, non-precessing binary neutron stars (BNS) within the effective-one-body (EOB) formalism. It builds on the binary-black-hole approximant SEOBNRv5HM and, compared to its predecessor SEOBNRv4T, it i) incorporates recent high-order post-Newtonian results in the inspiral, including higher-order adiabatic tidal contributions, spin-induced multipoles and dynamical tides for spin-aligned neutron stars, ii) includes the gravitational modes (ell, |m|)=(2,2),(3,3),(2,1),(4,4),(5,5),(3,2), and (4,3), iii) has a time of merger calibrated to BNS numerical-relativity (NR) simulations, iv) accurately models the pre-merger (2,2) mode through a novel phenomenological ansatz, and v) is 100 to 1000 times faster than its predecessor model for BNS systems with total mass M geq 2, M_odot. Thus, SEOBNRv5THM can be used in Bayesian parameter estimation, which we perform for two BNS events observed by the LIGO-Virgo Collaboration, GW170817 and GW190425. The model accurately reproduces BAM and SACRA NR waveforms with errors comparable to or lower than the intrinsic NR uncertainty. We validate the model against the other state-of-the-art BNS waveform models NRTidalv3 and TEOBResumS and find differences only for highly spinning and highly tidally deformable BNS, where there is no NR coverage and the models employ different spin prescriptions. Our model serves as a foundation for the development of subsequent SEOBNR waveform models with matter that incorporate further effects, such as spin-precession and eccentricity, to be employed for upcoming observing runs of the LIGO-Virgo-KAGRA Collaboration and future facilities on the ground.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_13: Binary black hole systems are typically assumed to evolve in vacuum. However, the environment surrounding the binary components can influence their properties, such as their tidal deformability, affecting the gravitational waveform produced by the binary and its interpretation in gravitational wave data analysis. In this work we focus on next-generation experiments, such as the Einstein Telescope and LISA, and we quantify the systematic biases in gravitational wave observations that arise when tidally deformed binaries are interpreted as occurring in vacuum. We consider binaries over a range of masses and we compare different phenomenological models for the dynamical evolution of the tidal deformability. We find that systematic biases could significantly affect the measurability of the binary parameters if tidal effects are not carefully modeled.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_14: The next generation of gravitational-wave observatories will achieve unprecedented strain sensitivities with an expanded observing band. They will detect  binary neutron star (BNS) mergers every year, the loudest of which will be in the band for  minutes with signal-to-noise ratios . Current techniques will not be able to determine the astrophysical parameters of the loudest of next-gen BNS signals. We show that subtleties arising from the rotation of the Earth and the free-spectral range of gravitational-wave interferometers dramatically increases the complexity of next-gen BNS signals compared to the one-minute signals seen by LIGO--Virgo. Various compression methods currently relied upon to speed up the most expensive BNS calculations -- reduced-order quadrature, multi-banding, and relative binning -- will no longer be effective. We carry out reduced-order inference on a simulated next-gen BNS signal taking into account the Earth's rotation and the observatories' free-spectral range. We show that standard data compression techniques become impractical, and the full problem becomes computationally infeasible, when we include data below Hz -- a part of the observing band that is critical for precise sky localisation. We discuss potential paths towards solving this complex problem.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_20_15: When a gravitational wave signal encounters a massive object, such as a galaxy or galaxy cluster, it undergoes strong gravitational lensing, producing multiple copies of the original signal. These strongly lensed signals exhibit identical waveform morphology in the frequency domain, allowing analysis without the need for complex lens models. However, stellar fields and dark matter substructures within the galactic lens introduce microlensing effects that alter individual signal morphologies. Identifying these microlensing signatures is computationally challenging within Bayesian frameworks. In this study, we propose a residual test to efficiently search for microlensing signatures by leveraging the fact that current Bayesian inference pipelines are optimized solely for the strong lensing hypothesis. Using cross-correlation techniques, we investigate the microlensing-induced deviations from the strong hypothesis, which are imprinted in the residuals. Most simulated signals from our realistic microlensing populations exhibit small mismatches between the microlensed and unlensed waveforms, but a fraction show significant deviations. We find that 28% (52%) and 34% (66%)of microlensed events with mismatch > 0.03 and > 0.1, respectively, can be discerned with O4 (O5) detector sensitivities, which demonstrates that high-mismatch events are more likely to be identified as microlensed. Including all events from a realistic population, 11% (21.5%) are identifiable with O4 (O5) sensitivity using our approach.","Main_20: We introduce $\texttt{GWFAST}$, a novel Fisher-matrix code for
gravitational-wave studies, tuned toward third-generation gravitational-wave
detectors such as Einstein Telescope (ET) and Cosmic Explorer (CE). We use it
to perform a comprehensive study of the capabilities of ET alone, and of a
network made by ET and two CE detectors, as well as to provide forecasts for
the forthcoming O4 run of the LVK collaboration. We consider binary neutron
stars, binary black holes and neutron star-black hole binaries, and compute
basic metrics such as the distribution of signal-to-noise ratio (SNR), the
accuracy in the reconstruction of various parameters (including distance, sky
localization, masses, spins and, for neutron stars, tidal deformabilities), and
the redshift distribution of the detections for different thresholds in SNR and
different levels of accuracy in localization and distance measurement. We
examine the expected distribution and properties of `golden events', with
especially large values of the SNR. We also pay special attention to the
dependence of the results on astrophysical uncertainties and on various
technical details (such as choice of waveforms, or the threshold in SNR), and
we compare with other Fisher codes in the literature. In a companion paper we
discuss the technical aspects of the code. Together with this paper, we
publicly release the code $\texttt{GWFAST}$ at
https://github.com/CosmoStatGW/gwfast, and the library $\texttt{WF4Py}$
implementing state-of-the-art gravitational-wave waveforms in pure
$\texttt{Python}$ at https://github.com/CosmoStatGW/WF4Py.
",0.75
"Cite_21_1: The concept of Cyber-Physical Systems (CPS) enables the creation of a complex network that includes sensors integrated into vehicles and infrastructure, facilitating seamless data acquisition and transfer. This review examines the convergence of CPS and Industry 4.0 in the smart transportation sector, highlighting their transformative impact on Intelligent Transportation Systems (ITS) operations. It explores the integration of Industry 4.0 and CPS technologies in intelligent transportation, highlighting their roles in enhancing efficiency, safety, and sustainability. A systematic framework is proposed for developing, implementing, and managing these technologies in the transportation industry. Moreover, the review discusses frequent obstacles during technology integration in transportation and presents future research trends and innovations in intelligent transportation operations post-Industry 4.0 and CPS integration. Lastly, it emphasizes the critical need for standardized protocols and encryption methodologies to enhance the security of communication and data exchange among CPS components in transportation infrastructure.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_2: he proposed paper examines enhancements in Visual Question Answering (VQA) by systematically tuning hyperparameters and utilizing advanced image and text encoders. The study particularly explores the adaptation of these models to Multiple-Choice Question (MCQ) formats, aiming to refine their accuracy and applicability. MCQs consist of a question stem along with a set of options, from which the correct answer, the key needs to be identified among the distractors. Using MCQs provides the model with some context of the correct answer, improving its performance over a simple multiclass classification task. The research showcases the effectiveness of precise hyperparameter adjustments in improving the performance of VQA systems, through comparative analysis of varied sets of hyperparameters, highlighting their improved reasoning capabilities across various datasets, including samples from real world images and academic questions. This demonstrates the potential of VQA models for robust application in both educational and practical scenarios.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_3: Objective: This review explores the trustworthiness of multimodal artificial intelligence (AI) systems, specifically focusing on vision-language tasks. It addresses critical challenges related to fairness, transparency, and ethical implications in these systems, providing a comparative analysis of key tasks such as Visual Question Answering (VQA), image captioning, and visual dialogue. Background: Multimodal models, particularly vision-language models, enhance artificial intelligence (AI) capabilities by integrating visual and textual data, mimicking human learning processes. Despite significant advancements, the trustworthiness of these models remains a crucial concern, particularly as AI systems increasingly confront issues regarding fairness, transparency, and ethics. Methods: This review examines research conducted from 2017 to 2024 focusing on forenamed core vision-language tasks. It employs a comparative approach to analyze these tasks through the lens of trustworthiness, underlining fairness, explainability, and ethics. This study synthesizes findings from recent literature to identify trends, challenges, and state-of-the-art solutions. Results: Several key findings were highlighted. Transparency: Explainability of vision language tasks is important for user trust. Techniques, such as attention maps and gradient-based methods, have successfully addressed this issue. Fairness: Bias mitigation in VQA and visual dialogue systems is essential for ensuring unbiased outcomes across diverse demographic groups. Ethical Implications: Addressing biases in multilingual models and ensuring ethical data handling is critical for the responsible deployment of vision-language systems. Conclusion: This study underscores the importance of integrating fairness, transparency, and ethical considerations in developing vision-language models within a unified framework.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_4: Visual reasoning refers to the task of solving questions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text. Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks. To address these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the question with underspecification. Additionally, we design a novel prompting method called Chain-of-Evidence (CoE), which leverages the power of ``evidence for reasoning'' to endow VIKSER with interpretable reasoning capabilities. Meanwhile, the integration of self-reflection technology empowers VIKSER with the ability to learn and improve from its mistakes. Experiments conducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_5: Visual Question Answering (VQA) is a complex task that requires a deep understanding of both visual content and natural language questions. The challenge lies in enabling models to recognize and interpret visual elements and to reason through questions in a multi-step, compositional manner. We propose a novel Transformer-based model that introduces specialized tokenization techniques to effectively capture intricate relationships between visual and textual features. The model employs an enhanced self-attention mechanism, enabling it to attend to multiple modalities simultaneously, while a co-attention unit dynamically guides focus to the most relevant image regions and question components. Additionally, a multi-step reasoning module supports iterative inference, allowing the model to excel at complex reasoning tasks. Extensive experiments on benchmark datasets demonstrate the model's superior performance, with accuracies of 98.6% on CLEVR, 63.78% on GQA, and 68.67% on VQA v2.0. Ablation studies confirm the critical contribution of key components, such as the reasoning module and co-attention mechanism, to the model's effectiveness. Qualitative analysis of the learned attention distributions further illustrates the model's dynamic reasoning process, adapting to task complexity. Overall, our study advances the adaptation of Transformer architectures for VQA, enhancing both reasoning capabilities and model interpretability in visual reasoning tasks.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_6: Visual Question Answering (VQA) presents a multifaceted challenge, requiring models to process and integrate both textual and visual data. The Transformer architecture, with its self-attention mechanism, has proven highly effective in managing global dependencies, making it a popular choice for many VQA models. However, a key challenge lies in achieving an optimal balance between global and local dependency modeling within conventional Transformer frameworks. Addressing this issue, our research introduces an innovative Transformer-based approach that employs region-specific self-attention for VQA. In contrast to models that concentrate exclusively on global dependencies, our method concurrently executes intra and inter-window attention operations to effectively capture contextual information from images. By utilizing regional windows for visual feature processing, our model eliminates redundancy in global self-attention while still preserving rich contextual details. The efficacy of our proposed technique, which relies on the utilization of visual features, has been extensively evaluated through experiments. These assessments were conducted using widely recognized VQA benchmark datasets, specifically VQA 2.0 and CLEVR. Our findings demonstrate that the model outperforms existing benchmark alternatives across various performance metrics. Notably, it achieved superior test results of 71.93% and 98.62% on the VQA 2.0 and CLEVR datasets, respectively, underscoring its effectiveness in tackling complex VQA tasks.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_7: Recent advances in graph convolutional networks (GCNs) have demonstrated their effectiveness in vision-language tasks such as visual question answering (VQA), primarily due to their ability to capture both spatial and semantic relationships. However, through a series of experiments applying GCNs to VQA tasks, we observe that the performance of GCNs is highly sensitive to various subtasks, often resulting in unstable outputs. This variability can be attributed to factors such as the size of the training and testing datasets, evaluation metrics, and the selection of hyperparameters. To ensure fair evaluations of GCN-based VQA models, it is essential to implement consistent evaluation schemes across these factors. In this study, we propose a GCN framework for VQA that leverages fine-tuned word representations from the GloVe model, specifically tailored to address reasoning-based questions. We comprehensively assess the performance of our framework using multiple evaluation metrics on both the GQA and VQA 2.0 datasets. Our experimental results demonstrate significant improvements over existing methods, showcasing the effectiveness of the proposed approach in evaluation setting.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_8: This paper presents a novel approach to enhancing Visual Question Answering (VQA) systems in the Vietnamese language by integrating La rge Language Models (LLMs) with Optical Character Recognition (OCR) technology. VQA systems are designed to understand and answer questions based on visual content, but their performance may not be good when dealing with textual information embedded in images. To address this challenge, we propose a hybrid framework that leverages the strengths of LLMs in natural language understanding and generation, alongside OCR systems for accurate text extraction from images. Our approach involves the preprocessing of images through OCR to extract textual data, which is then combined with visual features and processed by an LLM to generate more accurate and contextually relevant answers. We conducted extensive experiments on a Vietnamese VQA dataset to evaluate the effectiveness of our method. The results demonstrate significant improvements in the accuracy of the answer and contextual understanding, with a 2% increase in the BLEU score and a 1.11 increase in the CIDEr score compared to the state-of-the-art VQA models.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_9: Business intelligence (BI) is crucial for informed decision-making, optimizing operations, and gaining a competitive edge. The rapid growth of unstructured text data has created a need for advanced text analysis techniques in BI. Natural language processing (NLP) is essential for analyzing unstructured textual data. This chapter covers foundational NLP techniques for text analysis, the role of text analysis in BI, and challenges and opportunities in this area. Real-world applications of NLP in BI demonstrate how organizations use NLP-driven text analysis to gain insights, improve customer experience, and anticipate market trends. Future directions and emerging trends, including multimodal learning, contextualized embeddings, conversational AI, explainable AI, federated learning, and knowledge graph integration, were explored. These advancements enhance the scalability, interpretability, and privacy of NLP-driven BI systems, enabling organizations to derive deeper insights and drive innovation in data-driven business landscapes.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_10: Recent advancements have introduced multiple vision-language models (VLMs) demonstrating impressive commonsense reasoning across various domains. Despite their individual capabilities, the potential of synergizing these complementary VLMs remains underexplored. The Cola Framework addresses this by showcasing how a large language model (LLM) can efficiently coordinate multiple VLMs through natural language communication, leveraging their distinct strengths. We have verified this claim on the challenging A-OKVQA dataset, confirming the effectiveness of such coordination. Building on this, our study investigates whether the same methodology can be applied to surveillance videos for action recognition. Specifically, we explore if leveraging the combined knowledge base of VLMs and LLM can effectively deduce actions from a video when presented with only a few selectively important frames and minimal temporal information. Our experiments demonstrate that LLM, when coordinating different VLMs, can successfully recognize patterns and deduce actions in various scenarios despite the weak temporal signals. However, our findings suggest that to enhance this approach as a viable alternative solution, integrating a stronger temporal signal and exposing the models to slightly more frames would be beneficial.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_21_11: Visual Language Models (VLMs) are essential for various tasks, particularly visual reasoning tasks, due to their robust multi-modal information integration, visual reasoning capabilities, and contextual awareness. However, existing VLMs{}' visual spatial reasoning capabilities are often inadequate, struggling even with basic tasks such as distinguishing left from right. To address this, we propose the ours{} model, designed to enhance the visual spatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D reconstruction model for obtaining different views of the input images and incorporates a prompting mechanism to further improve visual spatial reasoning. Experimental results on four visual spatial reasoning datasets show that our ours{} achieves up to 19.48% accuracy improvement, which indicates the effectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.","Main_21: Artificial Intelligence (AI) and its applications have sparked extraordinary
interest in recent years. This achievement can be ascribed in part to advances
in AI subfields including Machine Learning (ML), Computer Vision (CV), and
Natural Language Processing (NLP). Deep learning, a sub-field of machine
learning that employs artificial neural network concepts, has enabled the most
rapid growth in these domains. The integration of vision and language has
sparked a lot of attention as a result of this. The tasks have been created in
such a way that they properly exemplify the concepts of deep learning. In this
review paper, we provide a thorough and an extensive review of the state of the
arts approaches, key models design principles and discuss existing datasets,
methods, their problem formulation and evaluation measures for VQA and Visual
reasoning tasks to understand vision and language representation learning. We
also present some potential future paths in this field of research, with the
hope that our study may generate new ideas and novel approaches to handle
existing difficulties and develop new applications.
",0.55
"Cite_22_1: We compute the Euler characteristics of tautological vector bundles and their exterior powers over the Quot schemes of curves. We give closed-form expressions over punctual Quot schemes in all genera. For higher rank quotients of a trivial vector bundle, we obtain answers in genus 0. We also study the Euler characteristics of the symmetric powers of the tautological bundles, for rank 0 quotients.","Main_22:   Let $\mathcal{E}$ be a locally free sheaf of rank $r$ on a smooth projective
surface $S$. The Quot scheme $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ of length $l$
coherent sheaf quotients of $\mathcal{E}$ is a natural higher rank
generalization of the Hilbert scheme of $l$ points of $S$. We study the virtual
intersection theory of this scheme. If $C\subset S$ is a smooth canonical
curve, we use cosection localization to show that the virtual fundamental class
of $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ is $(-1)^{l}$ times the fundamental
class of the smooth subscheme
$\mathrm{Quot}^{l}_{C}(\mathcal{E}\vert_{C})\subset\mathrm{Quot}^{l}_{S}(\mathcal{E})$.
We then prove a structure theorem for virtual tautological integrals over
$\mathrm{Quot}^{l}_{S}(\mathcal{E})$. From this we deduce, among other things,
the equality of virtual Euler characteristics
$\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{E}))=\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{O}^{\oplus
r}))$.
",0.2
"Cite_22_2: Let T := Gdm bethetorusacting on the Quot scheme of points nQuotn Or/Ad/Z via the standard action on Ad. We analyze the fixed locus of the Quot scheme under this action. In particular we show that for d ≤ 2 or r ≤ 2, this locus is smooth, and that for d ≥ 4 and r ≥ 3 it satisfies Murphy’s law as introduced by Vakil, meaning that it has arbitrarily bad singularities. These results are obtained by giving a decomposition of the fixed locus into connected components, and identifying the components with incidence schemes of subspaces of Pr−1. We then obtain a characterization of the incidence schemes which occur, in terms of their graphs of incidence relations.","Main_22:   Let $\mathcal{E}$ be a locally free sheaf of rank $r$ on a smooth projective
surface $S$. The Quot scheme $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ of length $l$
coherent sheaf quotients of $\mathcal{E}$ is a natural higher rank
generalization of the Hilbert scheme of $l$ points of $S$. We study the virtual
intersection theory of this scheme. If $C\subset S$ is a smooth canonical
curve, we use cosection localization to show that the virtual fundamental class
of $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ is $(-1)^{l}$ times the fundamental
class of the smooth subscheme
$\mathrm{Quot}^{l}_{C}(\mathcal{E}\vert_{C})\subset\mathrm{Quot}^{l}_{S}(\mathcal{E})$.
We then prove a structure theorem for virtual tautological integrals over
$\mathrm{Quot}^{l}_{S}(\mathcal{E})$. From this we deduce, among other things,
the equality of virtual Euler characteristics
$\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{E}))=\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{O}^{\oplus
r}))$.
",0.2
"Cite_22_3: In analogy to Nekrasov's theory of gauge origami on intersecting branes, we introduce the gauge origami moduli space on broken lines. We realize this moduli space as a Quot scheme parametrising zero-dimensional quotients of a torsion sheaf on two intersecting affine lines, and describe it as a moduli space of quiver representations. We construct a virtual fundamental class and virtual structure sheaf, by which we define K-theoretic invariants. We compute its associated partition function for all ranks, and show that it reproduces the generating series of equivariant chi_{y}-genus when the moduli space is smooth. Finally, we relate our partition function with the virtual invariants of the Quot schemes of the affine plane and Nekrasov's partition function.","Main_22:   Let $\mathcal{E}$ be a locally free sheaf of rank $r$ on a smooth projective
surface $S$. The Quot scheme $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ of length $l$
coherent sheaf quotients of $\mathcal{E}$ is a natural higher rank
generalization of the Hilbert scheme of $l$ points of $S$. We study the virtual
intersection theory of this scheme. If $C\subset S$ is a smooth canonical
curve, we use cosection localization to show that the virtual fundamental class
of $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ is $(-1)^{l}$ times the fundamental
class of the smooth subscheme
$\mathrm{Quot}^{l}_{C}(\mathcal{E}\vert_{C})\subset\mathrm{Quot}^{l}_{S}(\mathcal{E})$.
We then prove a structure theorem for virtual tautological integrals over
$\mathrm{Quot}^{l}_{S}(\mathcal{E})$. From this we deduce, among other things,
the equality of virtual Euler characteristics
$\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{E}))=\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{O}^{\oplus
r}))$.
",0.2
"Cite_22_4: We construct an almost perfect obstruction theory of virtual dimension zero on the Quot scheme parametrizing zero-dimensional quotients of a locally free sheaf on a smooth projective 3-fold. This gives a virtual class in degree zero and therefore allows one to define virtual invariants of the Quot scheme. We compute these invariants proving a conjecture by Ricolfi. The computation is done by reducing to the toric case via cobordism theory and a degeneration argument. The toric case is solved by reducing to the computation on the Quot scheme of points on mathbb{A}^3 via torus localization, the torus-equivariant Siebert's formula for almost perfect obstruction theories and the torus-equivariant Jouanolou trick.","Main_22:   Let $\mathcal{E}$ be a locally free sheaf of rank $r$ on a smooth projective
surface $S$. The Quot scheme $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ of length $l$
coherent sheaf quotients of $\mathcal{E}$ is a natural higher rank
generalization of the Hilbert scheme of $l$ points of $S$. We study the virtual
intersection theory of this scheme. If $C\subset S$ is a smooth canonical
curve, we use cosection localization to show that the virtual fundamental class
of $\mathrm{Quot}^{l}_{S}(\mathcal{E})$ is $(-1)^{l}$ times the fundamental
class of the smooth subscheme
$\mathrm{Quot}^{l}_{C}(\mathcal{E}\vert_{C})\subset\mathrm{Quot}^{l}_{S}(\mathcal{E})$.
We then prove a structure theorem for virtual tautological integrals over
$\mathrm{Quot}^{l}_{S}(\mathcal{E})$. From this we deduce, among other things,
the equality of virtual Euler characteristics
$\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{E}))=\chi^{\mathrm{vir}}(\mathrm{Quot}^{l}_{S}(\mathcal{O}^{\oplus
r}))$.
",0.2
"Cite_23_1: Cyber-physical systems (CPS) are required to satisfy safety constraints in various application domains such as robotics, industrial manufacturing systems, and power systems. Faults and cyber attacks have been shown to cause safety violations, which can damage the system and endanger human lives. Resilient architectures have been proposed to ensure safety of CPS under such faults and attacks via methodologies including redundancy and restarting from safe operating conditions. The existing resilient architectures for CPS utilize different mechanisms to guarantee safety, and currently, there is no common framework to compare them. Moreover, the analysis and design undertaken for CPS employing one architecture is not readily extendable to another. In this article, we propose a timing-based framework for CPS employing various resilient architectures and develop a common methodology for safety analysis and computation of control policies and design parameters. Using the insight that the cyber subsystem operates in one out of a finite number of statuses, we first develop a hybrid system model that captures CPS adopting any of these architectures. Based on the hybrid system, we formulate the problem of joint computation of control policies and associated timing parameters for CPS to satisfy a given safety constraint and derive sufficient conditions for the solution. Utilizing the derived conditions, we provide an algorithm to compute control policies and timing parameters relevant to the employed architecture. We also note that our solution can be applied to a wide class of CPS with polynomial dynamics and also allows incorporation of new architectures. We verify our proposed framework by performing a case study on adaptive cruise control of vehicles.","Main_23:   Cyber-physical systems (CPS) are required to operate safely under fault and
malicious attacks. The simplex architecture and the recently proposed cyber
resilient architectures, e.g., Byzantine fault tolerant++ (BFT++), provide
safety for CPS under faults and malicious cyber attacks, respectively. However,
these existing architectures make use of different timing parameters and
implementations to provide safety, and are seemingly unrelated. In this paper,
we propose an analytical framework to represent the simplex, BFT++ and other
practical cyber resilient architectures (CRAs). We construct a hybrid system
that models CPS adopting any of these architectures. We derive sufficient
conditions via our proposed framework under which a control policy is
guaranteed to be safe. We present an algorithm to synthesize the control
policy. We validate the proposed framework using a case study on lateral
control of a Boeing 747, and demonstrate that our proposed approach ensures
safety of the system.
",0.15
"Cite_23_2: Cyber-physical systems (CPS) are used in various safety-critical domains such as robotics, industrial manufacturing systems, and power systems. Faults and cyber attacks have been shown to cause safety violations, which can damage the system and endanger human lives. Traditional resiliency techniques fall short of protecting against cyber threats. In this article, we show how to extend resiliency to cyber resiliency for CPS using a specific combination of diversification, redundancy, and the physical inertia of the system.","Main_23:   Cyber-physical systems (CPS) are required to operate safely under fault and
malicious attacks. The simplex architecture and the recently proposed cyber
resilient architectures, e.g., Byzantine fault tolerant++ (BFT++), provide
safety for CPS under faults and malicious cyber attacks, respectively. However,
these existing architectures make use of different timing parameters and
implementations to provide safety, and are seemingly unrelated. In this paper,
we propose an analytical framework to represent the simplex, BFT++ and other
practical cyber resilient architectures (CRAs). We construct a hybrid system
that models CPS adopting any of these architectures. We derive sufficient
conditions via our proposed framework under which a control policy is
guaranteed to be safe. We present an algorithm to synthesize the control
policy. We validate the proposed framework using a case study on lateral
control of a Boeing 747, and demonstrate that our proposed approach ensures
safety of the system.
",0.15
"Cite_23_3: Threshold cryptosystems (TCs), developed to eliminate single points of failure in applications such as key management-as-a-service, signature schemes, encrypted data storage and even blockchain applications, rely on the assumption that an adversary does not corrupt more than a fixed number of nodes in a network. This assumption, once broken, can lead to the entire system being compromised. In this paper, we present a systems-level solution, viz., a reboot-based framework, Groundhog, that adds a layer of resiliency on top of threshold cryptosystems (as well as others); our framework ensures the system can be protected against malicious (mobile) adversaries that can corrupt up all but one device in the network. Groundhog ensures that a sufficient number of honest devices is always available to ensure the availability of the entire system. Our framework is general-izable to multiple threshold cryptosystems - we demonstrate this by integrating it with two well-known TC protocols - the Distributed Symmetric key Encryption system (DiSE) and the Boneh, Lynn and Shacham Distributed Signatures (BLS) system. In fact, Groundhog may have applicability in systems beyond those based on threshold cryptography - we demonstrate this on a simpler cryptographic protocol that we developed named PassAround11In fact, this protocol was suggested by a USENIX Security reviewer that we then refined, implemented and evaluated in conjunction with Groundhog (see §6). . We developed a (generalizable) container-based framework that can be used to combine Groundhog (and its guarantees) with cryptographic protocols and evaluated our system using, (a) case studies of real world attacks as well as (b) extensive measurements by implementing the aforementioned DiSE, BLS and PassAround protocols on Groundhog. We show that Groundhog is able to guarantee high availability with minimal overheads (less than 7%). In some instances, Groundhog actually improves the performance of the TC schemes!22While it seems counter-intuitive, we explain the reasoning in §5.","Main_23:   Cyber-physical systems (CPS) are required to operate safely under fault and
malicious attacks. The simplex architecture and the recently proposed cyber
resilient architectures, e.g., Byzantine fault tolerant++ (BFT++), provide
safety for CPS under faults and malicious cyber attacks, respectively. However,
these existing architectures make use of different timing parameters and
implementations to provide safety, and are seemingly unrelated. In this paper,
we propose an analytical framework to represent the simplex, BFT++ and other
practical cyber resilient architectures (CRAs). We construct a hybrid system
that models CPS adopting any of these architectures. We derive sufficient
conditions via our proposed framework under which a control policy is
guaranteed to be safe. We present an algorithm to synthesize the control
policy. We validate the proposed framework using a case study on lateral
control of a Boeing 747, and demonstrate that our proposed approach ensures
safety of the system.
",0.15
"Cite_24_1: Interconnected systems such as power systems and chemical processes are often required to satisfy safety properties in the presence of faults and attacks. Verifying safety of these systems, however, is computationally challenging due to nonlinear dynamics, high dimensionality, and combinatorial number of possible faults and attacks that can be incurred by the subsystems interconnected within the network. In this paper, we develop a compositional resilience index to verify safety properties of interconnected systems under faults and attacks. The resilience index is a tuple serving the following two purposes. First, it quantifies how a safety property is impacted when a subsystem is compromised by faults and attacks. Second, the resilience index characterizes the needed behavior of a subsystem during normal operations to ensure safety violations will not occur when future adverse events occur. We develop a set of sufficient conditions on the dynamics of each subsystem to satisfy its safety constraint, and leverage these conditions to formulate an optimization program to compute the resilience index. When multiple subsystems are interconnected and their resilience indices are given, we show that the safety constraints of the interconnected system can be efficiently verified by solving a system of linear inequalities. We demonstrate our developed resilience index using a numerical case study on chemical reactors connected in series.","Main_24:   Complex, interconnected Cyber-physical Systems (CPS) are increasingly common
in applications including smart grids and transportation. Ensuring safety of
interconnected systems whose dynamics are coupled is challenging because the
effects of faults and attacks in one sub-system can propagate to other
sub-systems and lead to safety violations. In this paper, we study the problem
of safety-critical control for CPS with coupled dynamics when some sub-systems
are subject to failure or attack. We first propose resilient-safety indices
(RSIs) for the faulty or compromised sub-systems that bound the worst-case
impacts of faulty or compromised sub-systems on a set of specified safety
constraints. By incorporating the RSIs, we provide a sufficient condition for
the synthesis of control policies in each failure- and attack- free
sub-systems. The synthesized control policies compensate for the impacts of the
faulty or compromised sub-systems to guarantee safety. We formulate
sum-of-square optimization programs to compute the RSIs and the safety-ensuring
control policies. We present a case study that applies our proposed approach on
the temperature regulation of three coupled rooms. The case study demonstrates
that control policies obtained using our algorithm guarantee system's safety
constraints.
",0.05
"Cite_25_1: Non-Hermitian quantum systems exhibit various interesting and inter-connected spectral, topological, and boundary-sensitive features. By introducing conditional boundary conditions (CBCs) for non-Hermitian quantum systems, we explore a winding-control mechanism that selectively collapses specific periodic boundary condition (PBC) spectra onto their open boundary condition (OBC) counterparts, guided by their specific winding numbers, together with a composite reconstruction of the Brillouin zone (BZ) and generalized Brillouin zone (GBZ). The corresponding eigenstates also manifest nontrivial skin effects or extended behaviors arising from the interplay between BZ and GBZ structures. Furthermore, we can generalize our control by incorporating similarity transformations and holomorphic mappings with the boundary controls. We demonstrate the winding control numerically within various models, which enriches our knowledge of non-Hermitian physics across the spectrum, topology, and bulk-boundary correspondence.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_2: Non-equilibrium dynamics in non-Hermitian systems has attracted significant interest, particularly due to the skin effect and its associated anomalous phenomena. Previous studies have primarily focused on initial states with a definite particle number. Here, we present a systematic study of non-reciprocal quench dynamics in the pairing states with indefinite particle number. Our study uncovers a range of novel behaviors. Firstly, we demonstrate a universal tendency towards half-filling of particle density at late times. At early times for certain initial states, we observe a chiral wavefront in both particle number distribution and charge inflow, associated with a sharp decrease in particle number. Furthermore, we find that non-Hermiticity could enhance the growth of entanglement in the initial stages of evolution. In the intermediate time regime, the characteristic skin effect leads to particle accumulation on one side, leading to a pronounced reduction in entanglement entropy. Moreover, our results reveal the presence of the quantum Mpemba effect during the restoration of U(1) symmetry. Our findings open new avenues for exploring exotic dynamic phenomena in quantum many-body systems arising from the interplay of symmetry breaking and non-Hermiticity.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_3: The interplay of non-Hermiticity and spin-orbit (SO) coupling has attracted great interest in condensed-matter physics, ultracold atomic physics, optics, and other fields. Here we study the ground states, collective modes, and free-expansion dynamics of non-Hermitian SO-coupled Bose-Einstein condensates (BECs) with momentum-dependent gain and loss in a harmonic trap. An efficient analytical method for revealing the impact of non-Hermiticity on the ground states and dynamics of SO-coupled BECs is proposed. A dispersion relation with effective mass 𝑚2 that can be larger than 1, less than 1, or negative is engineered by the momentum-dependent gain and loss, which is distinct from the ordinary SO-coupled BECs, where 𝑚2>1 always holds. Negative effective mass can lead to the self-interference effect of condensates, while 𝑚2<1 means that condensates can gain a larger acceleration when they are subjected to a constant force. Interestingly, the high-order zero-point kinetic energies are derived analytically and confirmed numerically, establishing a link between the zero-point kinetic energy and dispersion relation. In particular, the second-order zero-point kinetic energy can be controlled by the effective mass 𝑚2, which has an important impact on many phenomena of ultracold atomic physics. In addition, another key result is that the frequencies of dipole and breathing modes can exceed the values of traditional BECs without SO coupling when the effective mass 𝑚2<1, showing that gain (loss) is a powerful tool for probing and controlling quantum systems. Moreover, the asymmetric free expansion and self-interference effect of the condensates caused by negative effective masses are observed, and it is found that the rate of expansion is controlled by the effective mass 𝑚2. Our results show that the controllable gain-loss engineering in SO-coupled BECs provides a prospect to explore novel quantum many-body physics.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_4: We present a mechanism to generate unidirectional pulse-shaped propagating waves, tamed to exponential growth and dispersion, in active systems with nonreciprocal and nonlinear couplings. In particular, when all bulk modes are exponentially localized at one side of the lattice (skin effect), it is expected that wave dynamics is governed by amplification or decay until reaching the boundaries, even in the presence of dissipation. Our analytical results, and experimental demonstrations in an active electrical transmission line metamaterial, reveal that nonlinearity is a crucial tuning parameter in mediating a delicate interplay between nonreciprocity, dispersion, and dissipation. Consequently, undistorted unidirectional solitonic pulses are supported both for low and high nonreciprocity and pulse amplitude strength. The proposed mechanism facilitates robust pulse propagation in signal processing and energy transmission applications.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_5: The interplay among interaction, non-Hermiticity, and disorder opens a new avenue for engineering novel phase transitions. We study here the spectral and localization features of two interacting bosons in one-dimensional nonreciprocal quasicrystals. Specifically, by considering a quasiperiodic Hubbard lattice with nonreciprocal hoppings, we show that the interaction can lead to a mobility edge, which arises from the fact that the bound states display a much lower threshold for spectral and extended-localized transitions than scattering states. The localization transition of bound or scattering states is accompanied by a complex-real spectrum transition. Moreover, while the two-particle localized states are robust to the boundary conditions, the two-particle extended states turn into skin modes under open boundary conditions. We also show the correlated dynamics to characterize these localization transitions. Finally, we reveal that the bound states can form a mobility edge on their own by introducing a dimerized nonreciprocal quasicrystal. Our paper may pave the way for the study of non-Hermitian few-body physics.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_6: Non-Hermitian band structures have gained considerable attention due to the novel phenomena not present in their Hermitian counterparts and their connection to various branches of mathematics such as topology and complex analysis. The study of such band structures may also find applications in laser design and in sensing. The spectra and eigenmode characteristics of extended non-Hermitian systems depend strongly on the boundary conditions. With periodic boundary conditions, the spectra can become complex, leading to band winding on the complex frequency plane. With open boundary conditions, the eigenmodes have spatial profiles that are localized at the boundary, an effect known as the non-Hermitian skin effect. Here we provide an overview of the band winding and skin effects in non-Hermitian photonics bands, focusing on one-dimensional cases and photonic applications. We aim to provide a detailed, consistent, and unifying treatment of various phenomena associated with non-Hermitian band structures.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_25_7: The Liouvillian skin effect describes the boundary affinity of Liouvillian eignemodes that originates from the intrinsic non-Hermiticity of the Liouvillian superoperators. Dynamically, it manifests as directional flow in the transient dynamics, and the accumulation of population near open boundaries at long times. Intriguingly, similar dynamic phenomena exist in the well-known process of optical pumping, where the system is driven into a desired state (or a dark-state subspace) through the interplay of dissipation and optical drive. In this work, we show that typical optical pumping processes can indeed be understood in terms of the Liouvillian skin effect. By studying the Liouvillian spectra under different boundary conditions, we reveal that the Liouvillian spectra of the driven-dissipative pumping process sensitively depend on the boundary conditions in the state space, a signature that lies at the origin of the Liouvillian skin effect. Such a connection provides insights and practical means for designing efficient optical pumping schemes through engineering Liouvillian gaps under the open boundary condition. Based on these understandings, we show that the efficiency of a typical sideband cooling scheme for trapped ions can be dramatically enhanced by introducing counterintuitive dissipative channels. Our results provide a useful perspective for optical pumping, with interesting implications for state preparation and cooling.","Main_25: Non-Hermitian skin effect, which refers to the phenomenon that an extensive
number of eigenstates are localized at the boundary, has been widely studied in
lattice models and experimentally observed in several classical systems. In
this work, we predict that the existence of the non-Hermitian skin effect in
the dissipative ultracold fermions with spin-orbit coupling, a continuous model
that has been implemented by the Hong-Kong group in a recent experiment. This
skin effect is robust against the variation of external parameters and trapping
potentials. We further reveal a dynamic sticky effect in our system, which has
a common physical origin with the non-Hermitian skin effect. Our work paves the
way for studying novel physical responses of non-Hermitian skin effect in
quantum systems.
",0.35
"Cite_26_1: Ordinal patterns have proven to be a valuable tool in many fields. Here, we address the need for theoretical models. A paradigmatic example shows that a model for frequencies of ordinal patterns can be determined without any numerical values. We specify the important concept of stationary order and the fundamental problems to be solved in order to establish a genuine statistical methodology for ordinal time series.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_2: Imaging time series have become an important method for constructing pattern features. The idea that transforming the original 1D time series into a 2D image makes properties from different levels in a spanned space. It realizes to analyze short-term autoregression and long-term relations of points at the same time. In this work, an image-time-series-based attention scheme has been investigated by high-frequency forecasting tasks with S&P500 and CSI300 datasets. By comparing non-parametric models and parametric models, as well as deep learning models, the proposed methods take the least residual. The MSE of the model in predicting the next 15 min’ logarithmic indices is 0.0038, and the MSPE value is 0.0001. Besides, the pattern-based CNN model has the fastest convergence rate when predicting the volatility at the same accuracy level. It verifies that the image representation of time series makes the potential information more efficient in analyzing financial time series.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_3: One of the most popular and innovative methods to analyse signals is by using Ordinal Patterns (OPs). The OP encoding is based on transforming a (univariate) signal into a symbolic sequence of OPs, where each OP represents the number of permutations needed to order a small subset of the signal's magnitudes. This implies that OPs are conceptually clear, methodologically simple to implement, robust to noise, and can be applied to short signals. Moreover, they simplify the statistical analyses that can be carried out on a signal, such as entropy and complexity quantifications. However, because of the relative ordering, information about the magnitude of the signal at each timestamp is lost -- this being one of the major drawbacks in the method. Here, we propose a way to use the signal magnitudes discarded in the OP encoding as a complementary variable to its permutation entropy. To illustrate our approach, we analyse synthetic trajectories from logistic and H{é}non maps -- with and without added noise -- and intracranial electroencephalographic recordings from rats in different sleep-wake states. Our results show that, when complementing the permutation entropy with the variability in the signal magnitudes, the characterisation of the dynamical behaviours of the maps and the sleep-wake states is improved. This implies that our approach can be useful for feature engineering and improving AI classifiers, where typical machine learning algorithms need complementary signal features as inputs to improve classification accuracy.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_4: The ordinal patterns of a fixed number of consecutive values in a time series is the spatial ordering of these values. Counting how often a specific ordinal pattern occurs in a time series provides important insights into the properties of the time series. In this work, we prove the asymptotic normality of the relative frequency of ordinal patterns for time series with linear increments. Moreover, we apply ordinal patterns to detect changes in the distribution of a time series.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_5: This paper explores the effectiveness of using ordinal pattern probabilities to evaluate antipersistency in the sign decomposition of long-range anti-correlated Gaussian fluctuations. It is numerically shown that ordinal patterns are able to effectively measure both persistent and antipersistent dynamics by analyzing the sign decomposition derived from fractional Gaussian noise. These findings are crucial given that traditional methods such as Detrended Fluctuation Analysis are unsuccessful in detecting anti-correlations in such sequences. The numerical results are supported by physiological and environmental data, illustrating its applicability in real-world situations.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_6: The relation between electroencephalography (EEG) rhythms, brain functions, and behavioral correlates is well-established. Some physiological mechanisms underlying rhythm generation are understood, enabling the replication of brain rhythms in silico. This offers a pathway to explore connections between neural oscillations and specific neuronal circuits, potentially yielding fundamental insights into the functional properties of brain waves. Information theory frameworks, such as Integrated Information Decomposition (Φ-ID), relate dynamical regimes with informational properties, providing deeper insights into neuronal dynamic functions. Here, we investigate wave emergence in an excitatory/inhibitory (E/I) balanced network of integrate and fire neurons with short-term synaptic plasticity. This model produces a diverse range of EEG-like rhythms, from low δ waves to high-frequency oscillations. Through Φ-ID, we analyze the network’s information dynamics and its relation with different emergent rhythms, elucidating the system’s suitability for functions such as robust information transfer, storage, and parallel operation. Furthermore, our study helps to identify regimes that may resemble pathological states due to poor informational properties and high randomness. We found, e.g., that in silico β and δ waves are associated with maximum information transfer in inhibitory and excitatory neuron populations, respectively, and that the coexistence of excitatory θ, α, and β waves is associated to information storage. Additionally, we observed that high-frequency oscillations can exhibit either high or poor informational properties, potentially shedding light on ongoing discussions regarding physiological versus pathological high-frequency oscillations. In summary, our study demonstrates that dynamical regimes with similar oscillations may exhibit vastly different information dynamics. Characterizing information dynamics within these regimes serves as a potent tool for gaining insights into the functions of complex neuronal networks. Finally, our findings suggest that the use of information dynamics in both model and experimental data analysis, could help discriminate between oscillations associated with cognitive functions and those linked to neuronal disorders.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_26_7: Ordinal pattern analysis has a wide spectrum of applications which a novice to the field can hardly keep track of. The applications range from entropy concepts to estimators of the Hurst parameter and also include various dependence measures. In the present article, we give an overview of the applications we are aware of and we provide the reader with literature to further deepen their knowledge.","Main_26: Order patterns and permutation entropy have become useful tools for studying
biomedical, geophysical or climate time series. Here we study day-to-day market
data, and Brownian motion which is a good model for their order patterns. A
crucial point is that for small lags (1 up to 6 days), pattern frequencies in
financial data remain essentially constant. The two most important order
parameters of a time series are turning rate and up-down balance. For change
points in EEG brain data, turning rate is excellent while for financial data,
up-down balance seems the best. The fit of Brownian motion with respect to
these parameters is tested, providing a new version of a forgotten test by
Bienaym'e.
",0.35
"Cite_27_1: We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Furthermore, after training, the proposed approach requires significantly less effort in terms of implementation and computing time compared to the traditional methods. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. We tested this methodology on rate-independent processes such as the classical von Mises plasticity model with a nonlinear hardening law, as well as local damage models for interface cracking behavior with a nonlinear softening law. In order to demonstrate the applicability of the methodology in handling complex path dependency in a three-dimensional (3D) scenario, we tested the approach using the equations governing a damage model for a three-dimensional interface model. Such models are frequently employed for intergranular fracture at grain boundaries. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. As long as we are in the training regime, we have observed a perfect agreement between the results obtained through the proposed methodology and those obtained using the classical approach. Finally, we compare this new approach against available standard methods and discuss the potential and remaining challenges for future developments.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_2: In this paper, we presented three neural network models including deep neural network (DNN), recurrent neural network (RNN), and long short-term memory neural network (LSTM), which are proposed to predict the cohesive zone parameter of sintered silver DCB joint with different contents of nickel modified carbon nanotube, thus reducing the complexity of CZM parameter acquisition for nanoparticle reinforced adhesive. The bilinear CZM model is used as the prediction target model for sintered silver joints with different contents of nickel-modified carbon nanotube filler, and data sets suitable for different networks are established through experimental and numerical simulation results. Three kinds of networks are trained based on the optimized hyperparameters obtained from the Bayesian hyperparameters tuning process. The results show that DNN, RNN, and LSTM frameworks can all predict CZM parameters of nanoparticle-reinforced sintered silver adhesive through load-displacement curves. Based on loss analysis and statistical indicator comparison after K-fold cross-validation, the RNN and LSTM models have better prediction accuracy and performance than the DNN model, and the accuracy of the LSTM model is further improved compared with the DNN model. RNN and LSTM models have high prediction accuracy and stronger recognition ability for the time series data, they can be used as suitable alternative models for inverse recognition of CZM parameters of nanoparticle-reinforced adhesives, and have broad application prospects.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_3: Composite solid propellant is a kind of viscoelastic composite with high filling ratio and multi-scale composition characteristics, and its macroscopic mechanical properties strongly depend on the microstructure of the propellant materia. However, with the increasing complexity of composition, structure and properties of composite solid propellants, the traditional research paradigm based on experimental observation, theoretical modeling and numerical simulation has encountered new scientific challenges and technical bottlenecks in the mechanical behavior analysis, charge design and manufacturing of composite solid propellants. Among them, the problems such as insufficient experimental observation, lack of theoretical model, limited numerical analysis and difficult verification of results restrict the development of composite solid propellants in future-oriented engineering applications to a certain extent. The data-driven computational mechanics method can directly establish complex relationships between variables from high-dimensional and high-throughput data, which can capture trends that are difficult to be found by traditional mechanics research methods, and has inherent advantages in the analysis, prediction and optimization of complex systems. This paper mainly reviews and evaluates the research of neural network based modeling, model-free data-driven calculation and data-driven multi-scale calculation, which provides the correct direction for the subsequent research of multi-scale mechanical behavior of composite solid propellants based on data-driven.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_4: The cohesive zone law represents the constitutive traction versus separation response along the crack-tip process zone of a material, which bridges the microscopic fracture process to the macroscopic failure behavior. Elucidating the exact functional form of the cohesive zone law is a challenging inverse problem since it can only be inferred indirectly from the far-field in experiments. Here, we construct the full functional form of the cohesive traction and separation relationship along the fracture process zone from far-field stresses and displacements using a physics-informed neural network (PINN), which is constrained to satisfy the Maxwell-Betti's reciprocal theorem with a reciprocity gap to account for the plastically deforming background material. Our numerical studies simulating crack growth under small-scale yielding, mode I loading, show that the PINN is robust in inversely extracting the cohesive traction and separation distributions across a wide range of simulated cohesive zone shapes, even for those with sharp transitions in the traction-separation relationships. Using the far-field elastic strain and residual elastic strain measurements associated with a fatigue crack for a ZK60 magnesium alloy specimen from synchrotron X-ray diffraction experiments, we reconstruct the cohesive traction-separation relationship and observe distinct regimes corresponding to transitions in the micromechanical damage mechanisms.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_5: This paper presents a data-driven framework based on distance functional for chemo-mechanical cohesive interfaces to capture transient diffusion and resulting interfacial damage evolution. The framework eliminates reliance on complex constitutive models and phenomenological assumptions, especially avoiding the coupling tangent terms with a given material database. The interfacial chemical potential and its jumps are used to expand the phase space for describing the chemical states of interfaces. Subsequently, a novel chemo-mechanical distance norm, including traction-separation pair, interfacial chemical potential-concentration pair and corresponding chemical potential jump-flux pair, is presented. Momentum conservation law for interfacial tractions and mass conservation law for interfacial concentration are enforced via Lagrange multipliers. For tracking the history-dependent state of the interface damage, an internal variable, termed interface integrity, is introduced to condition the interfacial database and manage the subsets mapping strategies, following physically motivated evolution constraints. Numerical examples are conducted to investigate the efficiency and capability of the present framework. A monotonic loading test validates a good numerical convergence relative to the number of data points. Subsequent cyclic loading simulations, compared with the reference solutions, show that the algorithm is well suitable to predict the history-dependent interface damage evolution under complex loading paths. Finally, the chemo-mechanically coupled examples capture phenomena such as interfacial swelling and interface degradation induced by transient diffusion and stress-driven diffusion. The current work provides a promising tool for understanding chemo-mechanical behaviors of interfaces within heterogeneous composites.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_6: Composite materials and multi-material components often fail at their internal interfaces/adhesive joints, and hence special attention should be given to such catastrophic delamination events to guarantee the system’s functional requirements. So far, however, the majority of structural topology optimization problems have focused on optimal distribution of the bulk materials by considering interfaces as perfectly bonded. This motivates the introduction of optimization methods that explicitly take into account the role of the material interfaces to optimize structures against delamination. In this work, we propose a data-driven heuristic optimization approach for the identification of optimal cohesive interfaces with linearly graded fracture properties to increase the ability of the composite structure to withstand peeling. Moreover, for given cohesive interface properties, we investigate the applicability of the physics-based Solid Isotropic Material with Penalty (SIMP) topology optimization approach to optimize the internal structure of a substrate in problems where the stress field is affected by interface delamination.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_7: The degradation of stone cultural heritage due to weathering has posed significant challenges in cultural relics preservation. This study focuses on the role of seepage in rock weathering and its effects on the interface between the rock matrix and the restoration layer of stone cultural relics. Specifically, a hydraulic-mechanical coupling model was developed for the rock-restoration layer interface of the Leshan Giant Buddha. The stress distribution and seepage characteristics in the rock matrix were analyzed, revealing the influence of seepage on interface stress, cracking, and hollowing of the restoration layer. The following conclusions are obtained through the research:Seepage at the interface between the rock matrix and the restoration layer leads to cracking and hollowing, creating obstacles in the protection of cultural relics; shear stress at the interface between the rock matrix and the restoration layer is the main cause of cracking in the repaired layer; uneven distribution of bonding strength between the repaired layer and the bedrock results in differential cracking and eventually forms a bulge; the properties and lithology of the rock matrix significantly affect interface deformation during seepage, exerting a direct impact on the cracking of the repaired layer. These findings have important practical implications for the preventive protection of the Leshan Giant Buddha and other stone cultural heritage sites. By understanding the mechanisms of seepage-induced cracking, conservation efforts can be enhanced, leading to more effective preservation of these significant cultural symbols.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_8: The size effect and strength discreteness observed in concrete stem primarily from its mesoscopic heterogeneity. Despite this understanding, establishing a clear relationship between the macroscopic and mesoscopic mechanical behaviors remains a considerable challenge. While some advancements have been made in studying the size effect of concrete, particularly regarding the rate effect, there has been relatively less emphasis on addressing the influence of random meso-component distribution. To investigate the tensile behaviors of concrete across varying low strain rates (10−5 s−1∼10−1 s−1) and model sizes, a meso-model dataset was established comprising double edge notched concrete with mortar matrix, coarse aggregate, and interface transition zone. A convolutional neural network introducing physical parameters was implemented to capture the potential non-linear relationship of concrete from meso-structure, strain rate and model size to tensile peak stress. Subsequently, the data-driven solution for the “Static and Dynamic unified” Size Effect Law (SD-SEL) in concrete tensile strength was developed by deconstructing the back propagation (BP) neural network to enhance its rationality, followed by the verification of the proposed formula. The findings indicate that the Data-Physical Hybrid-Driven approach effectively analyzes the mechanical response of concrete without the need for complex mechanical derivations, offering a promising tool for studying the size effect of composite materials.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_9: We applied physics-informed neural networks to solve the constitutive relations for nonlinear, path-dependent material behavior. As a result, the trained network not only satisfies all thermodynamic constraints but also instantly provides information about the current material state (i.e., free energy, stress, and the evolution of internal variables) under any given loading scenario without requiring initial data. One advantage of this work is that it bypasses the repetitive Newton iterations needed to solve nonlinear equations in complex material models. Additionally, strategies are provided to reduce the required order of derivative for obtaining the tangent operator. The trained model can be directly used in any finite element package (or other numerical methods) as a user-defined material model. However, challenges remain in the proper definition of collocation points and in integrating several non-equality constraints that become active or non-active simultaneously. We tested this methodology on rate-independent processes such as the classical von Mises plasticity model with a nonlinear hardening law, as well as local damage models for interface cracking behavior with a nonlinear softening law. In order to demonstrate the applicability of the methodology in handling complex path dependency in a three-dimensional (3D) scenario, we tested the approach using the equations governing a damage model for a three-dimensional interface model. Such models are frequently employed for intergranular fracture at grain boundaries. We have observed a perfect agreement between the results obtained through the proposed methodology and those obtained using the classical approach. Furthermore, the proposed approach requires significantly less effort in terms of implementation and computing time compared to the traditional methods.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_10: The traction-separation relationship of an interface is a critical component to understand and model the delamination behavior of multi-layer composites in situations where large scale bridging is active. Limited by current experimental techniques, the extraction of traction-separation relationships often relies on inverse approaches, where far field measurements are used as input data. In this article, a data-driven model based on Green’s function embedded neural networks is proposed (namely Deep Green Inversion, or DGI), where the input consists of far field displacement fields while the output is the desired but initially unknown tractions and separations on the interface. Specifically, Green’s functions are embedded as loss function terms along with other terms based on mean squared error and the field equations associated with loaded elastic bodies. The approach is first verified via analytical solutions to a simply supported beam subject to end moments and a non-uniform traction applied to a portion of one boundary of the beam. Hyperparameter training is included in order to elucidate the influence of weight factors that are applied to the different constraints that are considered. The developed approach is then successfully validated for mode-I and mixed-mode cohesive zone extraction problems using only far-field displacement synthetic data that are generated from numerical solutions to the problems. As part of the validation process and consideration of any limitations of the approach, the amount of displacement data required to produce robust traction-separation relations is deliberated. The tractions and separations within the cohesive zone are extracted with an average global error of 9.06 % and local error of 9.24%, using only 55–60% of the available experimental data. The proposed approach is then applied to double cantilever beam experiments with six different types of interaction between the beams, where the input displacement fields are obtained using digital image correlation. The traction-separation relationships extracted via the DGI neural network developed here agree very well with the results obtained via a direct extraction approach. These results suggest that the proposed approach is quite general, considering the range of traction-separation relations that were explored.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_27_11: The optimization of traction-separation parameters within the cohesive zone model (CZM) is crucial for accurately simulating electrode peeling failure in lithium-ion batteries. This study performed 180° peeling tests on NCM electrodes and used Abaqus software alongside the sim-flow optimization tool for simulation analysis, aiming to accurately replicate the failure behavior at the lithium-ion battery electrode interface. Experimental results were used to determine the fracture energy of the electrode interface, while numerical simulations analyzed the peeling performance of the electrode. By fitting the force-displacement curves from both simulation and experiment, the traction-separation parameters within the bilinear CZM were optimized. Additionally, the study analyzed the effects of traction-separation parameters, as well as the modulus and thickness of the current collector, on peeling behavior. The results showed that a significant increase in the interface stiffness and strength increased the steady-state peeling force. Among these, the fracture energy had the most significant influence on the steady-state peeling force. Furthermore, while the modulus and thickness of the current collector affected the maximum peeling force, their effect on the steady-state peeling force was negligible. These findings enhance the accuracy of simulating the failure behavior of the electrode interface.","Main_27:   For multilayer structures, interfacial failure is one of the most important
elements related to device reliability. For cohesive zone modelling,
traction-separation relations represent the adhesive interactions across
interfaces. However, existing theoretical models do not currently capture
traction-separation relations that have been extracted using direct methods,
particularly under mixed-mode conditions. Given the complexity of the problem,
models derived from the neural network approach are attractive. Although they
can be trained to fit data along the loading paths taken in a particular set of
mixed-mode fracture experiments, they may fail to obey physical laws for paths
not covered by the training data sets. In this paper, a thermodynamically
consistent neural network (TCNN) approach is established to model the
constitutive behavior of interfaces when faced with sparse training data sets.
Accordingly, three conditions are examined and implemented here: (i)
thermodynamic consistency, (ii) maximum energy dissipation path control and
(iii) J-integral conservation. These conditions are treated as constraints and
are implemented as such in the loss function. The feasibility of this approach
is demonstrated by comparing the modeling results with a range of physical
constraints. Moreover, a Bayesian optimization algorithm is then adopted to
optimize the weight factors associated with each of the constraints in order to
overcome convergence issues that can arise when multiple constraints are
present. The resultant numerical implementation of the ideas presented here
produced well-behaved, mixed-mode traction separation surfaces that maintained
the fidelity of the experimental data that was provided as input. The proposed
approach heralds a new autonomous, point-to-point constitutive modeling concept
for interface mechanics.
",0.55
"Cite_29_1: Recent anomalies observed in e+e− nuclear transitions of 8Be, 4He, and 12C by the ATOMKI collaboration may hint at the existence of a vector boson with a mass around 17 MeV, referred to as X17. If it exists, this boson would also affect similar processes in particle physics, including the Dalitz decays of vector mesons. Recently, the BESIII collaboration measured the Dalitz decay D∗0 →D0e+e− for the first time and reported a 3.5σ excess over the theoretical prediction based on the vector meson dominance (VMD) model. This excess may be another signal of the X17. In this study, we investigate the possible effects of the X17 on the Dalitz decays D∗ (s) → D(s)e+e−, B∗ (s) → B(s)e+e−, and J/ψ → ηce+e−. The required hadronic form factors are calculated within the framework of our covariant confined quark model, without relying on heavy quark effective theory or the VMD model. We present predictions for the Dalitz decay widths and the ratios Ree(V ) ≡ Γ(V → Pe+e−)/Γ(V → Pγ) within the Standard Model and in several new physics scenarios involving modifications due to the X17. Our results are compared with other theoretical calculations.","Main_29:   Rare electromagnetic decays of charmed mesons are useful laboratories to
explore the structure of hadronic states and the interactions between photon
and charmed mesons, to test the chiral perturbation theory in flavor sector and
to search for new physics including dark photons. In this paper, we calculate
the relative branching ratios of electromagnetic Dalitz decays $D_{(s)}^\ast\to
D_{(s)}\ell^+\ell^-$ to their corresponding radiative decays $D_{(s)}^\ast\to
D_{(s)}\gamma$, the dileptonic invariant mass spectra and the leptonic angular
distributions with transition form factor in Vector-Meson Dominance model,
where $D_{(s)}^\ast$ represents $D^\ast(2007)^0$, $D^\ast(2010)^\pm$,
$D^\ast(2640)^\pm$, $D_s^{\ast\pm}$, $D_{s1}^\ast(2700)^\pm$ and
$D_{s1}^\ast(2860)^\pm$.
",0.05
"Cite_30_1: Various methods have been explored to prepare the spin-adapted ground state, the lowest energy state within the Hilbert space constrained by externally specified values of the total spin magnitude and the spin-z component. In such problem settings, variational and non-variational methods commonly incorporate penalty terms into the original Hamiltonian to enforce the desired constraints. While in variational approaches, only O(n_{	extrm{spin}}^2) measurements are required for the calculation of the penalty terms for the total spin magnitude, non-variational approaches, such as probabilistic imaginary-time evolution or adiabatic time evolution, are expected to be more computationally intensive, requiring O(n_{	extrm{spin}}^4) gates naively. This paper proposes a new procedure based on non-variational quantum algorithms to obtain the spin-adapted ground state. The proposed method consists of two steps: the first step is to prepare a spin-magnitude adapted state and the second step is post-processing for the desired S_z. By separating into two steps, the procedure achieves the desired spin-adapted ground state while reducing the number of penalty terms from O(n_{	extrm{spin}}^4) to O(n_{	extrm{spin}}^2). We conducted numerical experiments for spin-1/2 Heisenberg ring models and manganese trimer systems. The results confirmed the effectiveness of our method, demonstrating a significant reduction in gate complexity and validating its practical usefulness.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_2: In this work, we combine the recently developed double unitary coupled cluster (DUCC) theory with the adaptive, problem-tailored variational quantum eigensolver (ADAPT-VQE) to explore accuracy of unitary downfolded Hamiltonians for quantum simulation of chemistry. We benchmark the ability of DUCC effective Hamiltonians to recover dynamical correlation energy outside of an active space. We consider the effects of strong correlation, commutator truncation, higher-body terms, and approximate external amplitudes on the accuracy of these effective Hamiltonians. When combining these DUCC Hamiltonians with ADAPT-VQE, we observe similar convergence of the ground state as compared to bare active space Hamiltonians, demonstrating that DUCC Hamiltonians provide increased accuracy without increasing the load on the quantum processor.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_3: Variational Quantum Eigensolver (VQE) provides a lucrative platform to determine molecular energetics in near-term quantum devices. While the VQE is traditionally tailored to determine the ground state wavefunction with the underlying Rayleigh-Ritz principle, for molecules characterized by a given point group symmetry, we propose to unify the VQE framework to treat the lowest energy states of any irreducible representation and spin-multiplicity. The method relies on the construction of a symmetry adapted multi determinantal reference where the constituent determinants are entangled through appropriate Clebsch-Gordan coefficients to ensure the desired spin-multiplicity. The unitary operator, defined in terms of totally symmetric spin-free generators, safeguards the method against variational collapse to symmetry broken solutions. We also propose an energy sorting based adaptive ansatz construction algorithm starting from a pool of totally symmetric spin-free unitary generators to come up with dynamically optimal ansatz. The proposed methodology allows us to build up further search algorithms within a reduced dimensional symmetry-adapted sub-Hilbert-space. With a highly compact circuit structure, it is expected to be realized in the near-term quantum devices to study emerging chemical phenomena and exploration of novel chemical space.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_4: Training the Variational Quantum Eigensolver (VQE) is a task that requires substantial compute. We propose the use of concepts from transfer learning to considerably reduce the training time when solving similar problem instances. We demonstrate that its utilization leads to accelerated convergence and provides a similar quality of results compared to circuits with parameters initialized around zero. Further, transfer learning works better when the distance between the source-solution is close to that of the target-solution. Based on these findings, we present an accelerated VQE approach tested on the MaxCut problem with a problem size of 12 nodes solved with two different circuits utilized. We compare our results against a random baseline and non transfer learning trained circuits. Our experiments demonstrate that transfer learning can reduce training time by around 93% in post-training, relative to identical circuits without the use of transfer learning. The accelerated VQE approach beats the standard approach by seven, respectively nine percentage points in terms of solution quality, if the early-stopping is considered. In settings where time-to-solution or computational costs are critical, this approach provides a significant advantage, having an improved trade-off between training effort and solution quality.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_5: Quantum computers promise to provide groundbreaking speed in solving complex problems. However, in the present-day noisy intermediate-scale quantum era, algorithms that require fewer resources are highly desired. In this work, we develop a new method called the variational rodeo eigensolver (VRE) for efficiently searching eigenstates and estimating eigenvalues with shallow circuits. We experimentally demonstrate this method on a programmable photonic chip with a single-qubit exciton transfer Hamiltonian whose eigenstates are searched with fidelities of more than 99% and their eigenvalues are estimated, reaching chemical accuracy. Furthermore, we experimentally estimate the ground energies of the simplified Hamiltonian of the hydrogen molecule with different atomic separations. To verify the scalability of VRE, we numerically search eigenstates of a tapered two-qubit Hamiltonian of hydrogen–helium ion. Our work provides a systematic and promising approach for the efficient estimation of Hamiltonian spectra.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_6: Exponential scaling presents a significant challenge in electronic structure calculations performed on classical computers. This paper explores how quantum computer algorithms can accurately represent quantum systems. Variational Quantum Eigensolver (VQE) algorithm is used to compute the ground state energy of hydrogen and helium sequences by implementing variational principle and quantum gates as trial wavefunction. This technique combines classical optimization with quantum computing calculations to simulate quantum systems on noisy and resource-limited computers. The resulting calculated energy is highly consistent to the corresponding exact values and Hartree-Fock calculations with a trend of when the number of atoms increases the calculated energy becomes more negative, leading to a decrease in the percentage error. Moreover, the convergence of the ground state energy of hydrogen and helium atoms was effectively optimized. The desired energy was reached, proven by adjusting the expectation value, and gradually achieving unity in state overlap. These findings demonstrate the VQE method's accuracy in calculating simple quantum systems and its scalability for larger atomic and molecular system, such as those in quantum chemistry and material science. However, challenges in quantum computer simulations, such as limited in qubit numbers and the presence of noise, require further advancements. Therefore, implementing a larger basis sets, advanced qubit mapping, specific chemistry ansatz, and flexible optimization techniques is one way to improve overall calculation.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_7: Quantum computers have proven to be effective in simulating many quantum systems. Simulating nuclear processes and state preparation poses significant challenges, even for traditional supercomputers. This study demonstrates the feasibility of a complete simulation of a nuclear transition, including the preparation of both ground and first excited states. To tackle the complexity of strong interactions between two and three nucleons, the states are modeled on the tritium nucleus. Both the initial and final states are represented using quantum circuits with variational quantum algorithms and inductive biases. Describing the spin-isospin states requires four qubits, and a parameterized quantum circuit that exploits a total of 16 parameters is initialized. The estimated energy has a relative error of 2% for the ground state and 10% for the first excited state of the system. The simulation estimates the transition probability between the two states as a function of the dipole polarization angle. This work marks a first step toward leveraging digital quantum computers to simulate nuclear physics.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_8: The world obeys quantum physics and quantum computing presents an alternative way to map physical problems to systems that follow the same laws. Such computation fundamentally constitutes a better way to understand the most challenging quantum problems. One such problem is the accurate simulation of highly correlated quantum systems. Still, modern-day quantum hardware has limitations and only allows for the modeling of simple systems. Here, we present for the first time a quantum computer model simulation of a complex hemocyanin molecule, which is an important respiratory protein involved in various physiological processes and is also used as a key component in therapeutic vaccines for cancer. To characterize the mechanism by which hemocyanin transports oxygen, variational quantum eigensolver (VQE) and quantum embedding methods are used in the context of dynamic mean field theory to solve the Anderson impurity model (AIM). Finally, it is concluded that the magnetic structure of hemocyanin is largely influenced by the many-body correction and that the computational effort for solving correlated electron systems could be substantially reduced with the introduction of quantum computing algorithms. We encourage the use of the Hamiltonian systems presented in this paper as a benchmark for testing quantum computing algorithms’ efficiency for chemistry applications.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_9: We present the first quantum-centric simulations of noncovalent interactions using a supramolecular approach. We simulate the potential energy surfaces (PES) of the water and methane dimers, featuring hydrophilic and hydrophobic interactions, respectively, with a sample-based quantum diagonalization (SQD) approach. Our simulations on quantum processors, using 27- and 36-qubit circuits, are in remarkable agreement with classical methods, deviating from complete active space configuration interaction (CASCI) and coupled-cluster singles, doubles, and perturbative triples (CCSD(T)) within 1 kcal/mol in the equilibrium regions of the PES. Finally, we test the capacity limits of the quantum methods for capturing hydrophobic interactions with an experiment on 54 qubits. These results mark significant progress in the application of quantum computing to chemical problems, paving the way for more accurate modeling of noncovalent interactions in complex systems critical to the biological, chemical and pharmaceutical sciences.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_10: Quantum computing, leveraging quantum phenomena like superposition and entanglement, is emerging as a transformative force in computing technology, promising unparalleled computational speed and efficiency crucial for engineering applications. This advancement presents both opportunities and challenges, requiring engineers to familiarize themselves with quantum principles, applications, and complexities. This paper systematically explores the foundational concepts of quantum mechanics and their implications for computational advancements, emphasizing the superiority of quantum algorithms in solving engineering problems. It identifies areas where gate-based quantum computing has the potential to outperform classical methods despite facing scalability and coherence issues. By offering clear examples with minimal reliance on in-depth quantum physics or hardware specifics, the aim is to make quantum computing accessible to engineers, addressing the steep learning curve and fostering its practical adoption for complex problem-solving and technological advancement as quantum hardware becomes more robust and reliable.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_11: Quantum computers have shown promise in simulating complex quantum systems, including nuclear processes that challenge even supercomputers. We summarize the key ingredients to demonstrate the feasibility of a full nuclear transition simulation, covering also ground and excited state preparation. The tritium nucleus has been used to model strong interactions between nucleons, with quantum circuits used to represent the initial and final states. Variational quantum algorithms aid in preparing such states, requiring four qubits to describe spin-isospin states. Our results show low relative errors in the energy estimation for the obtained eigenstates. Additionally, the transition probability between these states has been estimated as a function of dipole polarization angle. We also study the transition for the neutron capture d(n, γ)t and compare the result with the tritium de-excitation.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_30_12: Quantum systems in excited states are attracting significant interest with the advent of noisy intermediate-scale quantum (NISQ) devices. While ground states of small molecular systems are typically explored using hybrid variational algorithms like the variational quantum eigensolver (VQE), the study of excited states has received much less attention, partly due to the absence of efficient algorithms. In this work, we introduce the subspace search quantum imaginary time evolution (SSQITE) method, which calculates excited states using quantum devices by integrating key elements of the subspace search variational quantum eigensolver (SSVQE) and the variational quantum imaginary time evolution (VarQITE) method. The effectiveness of SSQITE is demonstrated through calculations of low-lying excited states of benchmark model systems including H2 and LiH molecules. A toy Hamiltonian is also employed to demonstrate that the robustness of VarQITE in avoiding local minima extends to its use in excited state algorithms. With this robustness in avoiding local minima, SSQITE shows promise for advancing quantum computations of excited states across a wide range of applications.","Main_30: The ground and excited state calculations at key geometries, such as the
Frank-Condon (FC) and the conical intersection (CI) geometries, are essential
for understanding photophysical properties. To compute these geometries on
noisy intermediate-scale quantum devices, we proposed a strategy that combined
a chemistry-inspired spin-restricted ansatz and a new excited state calculation
method called the variational quantum eigensolver under automatically-adjusted
constraints (VQE/AC). Unlike the conventional excited state calculation method,
called the variational quantum deflation, the VQE/AC does not require the
pre-determination of constraint weights and has the potential to describe
smooth potential energy surfaces. To validate this strategy, we performed the
excited state calculations at the FC and CI geometries of ethylene and phenol
blue at the complete active space self-consistent field (CASSCF) level of
theory, and found that the energy errors were at most 2 kcal mol$^{-1}$ even on
the ibm_kawasaki device.
",0.6
"Cite_31_1: Quantum error correction with erasure qubits promises significant advantages over standard error correction due to favorable thresholds for erasure errors. To realize this advantage in practice requires a qubit for which nearly all errors are such erasure errors, and the ability to check for erasure errors without dephasing the qubit. We demonstrate that a “dual-rail qubit” consisting of a pair of resonantly coupled transmons can form a highly coherent erasure qubit, where transmon 𝑇1 errors are converted into erasure errors and residual dephasing is strongly suppressed, leading to millisecond-scale coherence within the qubit subspace. We show that single-qubit gates are limited primarily by erasure errors, with erasure probability 𝑝erasure =2.19⁢(2) ×10−3 per gate while the residual errors are  ∼40 times lower. We further demonstrate midcircuit detection of erasure errors while introducing  <0.1% dephasing error per check. Finally, we show that the suppression of transmon noise allows this dual-rail qubit to preserve high coherence over a broad tunable operating range, offering an improved capacity to avoid frequency collisions. This work establishes transmon-based dual-rail qubits as an attractive building block for hardware-efficient quantum error correction.","Main_31: Gaussian boson sampling is originally proposed to show quantum advantage with
quantum linear optical elements. Recently, several experimental breakthroughs
based on Gaussian boson sampling pointing to quantum computing supremacy have
been presented. However, due to technical limitations, the outcomes of Gaussian
boson sampling devices are influenced severely by photon loss. Here, we present
an efficient and practical method to reduce the negative effect caused by
photon loss. With no hardware modifications, our method takes the data
post-selection process that discards low-quality data according to our
criterion to improve the performance of the final computational results, say
part is better than whole. As an example, we show that the post-selection
method can turn a GBS experiment that would otherwise fail in a ``non-classical
test"" into one that can pass that test. Besides improving the robustness of
computation results of current GBS devices, this photon loss mitigation method
may also benefit the further development of GBS-based quantum algorithms.
",0.1
"Cite_31_2: Real photonic devices are subject to photon losses that can decohere quantum information encoded in the system. In the absence of full fault tolerance, quantum error mitigation techniques have been introduced to help manage errors in noisy quantum devices. In this paper, we introduce an error mitigation protocol inspired by probabilistic error cancellation (a popular error mitigation technique in discrete variable systems) for continuous variable systems. We show that our quantum error cancellation protocol can undo photon losses in expectation value estimation tasks. To do this, we analytically derive the (nonphysical) inverse photon loss channel and decompose it into a sum over physically realizable channels with potentially negative coefficients. The bias of our ideal expectation value estimator can be made arbitrarily small at the cost of increasing the sampling overhead. The protocol requires a noiseless amplification followed by a series of photon subtractions. While these operations can be implemented probabilistically, for certain classes of initial state one can avoid the burden of carrying out the amplification and photon subtractions by leveraging Monte Carlo methods to give an unbiased estimate of the ideal expectation value. We validate our proposed mitigation protocol by simulating the scheme on squeezed vacuum states, cat states, and entangled coherent states.","Main_31: Gaussian boson sampling is originally proposed to show quantum advantage with
quantum linear optical elements. Recently, several experimental breakthroughs
based on Gaussian boson sampling pointing to quantum computing supremacy have
been presented. However, due to technical limitations, the outcomes of Gaussian
boson sampling devices are influenced severely by photon loss. Here, we present
an efficient and practical method to reduce the negative effect caused by
photon loss. With no hardware modifications, our method takes the data
post-selection process that discards low-quality data according to our
criterion to improve the performance of the final computational results, say
part is better than whole. As an example, we show that the post-selection
method can turn a GBS experiment that would otherwise fail in a ``non-classical
test"" into one that can pass that test. Besides improving the robustness of
computation results of current GBS devices, this photon loss mitigation method
may also benefit the further development of GBS-based quantum algorithms.
",0.1
"Cite_32_1: Cette thèse s’inscrit au croisement entre les théories de l’apprentissage et de la commande, proposant une méthodologie basée données, pour la modélisation et le contrôle des systèmes dynamiques nonlinéaires. En s’appuyant sur la théorie de la stabilité absolue et sur une représentation générale des modèles d’état neuronaux, plusieurs théorèmes de stabilité pour les réseaux de neurones sont présentés. Face aux limitations des approches traditionnelles d’optimisation sous contraintes LMI, nous développons un cadre théorique complet pour la paramétrisation des réseaux de neurones, compatible avec les algorithmes de gradient et les outils de différentiation automatique classiques. A l’aide de la théorie sur la linéarisation par bouclage, l’apprentissage en une seule étape d’un contrôleur approximativement linéarisant et d’un modèle de référence aux propriétés de stabilité garanties est présentée. Les résultats théoriques sont validés sur des exemples académiques d’atténuation de perturbations, ouvrant la voie à une utilisation plus systématique des réseaux de neurones dans la conception de lois de commande.","Main_32:   Linearising the dynamics of nonlinear mechanical systems is an important and
open research area. In this paper, we adopt a data-driven and feedback control
approach to tackle this problem. A model predictive control architecture is
developed that builds upon data-driven dynamic models obtained using nonlinear
system identification. The overall methodology shows a high degree of
performance combined with significant robustness against imperfect modelling
and extrapolation. These findings are demonstrated using large set of synthetic
experiments conducted on a asymmetric Duffing oscillator and using an
experimental prototype of a high-precision motion system.
",0.05
"Cite_33_1: While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.","Main_33: Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symbolic planning. However, these approaches
rely on coded primitives with little feature learning, and when applied to
high-dimensional visual scenes, they can suffer from scalability issues and
perform poorly when images have complex object interactions. To address these
challenges, we propose \textit{Differentiable Symbolic Expression Search}
(DiffSES), a novel symbolic learning approach that discovers discrete symbolic
policies using partially differentiable optimization. By using object-level
abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the
simplicity and scalability advantages of symbolic expressions, while also
incorporating the strengths of neural networks for feature learning and
optimization. Our experiments demonstrate that DiffSES is able to generate
symbolic policies that are simpler and more and scalable than state-of-the-art
symbolic RL methods, with a reduced amount of symbolic prior knowledge.
",0.25
"Cite_33_2: Artificial Intelligence (AI) assumes a pivotal role in Earth science, leveraging deep learning’s predictive capabilities. Despite its prevalence, the impact of AI on scientific discovery remains uncertain. In Earth sciences, the emphasis extends beyond mere accuracy, striving for groundbreaking discoveries with distinct physical properties essential for driving advancements through thorough analysis. Here, we introduce a novel knowledge-guided deep symbolic regression model (KG-DSR) incorporating prior knowledge of physical process interactions into the network. Using KG-DSR, we successfully derived the Penman-Monteith (PM) equation and generated a novel surface resistance parameterization. This new parameterization, grounded in fundamental cognitive principles, surpasses the conventional theory currently accepted in surface resistance parameterization. Importantly, the explicit physical processes generated by AI can generalize to future climate scenarios beyond the training data. Our results emphasize the role of AI in unraveling process intricacies and ushering in a new paradigm in tasks related to “AI for Land Surface Modeling.”","Main_33: Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symbolic planning. However, these approaches
rely on coded primitives with little feature learning, and when applied to
high-dimensional visual scenes, they can suffer from scalability issues and
perform poorly when images have complex object interactions. To address these
challenges, we propose \textit{Differentiable Symbolic Expression Search}
(DiffSES), a novel symbolic learning approach that discovers discrete symbolic
policies using partially differentiable optimization. By using object-level
abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the
simplicity and scalability advantages of symbolic expressions, while also
incorporating the strengths of neural networks for feature learning and
optimization. Our experiments demonstrate that DiffSES is able to generate
symbolic policies that are simpler and more and scalable than state-of-the-art
symbolic RL methods, with a reduced amount of symbolic prior knowledge.
",0.25
"Cite_33_3: Recent advancements in Large Language Models (LLMs) have showcased their ability to perform complex reasoning tasks, but their effectiveness in planning remains underexplored. In this study, we evaluate the planning capabilities of OpenAI's o1 models across a variety of benchmark tasks, focusing on three key aspects: feasibility, optimality, and generalizability. Through empirical evaluations on constraint-heavy tasks and spatially complex environments, we highlight o1-preview’s strengths in self-evaluation and constraint-following, while also identifying bottlenecks in decision-making and memory management, particularly in tasks requiring robust spatial reasoning. Our results reveal that while o1-preview outperforms GPT-4, the model often generates suboptimal solutions with redundant actions and struggles to generalize effectively in spatially complex tasks. This pilot study provides foundational insights into the planning limitations of LLMs, offering key directions for future research on improving memory management, decision-making, and generalization in LLM-based planning.","Main_33: Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symbolic planning. However, these approaches
rely on coded primitives with little feature learning, and when applied to
high-dimensional visual scenes, they can suffer from scalability issues and
perform poorly when images have complex object interactions. To address these
challenges, we propose \textit{Differentiable Symbolic Expression Search}
(DiffSES), a novel symbolic learning approach that discovers discrete symbolic
policies using partially differentiable optimization. By using object-level
abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the
simplicity and scalability advantages of symbolic expressions, while also
incorporating the strengths of neural networks for feature learning and
optimization. Our experiments demonstrate that DiffSES is able to generate
symbolic policies that are simpler and more and scalable than state-of-the-art
symbolic RL methods, with a reduced amount of symbolic prior knowledge.
",0.25
"Cite_33_4: This thesis explores optimizing fluid simulations in memory-constrained environments using advanced wavelet transforms to compress data, significantly reducing memory needs while maintaining accuracy. Tailored biorthogonal wavelets like LGT5/3, CDF9/7, and modified Haar wavelets ensure mass conservation and effective polynomial filtering. We integrated these methods into simulation workflows with attention to performance, developing strategies for graphics processors and balancing compression rates with performance trade-offs. Results demonstrate reduced memory footprints and potential performance boosts in scenarios where PCIe bus bandwidth limits speed. This research could significantly benefit industrial and research applications in fluid mechanics by managing large data efficiently, improving performance, and reducing energy consumption.","Main_33: Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symbolic planning. However, these approaches
rely on coded primitives with little feature learning, and when applied to
high-dimensional visual scenes, they can suffer from scalability issues and
perform poorly when images have complex object interactions. To address these
challenges, we propose \textit{Differentiable Symbolic Expression Search}
(DiffSES), a novel symbolic learning approach that discovers discrete symbolic
policies using partially differentiable optimization. By using object-level
abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the
simplicity and scalability advantages of symbolic expressions, while also
incorporating the strengths of neural networks for feature learning and
optimization. Our experiments demonstrate that DiffSES is able to generate
symbolic policies that are simpler and more and scalable than state-of-the-art
symbolic RL methods, with a reduced amount of symbolic prior knowledge.
",0.25
"Cite_33_5: Recent advancements in Large Language Models (LLMs) have showcased their ability to perform complex reasoning tasks, but their effectiveness in planning remains underexplored. In this study, we evaluate the planning capabilities of OpenAI's o1 models across a variety of benchmark tasks, focusing on three key aspects: feasibility, optimality, and generalizability. Through empirical evaluations on constraint-heavy tasks (e.g., 	extit{Barman}, 	extit{Tyreworld}) and spatially complex environments (e.g., 	extit{Termes}, 	extit{Floortile}), we highlight o1-preview's strengths in self-evaluation and constraint-following, while also identifying bottlenecks in decision-making and memory management, particularly in tasks requiring robust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4 in adhering to task constraints and managing state transitions in structured environments. However, the model often generates suboptimal solutions with redundant actions and struggles to generalize effectively in spatially complex tasks. This pilot study provides foundational insights into the planning limitations of LLMs, offering key directions for future research on improving memory management, decision-making, and generalization in LLM-based planning. ","Main_33: Learning efficient and interpretable policies has been a challenging task in reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symbolic planning. However, these approaches
rely on coded primitives with little feature learning, and when applied to
high-dimensional visual scenes, they can suffer from scalability issues and
perform poorly when images have complex object interactions. To address these
challenges, we propose \textit{Differentiable Symbolic Expression Search}
(DiffSES), a novel symbolic learning approach that discovers discrete symbolic
policies using partially differentiable optimization. By using object-level
abstractions instead of raw pixel-level inputs, DiffSES is able to leverage the
simplicity and scalability advantages of symbolic expressions, while also
incorporating the strengths of neural networks for feature learning and
optimization. Our experiments demonstrate that DiffSES is able to generate
symbolic policies that are simpler and more and scalable than state-of-the-art
symbolic RL methods, with a reduced amount of symbolic prior knowledge.
",0.25
"Cite_34_1: Coal-ablaze Brick Kiln industries are the major contributors of Particulate Matter (PM2.5, PM10) emissions that endanger the environment and pose a variety of health risks to all the living beings. Current static ambient pollutant monitoring stations are sparsely located due to their expensive deployment. Recent advancements in Internet of Things (IoT) technology tends to have portable sensors which could be easily deployed at any location to monitor the quality of air. Calibration for these portable sensors requires training data from static reference monitoring stations. In this study, Brick Kiln industry, which are usually remotely located from the reference stations, is chosen to monitor its emission through the IoT devices, and the calibration for the portable sensors are performed using data from a reference sensor. Calibration of the sensor reading is performed using proposed Meta Learning based Transfer Learning (MLTL) and its performance is evaluated utilizing evaluation metrics of various Machine Learning (ML) and Deep Learning (DL) based regression models. The proposed model shows the most significant scores 0.992236, 0.0002, 0.0048 for the evaluation metrics, R-squared, Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE), respectively, as compared to other ML models while calibrating the Particulate Matter (PM) pollutant’s emission rate obtained from the industry. ","Main_34: Sheaf theory, which is a complex but powerful tool supported by topological
theory, offers more flexibility and precision than traditional graph theory
when it comes to modeling relationships between multiple features. In the realm
of air quality monitoring, this can be incredibly useful in detecting sudden
changes in local dust particle density, which can be difficult to accurately
measure using commercial instruments. Traditional methods for air quality
measurement often rely on calibrating the measurement with public standard
instruments or calculating the measurements moving average over a constant
period. However, this can lead to an incorrect index at the measurement
location, as well as an oversmoothing effect on the signal. In this study, we
propose a compact device that uses sheaf theory to detect and count vehicles as
a local air quality change-causing factor. By inferring the number of vehicles
into the PM2.5 index and propagating it into the recorded PM2.5 index from
low-cost air monitoring sensors such as PMS7003 and BME280, we can achieve
self-correction in real-time. Plus, the sheaf-theoretic method allows for easy
scaling to multiple nodes for further filtering effects. By implementing sheaf
theory in air quality monitoring, we can overcome the limitations of
traditional methods and provide more accurate and reliable results.
",0.05
"Cite_35_1: A linear group is called unisingular if every element of it has eigenvalue 1. In this paper we develop some general machinery for the study of unisingular irreducible linear groups. A motivation for the study of such groups comes from several sources, including algebraic geometry, Galois theory, finite group theory and representation theory. In particular, a certain aspect of the theory of abelian varieties requires the knowledge of unisingular irreducible subgroups of the symplectic groups over the field of two elements, and in this paper we concentrate on this special case of the general problem. A more special but important question is that of the existence of such subgroups in the symplectic groups of particular degrees. We answer this question for almost all degrees , specifically, the question remains open only 7 values of n.","Main_35: A finite order element $g$ of a group $G$ is called rational if $g$ is
conjugate to $g^i$ for every integer $i$ coprime to the order $g$. We determine
all triples $(G,g,\phi)$, where $G$ is a simple algebraic group of type
$A_n,B_n$ or $C_n$ over an algebraically closed field of characteristic $p\geq
0$, $g\in G$ is a rational odd order semisimple element and $\phi$ is an
irreducible representation of $G$ such that $\phi(g)$ has eigenvalue 1.
",0.15
"Cite_35_2: The generalized order e_G(g) of an element g of a group G is the smallest positive integer k such that there exist x_1, ldots,x_k in G such that g^{x_1} ldots g^{x_k}=1, where g^x=x^{-1}gx. Let e(G) = max {e_G(g) | g in G}. We provide upper bounds for e(G) for every finite simple group G. In particular, we show that e(G)leq 8 unless G in {mbox{PSL}_n(q), mbox{PSU}_n(q), E_6(q),{^2}E_6(q)}. For the latter groups e(G) leq n,3n+3,36,36, respectively. In addition, we bound from above the generalized order of semisimple and unipotent elements of finite simple groups of Lie type.","Main_35: A finite order element $g$ of a group $G$ is called rational if $g$ is
conjugate to $g^i$ for every integer $i$ coprime to the order $g$. We determine
all triples $(G,g,\phi)$, where $G$ is a simple algebraic group of type
$A_n,B_n$ or $C_n$ over an algebraically closed field of characteristic $p\geq
0$, $g\in G$ is a rational odd order semisimple element and $\phi$ is an
irreducible representation of $G$ such that $\phi(g)$ has eigenvalue 1.
",0.15
"Cite_35_3: A linear group is called unisingular if every element of it has eigenvalue 1. A certain aspect of the theory of abelian varieties requires the knowledge of unisingular irreducible subgroups of the symplectic groups over the field of two elements. A more special, but an important question is on the existence of such subgroups in the symplectic groups of particular degree. We answer this question for almost all degrees 2n<250, specifically, the question remains open only 7 values of n. Additionally, the paper contains results of general nature on the structure of unisingular irreducible linear groups.","Main_35: A finite order element $g$ of a group $G$ is called rational if $g$ is
conjugate to $g^i$ for every integer $i$ coprime to the order $g$. We determine
all triples $(G,g,\phi)$, where $G$ is a simple algebraic group of type
$A_n,B_n$ or $C_n$ over an algebraically closed field of characteristic $p\geq
0$, $g\in G$ is a rational odd order semisimple element and $\phi$ is an
irreducible representation of $G$ such that $\phi(g)$ has eigenvalue 1.
",0.15
"Cite_36_1: Westudy the bilateral trade problem where a seller owns a single indivisible item, and a potential buyer seeks to purchase it. Previous mechanisms for this problem only considered the case where the values of the buyer and the seller are drawn from independent distributions. In contrast, this paper studies bilateral trade mechanisms when the values are drawn from a joint distribution. Weprove that the buyer-o ering mechanism guarantees an approximation ratio of 4 4−1 ≈ 1.582 to the social welfare even if the values are drawn from a joint distribution. The buyer-o ering mechanism is Bayesian incentive compatible, but the seller has a dominant strategy. We prove the buyer-o ering mechanism is optimal in the sense that no Bayesian mechanism where one of the players has a dominant strategy can obtain an approximation ratio better than 4 4−1 . We also show that no mechanism in which both sides have a dominant strategy can provide any constant approximation to the social welfare when the values are drawn from a joint distribution. Finally, we prove some impossibility results on the power of general Bayesian incentive compatible mechanisms. In particular, we show that no deterministic Bayesian incentive-compatible mechanismcanprovideanapproximationratiobetterthan1+ln2 2 ≈1.346.","Main_36: We consider a model of bilateral trade with private values. The value of the
buyer and the cost of the seller are jointly distributed. The true joint
distribution is unknown to the designer, however, the marginal distributions of
the value and the cost are known to the designer. The designer wants to find a
trading mechanism that is robustly Bayesian incentive compatible, robustly
individually rational, budget-balanced and maximizes the expected gains from
trade over all such mechanisms. We refer to such a mechanism as an optimal
robust mechanism. We establish equivalence between Bayesian incentive
compatible mechanisms (BIC) and dominant strategy mechanisms (DSIC). We
characterise the worst distribution for a given mechanism and use this
characterisation to find an optimal robust mechanism. We show that there is an
optimal robust mechanism that is deterministic (posted-price), dominant
strategy incentive compatible, and ex-post individually rational. We also
derive an explicit expression of the posted-price of such an optimal robust
mechanism. We also show the equivalence between the efficiency gains from the
optimal robust mechanism (max-min problem) and guaranteed efficiency gains if
the designer could choose the mechanism after observing the true joint
distribution (min-max problem).
",0.05
"Cite_37_1: In this work, we study the magnetic phases of a spatially modulated chain of spin-1 Rydberg excitons. Using the Density Matrix Renormalization Group (DMRG) technique, we study various magnetic and topologically nontrivial phases using both single-particle properties, such as local magnetization and quantum entropy, and many-body ones, such as pair-wise Néel and long-range string correlations. In particular, we investigate the emergence and robustness of the Haldane phase, a topological phase of anti-ferromagnetic spin-1 chains. Furthermore, we devise a hybrid quantum algorithm employing restricted Boltzmann machine to simulate the ground state of such a system that shows very good agreement with the results of exact diagonalization and DMRG.","Main_37:   Quantum states of a novel Bose-Einstein condensate, in which both
fermion-pair and exciton condensations are simultaneously present, have
recently been realized theoretically in a model Hamiltonian system. Here we
identify quantum phase transitions in that model between fermion-pair and
exciton condensations based on a geometric analysis of the convex set of
ground-state 2-particle reduced density matrices (2-RDMs). The 2-RDM set
provides a finite representation of the infinite parameter space of
Hamiltonians that readily reveals a fermion-pair condensate phase and two
distinct exciton condensate phases, as well as the emergence of first- and
second-order phase transitions as the particle number of the system is
increased. The set, furthermore, shows that the fermion-exciton condensate
(FEC) lies along the second-order phase transition between the exciton and
fermion-pair condensate phases. The detailed information about the exciton and
fermion-pair phases, the forces behind these phase, as well as their associated
transitions provides additional insight into the formation of the FEC
condensate, which we anticipate will prove useful in its experimental
realization.
",0.2
"Cite_37_2: Spin liquids─an emergent, exotic collective phase of matter─have garnered enormous attention in recent years. While experimentally many prospective candidates have been proposed and realized, theoretically modeling real materials that display such behavior may pose serious challenges due to the inherently high correlation content of such phases. Over the last few decades, the second-quantum revolution has been the harbinger of a novel computational paradigm capable of initiating a foundational evolution in computational physics. In this report, we strive to use the power of the latter to study a prototypical model, a spin-1/2-unit cell of a Kagome antiferromagnet. Extended lattices of such unit cells are known to possess a magnetically disordered spin-liquid ground state. We employ robust classical numerical techniques such as the density-matrix renormalization group (DMRG) to identify the nature of the ground state through a matrix-product state (MPS) formulation. We subsequently use the gained insight to construct an auxiliary Hamiltonian with reduced measurables and also design an ansatz that is modular and gate-efficient. With robust error-mitigation strategies, we are able to demonstrate that the said ansatz is capable of accurately representing the target ground state even on a real IBMQ backend within 1% accuracy in energy. Since the protocol is linearly scaling O(n) in the number of unit cells, gate requirements, and the number of measurements, it is straightforwardly extendable to larger Kagome lattices that can pave the way for efficient construction of spin-liquid ground states on a quantum device.","Main_37:   Quantum states of a novel Bose-Einstein condensate, in which both
fermion-pair and exciton condensations are simultaneously present, have
recently been realized theoretically in a model Hamiltonian system. Here we
identify quantum phase transitions in that model between fermion-pair and
exciton condensations based on a geometric analysis of the convex set of
ground-state 2-particle reduced density matrices (2-RDMs). The 2-RDM set
provides a finite representation of the infinite parameter space of
Hamiltonians that readily reveals a fermion-pair condensate phase and two
distinct exciton condensate phases, as well as the emergence of first- and
second-order phase transitions as the particle number of the system is
increased. The set, furthermore, shows that the fermion-exciton condensate
(FEC) lies along the second-order phase transition between the exciton and
fermion-pair condensate phases. The detailed information about the exciton and
fermion-pair phases, the forces behind these phase, as well as their associated
transitions provides additional insight into the formation of the FEC
condensate, which we anticipate will prove useful in its experimental
realization.
",0.2
"Cite_37_3: Elucidation of many chemical behaviors and properties depends on our ability to model the physics of electron-electron interactions at reasonable computational expense with correlation phenomena often being integral to predicting behaviors and properties of molecules and materials of contemporary interest. A particularly sought-after consequence of a type of correlation is superconductivity, which traditionally results from the Bose-Einstein-like condensation of Cooper (electron-electron) pairs. However, all currently-known superconductors condense at either too-low of temperatures or too-high of pressures to be commercially-viable. A possible solution to this limitation is the utilization of excitonic superfluidity arising from the condensation of particle-hole pairs (excitons) which are expected to condense at higher temperatures due to their decreased mass and which can result in the frictionless flow of excitation energy and—in bilayer systems—counterflow superconductivity. The first chapters of this text focus on the identification of the beginnings of  exciton condensation in molecular-scale analogues of extended systems in an effort to contribute to rational design of molecularly-scaled exciton condensates. Then, the simulation of both Cooper pair and exciton condensates on quantum devices is described, which establishes a new avenue for the creation and characterization of condensation phenomena and is an important step toward more complex modeling of phenomena with significant quantum long-range order on quantum computers. The following chapters introduce fermion-exciton condensates (FECs)—novel quantum states that simultaneously exhibit the character of superconducting states and exciton condensates and may demonstrate hybrid properties of both. In this thesis, these FECs are computationally and theoretically predicted, described with a model Hamiltonian, and experimentally prepared on a quantum device. Finally, machine learning is used to reduce the many-electron problem to an effective two-electron problem, decreasing effective computational scaling with system size.","Main_37:   Quantum states of a novel Bose-Einstein condensate, in which both
fermion-pair and exciton condensations are simultaneously present, have
recently been realized theoretically in a model Hamiltonian system. Here we
identify quantum phase transitions in that model between fermion-pair and
exciton condensations based on a geometric analysis of the convex set of
ground-state 2-particle reduced density matrices (2-RDMs). The 2-RDM set
provides a finite representation of the infinite parameter space of
Hamiltonians that readily reveals a fermion-pair condensate phase and two
distinct exciton condensate phases, as well as the emergence of first- and
second-order phase transitions as the particle number of the system is
increased. The set, furthermore, shows that the fermion-exciton condensate
(FEC) lies along the second-order phase transition between the exciton and
fermion-pair condensate phases. The detailed information about the exciton and
fermion-pair phases, the forces behind these phase, as well as their associated
transitions provides additional insight into the formation of the FEC
condensate, which we anticipate will prove useful in its experimental
realization.
",0.2
"Cite_37_4: Collective effects are a typical nuclear feature that refers to the behavior of a group of N fermions (protons and neutrons) within the atomic nucleus. Our interest lies in light nuclei only. Thus, we consider here, using the Lipkin model, a small-N fermion system at low-temperature T and discover -collective phenomena. Our fermion-model simplicity allows one to gain insight into collective fermion behavior. We focus attention, for N < 20 , on several quantifiers. These include standard ones related to thermal behavior, such as entropy, and quantifiers of another kind, like quantum purity","Main_37:   Quantum states of a novel Bose-Einstein condensate, in which both
fermion-pair and exciton condensations are simultaneously present, have
recently been realized theoretically in a model Hamiltonian system. Here we
identify quantum phase transitions in that model between fermion-pair and
exciton condensations based on a geometric analysis of the convex set of
ground-state 2-particle reduced density matrices (2-RDMs). The 2-RDM set
provides a finite representation of the infinite parameter space of
Hamiltonians that readily reveals a fermion-pair condensate phase and two
distinct exciton condensate phases, as well as the emergence of first- and
second-order phase transitions as the particle number of the system is
increased. The set, furthermore, shows that the fermion-exciton condensate
(FEC) lies along the second-order phase transition between the exciton and
fermion-pair condensate phases. The detailed information about the exciton and
fermion-pair phases, the forces behind these phase, as well as their associated
transitions provides additional insight into the formation of the FEC
condensate, which we anticipate will prove useful in its experimental
realization.
",0.2
"Cite_39_1: We solve the inverse problems to recover Dirac systems on an interval or semiaxis from their spectral functions (matrix valued functions) for the case of locally square-integrable potentials. Direct problems in terms of spectral functions are treated as well. Moreover, we present necessary and sufficient conditions on the given distribution matrix valued function to be a spectral function of some Dirac system with a locally square-integrable potential. Interesting connections with Paley-Wiener sampling measures appear in the case of scalar spectral functions.","Main_39:   Hamiltonians are 2-by-2 positive semidefinite real symmetric matrix-valued
functions satisfying certain conditions. In this paper, we solve the inverse
problem for which recovers a Hamiltonian from the solution of a first-order
system attached to a given Hamiltonian, consisting of ordinary differential
equations parametrized by a set of complex numbers, under certain conditions
for the solutions. This inverse problem is a generalization of the inverse
problem for two-dimensional canonical systems.
",0.1
Cite_39_2: We present a method to construct a chain of reproducing kernel Hilbert spaces controlled by a first-order system of differential equations from a given unimodular function satisfying several conditions. One of the applications of that method is a conditional but richly general solution to the inverse problem of recovering the structure Hamiltonian from a given de Branges space.,"Main_39:   Hamiltonians are 2-by-2 positive semidefinite real symmetric matrix-valued
functions satisfying certain conditions. In this paper, we solve the inverse
problem for which recovers a Hamiltonian from the solution of a first-order
system attached to a given Hamiltonian, consisting of ordinary differential
equations parametrized by a set of complex numbers, under certain conditions
for the solutions. This inverse problem is a generalization of the inverse
problem for two-dimensional canonical systems.
",0.1
"Cite_41_1: Computing in memory (CIM) could be used to overcome the von Neumann bottleneck and to provide sustainable improvements in computing throughput and energy efficiency. Underlying the different CIM schemes is the implementation of two kinds of computing primitive: logic gates and multiply–accumulate operations. Considering the input and output in either operation, CIM technologies differ in regard to how memory cells participate in the computation process. This complexity makes it difficult to build a comprehensive understanding of CIM technologies. Here, we provide a full-spectrum classification of all CIM technologies by identifying the degree of memory cells participating in the computation as inputs and/or output. We elucidate detailed principles for standard CIM technologies across this spectrum, and provide a platform for comparing the advantages and disadvantages of each of the different technologies. Our taxonomy could also potentially be used to develop other CIM schemes by applying the spectrum to different memory devices and computing primitives.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_2: In-memory computing (IMC) is an emerging computational approach that addresses the processor-memory divide in modern computing systems. The core concept is to leverage the physics of memory devices and their array-level organization to perform computations directly within the memory array. Phase-change memory (PCM) is a leading memory technology being explored for IMC. In this perspective, we review the current state of phase-change materials, PCM device physics, and the design and fabrication of PCM-based IMC chips. We also provide an overview of the application landscape and offer insights into future developments.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_3: Memristor-aided logic (MAGIC) design style holds a high promise for realizing digital logic-in-memory functionality. The ability to implement a specific gate in a MAGIC design style hinges on the SET-to-RESET threshold ratio. The TaOx memristive devices exhibit distinct SET-to-RESET ratios, enabling the implementation of OR and NOT operations. As the adoption of the MAGIC design style gains momentum, it becomes crucial to understand the breakdown of energy consumption in the various phases of its operation. This paper presents experimental demonstrations of the OR and NOT gates on a 1T1R crossbar array. Additionally, it provides insights into the energy distribution for performing these operations at different stages. Through our experiments across different gates, we found that the energy consumption is dominated by initialization in the MAGIC design style. The energy split-up is 14.8%, 85%, and 0.2% for execution, initialization, and read operations, respectively.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_4: Reducing power consumption in nowadays computer technologies represents an increasingly difficult challenge. Conventional computing architectures suffer from the so-called von Neumann bottleneck (VNB), which consists in the continuous need to exchange data and instructions between the memory and the processing unit, leading to significant and apparently unavoidable power consumption. Even the hardware typically employed to run Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNN), suffers from this limitation. A change of paradigm is so needed to comply with the ever-increasing demand for ultra-low power, autonomous, and intelligent systems. From this perspective, emerging memristive non-volatile memories are considered a good candidate to lead this technological transition toward the next-generation hardware platforms, enabling the possibility to store and process information in the same place, therefore bypassing the VNB. To evaluate the state of current public-available devices, in this work commercial-grade packaged Self Directed Channel memristors are thoroughly studied to evaluate their performance in the framework of in-memory computing. Specifically, the operating conditions allowing both analog update of the synaptic weight and stable binary switching are identified, along with the associated issues. To this purpose, a dedicated yet prototypical system based on an FPGA control platform is designed and realized. Then, it is exploited to fully characterize the performance in terms of power consumption of an innovative Smart IMPLY (SIMPLY) Logic-in-Memory (LiM) computing framework that allows reliable in-memory computation of classical Boolean operations. The projection of these results to the nanoseconds regime leads to an estimation of the real potential of this computing paradigm. Although not investigated in this work, the presented platform can also be exploited to test memristor-based SNN and Binarized DNNs (i.e., BNN), that can be combined with LiM to provide the heterogeneous flexible architecture envisioned as the long-term goal for ubiquitous and pervasive AI.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_5: Phase Change Memory (PCM) has emerged as a promising non-volatile memory technology with significant applications in both edge computing and analog in-memory computing. This paper synthesizes recent research contributions on the use of PCM for smart sensing, structural health monitoring, neural network acceleration, and binary pattern matching. By examining key advancements, challenges, and potential future developments, this work provides a comprehensive state-of-the-art overview of PCM in these domains, highlighting the possible employments of PCM technology in further edge computing scenarios including medical and human body monitoring.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_6: Digital processing-in-memory (PIM) architectures mitigate the memory wall problem by facilitating parallel bitwise operations directly within the memory. Recent works have demonstrated their algorithmic potential for accelerating data-intensive applications; however, there remains a significant gap in the programming model and microarchitectural design. This is further exacerbated by aspects unique to memristive PIM such as partitions and operations across both directions of the memory array. To address this gap, this paper provides an end-to-end architectural integration of digital memristive PIM from a high-level Python library for tensor operations (similar to NumPy and PyTorch) to the low-level microarchitectural design. We begin by proposing an efficient microarchitecture and instruction set architecture (ISA) that bridge the gap between the low-level control periphery and an abstraction of PIM parallelism. We subsequently propose a PIM development library that converts high-level Python to ISA instructions and a PIM driver that translates ISA instructions into PIM micro-operations. We evaluate PyPIM via a cycle-accurate simulator on a wide variety of benchmarks that both demonstrate the versatility of the Python library and the performance compared to theoretical PIM bounds. Overall, PyPIM drastically simplifies the development of PIM applications and enables the conversion of existing tensor-oriented Python programs to PIM with ease.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_7: This paper experimentally demonstrates a near-crossbar memory logic technique called Pinatubo. Pinatubo, an acronym for Processing In Non-volatile memory ArchiTecture for bUlk Bitwise Operations, facilitates the concurrent activation of two or more rows, enabling bitwise operations such as OR, AND, XOR, and NOT on the activated rows. We implement Pinatubo using phase change memory (PCM) and compare our experimental results with the simulated data from the original Pinatubo study. Our findings highlight a significant four-orders of magnitude difference between resistance states, suggesting the robustness of the Pinatubo architecture with PCM technology.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_41_8: hase Change Memory (PCM) adapts to space utilization and bit storage for displaying amorphous or crystalline states. This change of state relies on the nature of the information and its non-volatility period. As is well known Digital Logic operations are influential over memory modeling, this article introduces a Propagative Adaptability Decision Module (PADM) using DL. This module is utilized based on the information non-volatility across overflow and underflow memory conditions. This utilization-based classification induces the digital logic influence by swapping the states that are robust for further propagative adaptability. This means the possible logical combinations of 0’s and 1’s are used for deciding the states of PCM. The logical operations using OR and NOT are used for reducing overflows whereas the AND and NOT combinations are used for preventing underflows. This reduces the volatility show-up in two distinct phases of memory utilization. Therefore, the 0 and 1 combinations are validated for all the logical operations to ensure propagative memory swapping for balancing the overflow and underflow conditions. This process does not mark up the highest 1 or 0 combination individually due to state changes. The proposed module is validated using swapping rate, time, complexity, and overflow conditions.","Main_41: Stateful logic is a digital processing-in-memory technique that could address
von Neumann memory bottleneck challenges while maintaining backward
compatibility with standard von Neumann architectures. In stateful logic,
memory cells are used to perform the logic operations without reading or moving
any data outside the memory array. Stateful logic has been previously
demonstrated using several resistive memory types, mostly by resistive RAM
(RRAM). Here we present a new method to design stateful logic using a different
resistive memory - phase change memory (PCM). We propose and experimentally
demonstrate four logic gate types (NOR, IMPLY, OR, NIMP) using commonly used
PCM materials. Our stateful logic circuits are different than previously
proposed circuits due to the different switching mechanism and functionality of
PCM compared to RRAM. Since the proposed stateful logic form a functionally
complete set, these gates enable sequential execution of any logic function
within the memory, paving the way to PCM-based digital processing-in-memory
systems.
",0.4
"Cite_42_1: In this paper, we study the global phase space dynamics of single nonminimally coupled scalar field inflation models in the metric and Palatini formalisms. Working in the Jordan frame, we derive the scalar-tensor general field equations and flat Friedmann-Lemaître-Robertson-Walker cosmological equations and present the Palatini and metric equations in a common framework. We show that inflation is characterized by a “master” trajectory from a saddle-type de Sitter fixed point to a stable node fixed point, approximated by slow-roll conditions (presented for the first time in the Palatini formalism). We show that, despite different underlying equations, the fixed point structure and properties of many models are congruent in metric and Palatini formalisms, which explains their qualitative similarities and their suitability for driving inflation. On the other hand, the global phase portraits reveal how even models which predict the same values for observable perturbations differ, both to the extent of the phase space physically available to their trajectories, as well as their past asymptotic states. We also note how the slow-roll conditions tend to underestimate the end of inflationary accelerated expansion experienced by the true nonlinear “master” solution. The explicit examples we consider range from the metric and Palatini induced gravity quintic potential with a Coleman-Weinberg correction factor to Starobinsky, metric, and Palatini nonminimal Higgs, as well as second-order pole and several nontrivial Palatini models.","Main_42:   Theory of gravity with a quadratic contribution of scalar curvature is
investigated in terms of dynamical system approach. The simplest
Friedmann-Robertson-Walker metric is used to formulate dynamics in Jordan frame
as well as in conformally transformed Einstein frame. We show that in both
frames there are stable de Sitter states for which the Hubble function
expansion naturally gives terms corresponding to non-substantial dark matter.
Using invariant centre manifold we show that in the Einstein frame there is a
zero measure set of initial conditions leading from unstable to stable de
Sitter state. Additionally, the initial de Sitter state is plunged with a
parallelly propagated singularity. We show that the Jordan frame and the
Einstein frame formulation of the theory are physically nonequivalent.
",0.1
"Cite_42_2: In the phase space perspective, scalar field slow roll inflation is described by a heteroclinic orbit from a saddle type fixed point to a final attractive point. In many models the saddle point resides in the scalar field asymptotics, and thus for a comprehensive view of the dynamics a global phase portrait is necessary. For this task, in the literature one mostly encounters dynamical variables that either render the initial or the final state singular, thus obscuring the full picture. In this work we construct a hybrid set of variables which allow the depiction of both the initial and final states distinctly in nonsingular manner. To illustrate the method, we apply these variables to portray various interesting types of scalar field inflationary models like metric Higgs inflation, metric Starobinsky inflation, pole inflation, and a nonminimal Palatini model.","Main_42:   Theory of gravity with a quadratic contribution of scalar curvature is
investigated in terms of dynamical system approach. The simplest
Friedmann-Robertson-Walker metric is used to formulate dynamics in Jordan frame
as well as in conformally transformed Einstein frame. We show that in both
frames there are stable de Sitter states for which the Hubble function
expansion naturally gives terms corresponding to non-substantial dark matter.
Using invariant centre manifold we show that in the Einstein frame there is a
zero measure set of initial conditions leading from unstable to stable de
Sitter state. Additionally, the initial de Sitter state is plunged with a
parallelly propagated singularity. We show that the Jordan frame and the
Einstein frame formulation of the theory are physically nonequivalent.
",0.1
"Cite_43_1: The De Donder-Weyl (DW) Hamiltonian theory of fields treats space and time variables on equal footing. Its quantization, called precanonical quantization, leads to a hypercomplex generalization of quantum formalism to field theory as it follows from the quantization of Poisson-Gerstenhaber brackets defined on differential forms. Our recent work on precanonical quantization of general relativity is extended to the teleparallel equivalent of general relativity (TEGR) in tetrad Palatini formulation. The covariant precanonical Schrödinger equation for quantum TEGR and the relevant operators are constructed from the quantization of generalized Dirac brackets calculated using the constraints analysis generalized to the DW Hamiltonian theory. Our analysis of the ordering ambiguities in the precanonical Schrödinger equation allows us to estimate the contribution to the cosmological constant from the quantum TEGR and argue its consistency with the observed value, albeit with the current error of estimation of 13 orders of magnitude due to the theoretical uncertainties in the relation between the scale varkappa introduced by precanonical quantization and the mass gap in the pure gauge sector of QCD.","Main_43:   The covariant Hamilton-Jacobi formulation of Maxwell's equations is derived
from the first-order (Palatini-like) Lagrangian using the analysis of
constraints within the De~Donder-Weyl covariant Hamiltonian formalism and the
corresponding polysymplectic reduction.
",0.2
Cite_43_2: Quantization of the teleparallel equivalent of general relativity (TEGR) is discussed from the perspective of the space-time symmetric De Donder-Weyl (DW) Hamiltonian formulation with constraints and its quantization called precanonical quantization. The representations of operators and the covariant Schrödinger equation for TEGR are obtained from the quantization of generalized Dirac brackets calculated according to the analysis of constraints within the polysymplectic formulation of the DW Hamiltonian theory. We argue that the appropriate treatment of the operator ordering and the generalized Hermicity of operators results in an additional c-number term in the DW Hamiltonian operator which is identified with the cosmological constant and estimated to be consistent with its observed value.,"Main_43:   The covariant Hamilton-Jacobi formulation of Maxwell's equations is derived
from the first-order (Palatini-like) Lagrangian using the analysis of
constraints within the De~Donder-Weyl covariant Hamiltonian formalism and the
corresponding polysymplectic reduction.
",0.2
"Cite_43_3: We construct the simplest solutions of the previously obtained precanonical Schrödinger equation for quantum gravity, which correspond to the plane waves on the spin connection bundle and reproduce the Minkowski space-time on average. Quantum fluctuations lead to the emergence of the minimal acceleration a0 related to the range of the Yukawa modes in the fibers of the spin connection bundle. This minimal acceleration is proportional to the square root of the cosmological constant Λ generated by the operator re-ordering in the precanonical Schrödinger equation. Thus the mysterious connection between the minimal acceleration in the dynamics of galaxies as described by Milgrom’s MOND and the cosmological constant emerges as an elementary effect of precanonical quantum gravity. We also argue that the observable values of a0 and Λ can be obtained when the scale of the parameter ϰ introduced by precanonical quantization is subnuclear, in agreement with the previously established connection between the scale of ϰ and the mass gap in quantum SU(2) Yang-Mills theory.","Main_43:   The covariant Hamilton-Jacobi formulation of Maxwell's equations is derived
from the first-order (Palatini-like) Lagrangian using the analysis of
constraints within the De~Donder-Weyl covariant Hamiltonian formalism and the
corresponding polysymplectic reduction.
",0.2
"Cite_43_4: The aim of this paper is to understand the relation between the canonical Hamilton-Jacobi equation for Maxwell's electrodynamics, which is an equation with variational derivatives for a functional of field configurations, and the covariant (De Donder-Weyl) Hamilton-Jacobi equation, which is a partial derivative equation on a finite dimensional space of vector potentials and spacetime coordinates. We show that the procedure of spacetime splitting applied to the latter allows us to reproduce both the canonical Hamilton-Jacobi equation and the Gauss law constraint in the Hamilton-Jacobi form without a recourse to the canonical Hamiltonian analysis. Our consideration may help to analyze the quasi-classical limit of the connection between the standard quantization in field theory based on the canonical Hamiltonian formalism with a preferred time dimension and the precanonical quantization that uses the De Donder-Weyl Hamiltonian formulation where space and time dimensions treated equally.","Main_43:   The covariant Hamilton-Jacobi formulation of Maxwell's equations is derived
from the first-order (Palatini-like) Lagrangian using the analysis of
constraints within the De~Donder-Weyl covariant Hamiltonian formalism and the
corresponding polysymplectic reduction.
",0.2
"Cite_44_1: Two-point fermionic propagators in strongly-correlated media are considered with an emphasis on the dynamical interaction kernels of their equations of motion (EOM). With the many-body Hamiltonian confined by a two-body interaction, the EOMs for the two-point fermionic propagators acquire the Dyson form and, before taking any approximation, the interaction kernels decompose into the static and dynamical (time-dependent) contributions. The latter translate to the energy-dependent and the former map to the energy-independent terms in the energy domain. We dwell particularly on the energy-dependent terms, which generate long-range correlations while making feedback on their short-range static counterparts. The origin, forms, and various approximations for the dynamical kernels of one-fermion and two-fermion propagators, most relevant in the intermediate-coupling regime, are discussed. Applications to the electromagnetic dipole response of Ni and low-energy quadrupole response of Sn are presented.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_2: We adapt the proton-neutron finite-amplitude method, which in its original form is an efficient implementation of the Skyrme quasiparticle random phase approximation, to include the coupling of quasiparticles to like-particle phonons. The approach allows us to add beyond-quasiparticle random-phase approximation correlations to computations of Gamow-Teller strength and 𝛽-decay rates in deformed nuclei for the first time. We test the approach in several deformed isotopes for which measured strength distributions are available. The additional correlations dramatically improve agreement with the data, and will lead to improved global 𝛽-decay rates.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_3: Collective nuclear excitations, like giant resonances, are sensitive to nuclear deformation, as evidenced by alterations in their excitation energies and transition strength distributions. A common theoretical framework to study these collective modes, the random-phase approximation (RPA), has to deal with large dimensions spanned by all possible particle-hole configurations satisfying certain symmetries. It is the aim of this work to establish a new theoretical framework to study the impact of deformation on spin-isospin excitations, that is able to provide fast and reliable solutions of the RPA equations. The nuclear ground state is determined with the axially deformed relativistic Hartree-Bogoliubov (RHB) model based on relativistic point-coupling energy density functionals (EDFs). To study the excitations in the charge-exchange channel, an axially deformed proton-neutron relativistic quasiparticle RPA (pnRQRPA) is developed in the linear response approach. After benchmarking the axially deformed pnRQRPA in the spherical limit, a study of spin-isospin excitations including Fermi, Gamow-Teller (GT), and spin-dipole (SD) is performed for selected 𝑝⁢𝑓-shell nuclei. For GT transitions, it is demonstrated that deformation leads to a considerable fragmentation of the strength function. A mechanism inducing the fragmentation is studied by decomposing the total strength to different projections of total angular momentum 𝐾 and constraining the nuclear shape to either spherical, prolate, or oblate. A similar fragmentation is also observed for SD transitions, although somewhat moderated by the complex structure of these transitions, while, as expected, the Fermi strength is almost shape independent. The axially deformed pnRQRPA introduced in this work open perspectives for the future studies of deformation effects on astrophysically relevant weak interaction processes, in particular beta decay and electron capture.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_4: The r-process is the producer of about 50% total heavy elements existing beyond iron and plays a key role in the neutronization of massive stars and heavier nuclei. β-decay properties of waiting-point nuclei are necessary to determine the r-process path. In this investigation, we study the β-decay properties of N = 81 and 82 neutron-rich nuclei having Z = (42 - 49). We employ the proton-neutron Quasiparticle Random Phase Approximation (pn-QRPA) model to calculate Gamow-Teller transitions, β-decay half-lives, neutron emission probabilities and stellar rates of these neutron-rich isotones. Our investigation includes only allowed transitions. We compare our results with previous calculations and measured data where available. Our model reproduced 100% (82%) terrestrial β-decay half-lives within a factor 10 (2). Our model predictions are in decent agreement with the experimental data. We present the stellar weak rates of these neutron-rich nuclei for the first time. The calculated weak rates were found to be impactful only at low densities (≤ 107 g cm−3) or high temperatures (∼ 30 GK) of the stellar core. The reported stellar rates could prove useful for r-process nucleosynthesis calculations and simulations of late time stellar evolution.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_5: Photoabsorption cross sections for 235 stable nuclei, ranging from ^{40}Ca to ^{209}Bi, were investigated by the quasiparticle finite amplitude method (QFAM) based on the axially deformed relativistic Hartree-Bogoliubov (RHB) approach using relativistic point-coupling interaction DD-PC1, with extensions to odd-A nuclei. GDR parameters based on the standard Lorentzian (SLO) model were extracted from QFAM results and compared with those from experimental data recommended by IAEA. Good agreement was achieved for giant dipole resonance (GDR) peak energies, while resonance widths were underestimated and hence peak cross sections were overestimated due to the lack of higher-order many-body correlations. These discrepancies were much improved in deformed nuclei. The effects of deformation on photoabsorption cross sections were examined systematically. The comparison of photoabsorption cross sections among QFAM results and discrepant experimental data revealed the potential of QFAM calculations in the evaluation of photonuclear data.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_6: These notes summarise the lectures given at the International School of Physics 'Enrico Fermi' in Summer 2024 in Varenna (Italy) about the strongly coupled quantum many-body theory and its applications to nuclear structure. The lectures present a rather short overview of the subject with an emphasis on the analytical aspects of the nuclear many-body problem, aiming at a deep understanding of the complexity of strongly coupled nucleonic states and emergent collective phenomena. The major pedagogical focus is recognizing how all the models describing nuclear dynamics follow from a unified model-independent framework formulated in the universal language of quantum field theory. In particular, connections between the classes of ab initio, density functional theory, and beyond mean-field approaches are made accessible. Approximations of varying complexity are discussed in applications to excited states of medium-heavy nuclei.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_7: Independent particle model in nuclear physics assumes that the nucleon in the nucleus moves in the average (mean field) potential generated by all other nucleons. This chapter gives a short overview of basic features of the independent particle motion in atomic nuclei and its theoretical realization in the framework of shell models for spherical, deformed, and rotating nuclei as well as in more sophisticated approaches such as microscopic+macroscopic model and density functional theories. Independent particle motion of nucleons leads to global and single-particle consequences. The global ones manifest themselves in the shell structure, and its consequences for the global structure of nuclear landscape, the existence of superheavy nuclei, and the superdeformation at high spin are briefly reviewed. The latter shows itself in the single-particle properties such as energies, alignments, and densities; their manifestations are illustrated on specific examples.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_8: Level density and thermodynamic quantities of 250Cm96 and 287Mc115 super-heavy isotopes are calculated based on time dependent pairing energy bach shifted Fermi gas model T DP − BF GM. Woods-Saxon potential is considered for interaction of nucleons inside the nucleus. A temperature dependent pairing energy is also considered. In order to calculate level density and thermodynamic quantities like temperature, entropy and heat capacity of 250Cm96 and 287Mc115 super-heavy isotopes, the level density of theses nuclei are calculated by considering the effects of nuclear rotation and vibration. Variation of level density, entropy, temperature and heat capacity as a function of excitation energy for under consideration isotopes are compared by considering the effects of rotation and vibration. Obtained results on variation of heat capacity as a function of excitation energy indicate well the Cooper pair breaking and cooling effects of these super-heavy isotopes. The novelty of this work is the discontinuity in the specific heat at constant volume for these super-heavy isotopes that are happen in the excitation energies around 2.5 MeV for 250Cm96 and 1MeV for 287Mc115 super-heavy isotopes, which indicates a phase transition from the superfluid state to normal matter.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_44_9: We review the theory of nuclear collective vibrations evolved over decades from phenomenological quasiclassical picture to sophisticated microscopic approaches. The major focus is put on the underlying microscopic mechanisms of emergent effects, which define the properties of giant resonances and soft modes. The response of atomic nuclei to electromagnetic and weak fields is discussed in detail. Astrophysical implications of the giant resonances and soft modes are outlined.","Main_44:   Starting from a general many-body fermionic Hamiltonian, we derive the
equations of motion (EOM) for nucleonic propagators in a superfluid system. The
resulting EOM is of the Dyson type formulated in the basis of Bogoliubov's
quasiparticles. As the leading contributions to the dynamical kernel of this
EOM in strongly-coupled regimes contain phonon degrees of freedom in various
channels, an efficient method of calculating phonon's characteristics is
required to successfully model these kernels. The traditional quasiparticle
random phase approximation (QRPA) solvers are typically used for this purpose
in nuclear structure calculations, however, they become very prohibitive in
non-spherical geometries. In this work, by linking the notion of the
quasiparticle-phonon vertex to the variation of the Bogoliubov's Hamiltonian,
we show that the recently developed finite-amplitude method (FAM) can be
efficiently employed to compute the vertices within the FAM-QRPA. To illustrate
the validity of the method, calculations based on the relativistic
density-dependent point-coupling Lagrangian are performed for the
single-nucleon states in heavy and medium-mass nuclei with axial deformations.
The cases of $^{38}$Si and $^{250}$Cf are presented and discussed.
",0.45
"Cite_45_1: The ability to invent novel and interesting problems is a remarkable feature of human intelligence that drives innovation, art, and science. We propose a method that aims to automate this process by harnessing the power of state-of-the-art generative models to produce a diversity of challenging yet solvable problems, here in the context of Python programming puzzles. Inspired by the intrinsically motivated literature, Autotelic CodE Search (ACES) jointly optimizes for the diversity and difficulty of generated problems. We represent problems in a space of LLM-generated semantic descriptors describing the programming skills required to solve them (e.g. string manipulation, dynamic programming, etc.) and measure their difficulty empirically as a linearly decreasing function of the success rate of 	extit{Llama-3-70B}, a state-of-the-art LLM problem solver. ACES iteratively prompts a large language model to generate difficult problems achieving a diversity of target semantic descriptors (goal-directed exploration) using previously generated problems as in-context examples. ACES generates problems that are more diverse and more challenging than problems produced by baseline methods and three times more challenging than problems found in existing Python programming benchmarks on average across 11 state-of-the-art code LLMs.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_45_2: With the recent Nobel Prize awarded for radical advances in protein discovery, foundation models (FMs) for exploring large combinatorial spaces promise to revolutionize many scientific fields. Artificial Life (ALife) has not yet integrated FMs, thus presenting a major opportunity for the field to alleviate the historical burden of relying chiefly on manual design and trial-and-error to discover the configurations of lifelike simulations. This paper presents, for the first time, a successful realization of this opportunity using vision-language FMs. The proposed approach, called Automated Search for Artificial Life (ASAL), (1) finds simulations that produce target phenomena, (2) discovers simulations that generate temporally open-ended novelty, and (3) illuminates an entire space of interestingly diverse simulations. Because of the generality of FMs, ASAL works effectively across a diverse range of ALife substrates including Boids, Particle Life, Game of Life, Lenia, and Neural Cellular Automata. A major result highlighting the potential of this technique is the discovery of previously unseen Lenia and Boids lifeforms, as well as cellular automata that are open-ended like Conway's Game of Life. Additionally, the use of FMs allows for the quantification of previously qualitative phenomena in a human-aligned way. This new paradigm promises to accelerate ALife research beyond what is possible through human ingenuity alone.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_45_3: Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential computers. The program of an RNN is its weight matrix. How to learn useful representations of RNN weights that facilitate RNN analysis as well as downstream tasks? While the mechanistic approach directly looks at some RNN's weights to predict its behavior, the functionalist approach analyzes its overall functionality-specifically, its input-output mapping. We consider several mechanistic approaches for RNN weights and adapt the permutation equivariant Deep Weight Space layer for RNNs. Our two novel functionalist approaches extract information from RNN weights by 'interrogating' the RNN through probing inputs. We develop a theoretical framework that demonstrates conditions under which the functionalist approach can generate rich representations that help determine RNN behavior. We release the first two 'model zoo' datasets for RNN weight representation learning. One consists of generative models of a class of formal languages, and the other one of classifiers of sequentially processed MNIST digits. With the help of an emulation-based self-supervised learning technique we compare and evaluate the different RNN weight encoding techniques on multiple downstream applications. On the most challenging one, namely predicting which exact task the RNN was trained on, functionalist approaches show clear superiority.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_45_4: Detecting when a neural sequence model does 'interesting, computation is an open problem. The next token prediction loss is a poor indicator: Low loss can stem from trivially predictable sequences that are uninteresting, while high loss may reflect unpredictable but also irrelevant information that can be ignored by the model. We propose a better metric: measuring the model's ability to predict its own future hidden states. We show empirically that this metric -- in contrast to the next token prediction loss -- correlates with the intuitive interestingness of the task. To measure predictability, we introduce the architecture-agnostic prediction of hidden states (PHi) layer that serves as an information bottleneck on the main pathway of the network (e.g., the residual stream in Transformers). We propose a novel learned predictive prior that enables us to measure the novel information gained in each computation step, which serves as our metric. We show empirically that our metric predicts the description length of formal languages learned in-context, the complexity of mathematical reasoning problems, and the correctness of self-generated reasoning chains.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_45_5: Reinforcement Learning (RL) is a subfield of Artificial Intelligence that studies how machines can make decisions by learning from their interactions with an environment. The key aspect of RL is evaluating and improving policies, which dictate the behavior of artificial agents by mapping sensory input to actions. Typically, RL algorithms evaluate these policies using a value function, generally specific to one policy. However, when value functions are updated to track the learned policy, they can forget potentially useful information about previous policies. To address the problem of generalization across many policies, we introduce Parameter-Based Value Functions (PBVFs), a class of value functions that take policy parameters as inputs. A PBVF is a single model capable of evaluating the performance of any policy, given a state, a state-action pair, or a distribution over the RL agent's initial states, and it can generalize across different policies. We derive off-policy actor-critic algorithms based on PBVFs. To input the policy into the value function, we employ a technique called policy fingerprinting. This method compresses the policy parameters, rendering PBVFs invariant to changes in the policy architecture. This policy embedding extracts crucial abstract knowledge about the environment, distilled into a limited number of states sufficient to define the behavior of various policies. A policy can improve solely by modifying actions in such states, following the gradient of the value function's predictions. Extensive experiments demonstrate that our method outperforms evolutionary algorithms, demonstrating a more efficient direct search in the policy space. Furthermore, it achieves performance comparable to that of competitive continuous control algorithms. We apply this technique to learn useful representations of Recurrent Neural Network weight matrices, showing its effectiveness in several supervised learning tasks. Lastly, we empirically demonstrate how this approach can be integrated with HyperNetworks to train a single goal-conditioned neural network (NN) capable of generating deep NN policies that achieve any desired return observed during training.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_45_6: AI has made immense progress in the past 10 years, brought about by the increasing availability of computation, data, and by the invention of flexible algorithmic paradigms to leverage both: machine learning, neural networks, deep reinforcement learning, and large-scale self-supervised learning. However, current AI systems are still missing one of the fundamental drives of human beings: the drive to invent one’s own new problems and learn from striving to achieve them. This drive for creative curiosity is at the center of childrens’ play, scientists’ inventions, artists’ exploration of new forms, and underlies a large part of humanity’s cultural progress. In this thesis we use the framework of autotelic agents, agents that build a repertoire of skills by setting their own goals and learning to achieve them, to take first steps towards truly open-ended AI systems. We argue for language as a support for creative imagination of goals, and as a way to easily access open-ended behavior, as well as a domain where language goals are easy to ground and where we can stand on the shoulders of pretrained models to build capable agents without starting from scratch. We present several empirical contributions to this effect, among others: studying linguistic autotelic agents in the complex ScienceWorld text-based environment showing the impact of sampling goals of intermediate difficulty and appropriate social feedback; demonstrating how to collect open-ended goal repertoires with language models with LMA3, and in the realm of program synthesis, how to generate an open-ended diversity of programming puzzles with ACES, and finally how to frame autotelic learning as a 2-player game with Codeplay. We end the manuscript with a discussion of our results leading to a roadmap for future advances in linguistic autotelic agents and of how the framework could be pushed to build truly creative, open-ended AI systems.","Main_45:   There are two important things in science: (A) Finding answers to given
questions, and (B) Coming up with good questions. Our artificial scientists not
only learn to answer given questions, but also continually invent new
questions, by proposing hypotheses to be verified or falsified through
potentially complex and time-consuming experiments, including thought
experiments akin to those of mathematicians. While an artificial scientist
expands its knowledge, it remains biased towards the simplest, least costly
experiments that still have surprising outcomes, until they become boring. We
present an empirical analysis of the automatic generation of interesting
experiments. In the first setting, we investigate self-invented experiments in
a reinforcement-providing environment and show that they lead to effective
exploration. In the second setting, pure thought experiments are implemented as
the weights of recurrent neural networks generated by a neural experiment
generator. Initially interesting thought experiments may become boring over
time.
",0.3
"Cite_46_1: Extremely large aperture array operating in the near-field regime unlocks additional spatial resources that can be exploited to simultaneously serve multiple users even when they share the same angular direction, a capability not achievable in conventional far-field systems. A fundamental question, however, remains: What is the maximum spatial degree of freedom (DoF) of spatial multiplexing in the distance domain? In this paper, we address this open problem by investigating the spatial DoF of a line-of-sight (LoS) channel between a large two-dimensional transmit aperture and a linear receive array with collinearly-aligned elements (i.e., at the same angular direction) but located at different distances from the transmit aperture. We assume that both the aperture and linear array are continuous-aperture (CAP) arrays with an infinite number of elements and infinitesimal spacing, which establishes an upper bound for the spatial degrees of freedom (DoF) in the case of finite elements. First, we assume an ideal case where the transmit array is a single piece and the linear array is on the broad side of the transmit array. By reformulating the channel as an integral operator with a Hermitian convolution kernel, we derive a closed-form expression for the spatial DoF via the Fourier transform. Our analysis shows that the spatial DoF in the distance domain is predominantly determined by the extreme boundaries of the array rather than its detailed interior structure. We further extend the framework to non-broadside configurations by employing a projection method, which effectively converts the spatial DoF to an equivalent broadside case. Finally, we extend our analytical framework to the modular array, which shows the spatial DoF gain over the single-piece array given the constraint of the physical length of the array.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_2: Extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as a promising technology for next-generation communication systems. However, this will expand the near-field (NF) range, rendering more users more likely to be located in the NF region. In this paper, we aim to answer two questions: What are the new characteristics of the NF channel? Is it necessary to develop new transciver techniques to maintain system performance within the NF region? To this end, we first review current NF channel models and analyze the differences between the existing 3GPP TR 38.901 channel model and the NF channel model, including the spherical wavefront and spatially non-stationarity. Then, we provide examples on how these differences affect the XL-MIMO system performance in terms of beamforming gain and achievable rate. Simulation results demonstrate that, when using far-field (FF) technique under the NF channel, the maximum normalized beam gain loss is less than 3 dB for most users in the NF region defined by Rayleigh distance. Moreover, the achievable rate loss of beam training is less than 3% compared to that realized by NF technique. Finally, we demonstrate the necessity of employing NF transceiver techniques based on simulation results.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_3: This paper presents key performance metrics for near-field communication and sensing systems with a focus on their scaling behavior as a function of the antenna array aperture. Analytical expressions are derived for several standard array geometries to enable the design of the large antenna arrays for given system requirements. First, the near-field beam focusing is analyzed and the minimum beamdepth is observed to rapidly saturate to a low asymptotic limit as the array aperture increases. In contrast, the near-field region span is shown to scale quadratically with the array aperture. Based on these two metrics, the maximum number of resolvable beamspots at 3 dB separation is derived analytically, exhibiting a linear dependence on the array aperture. Finally, the number of significant singular values of a channel observed at the array's broadside is estimated, showing a power-law dependence on the aperture. The resulting expressions provide practical design guidelines for evaluating aperture requirements in near-field communication and sensing applications.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_4: In this letter, a general design criterion is proposed for optimizing non-uniform three-dimensional (3D) arrays to enhance near-field capacity in extremely large-scale MIMO (XL-MIMO) communications. Specifically, an analytical characterization for the asymptotic channel eigenvalues with deployed non-uniform 3D arrays is first presented. Subsequently, with the objective of maximizing system capacity, the array optimization problem is mathematically formulated based on the derived expressions. It is shown that the optimal non-uniform 3D antenna array exhibits a centrifugal arrangement, with elements sparsely arranged at the center and densely packed in the surrounding areas. Numerical results demonstrate a significant capacity gain of the optimized non-uniform 3D arrays over regular arrays.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_5: A novel uniform circular array (UCA) based near-field (NF) integrated sensing and communication (ISAC) framework is proposed, where the Cylindrical coordinate is invoked to evaluate the joint positioning performance. The joint squared position error bound (SPEB) of the sensing target (ST) is derived for the coplanar and non-coplanar cases. For the coplanar case, where the ST is located in the coplanar region of the UCA, the approximate Cram{é}r-Rao bound (CRB) expressions for the separate angle and distance estimation are given by exploiting the uniform spherical wavefront model. A SPEB minimization problem is formulated with the constraints of communication requirement and power budget, where the closed-form solution to minimize the CRB of the angle is derived. Inspired by the close-form expression, a low complexity vector-based quadratic transformation (VQF) algorithm is proposed by invoking the Rayleigh quotient. For the non-coplanar case, where the ST is located beyond the coplanar region of the UCA, the separate CRBs over three-dimensional coordinates and the joint SPEB approximations are derived. To minimize the SPEB performance, the semi-definite relaxation (SDR) method and extended low-complexity VQF algorithm are proposed. Numerical results validated that i) the Fisher Information Matrix about angle and distance in NF propagation can be approximated as a diagonal matrix with the trinity loss; ii) Compared with the uniform planar array, the UCA achieve better positioning performance when ST located in the coplanar of the antenna array; and iii) the proposed VQF algorithms reach higher solution precision than conventional SDR algorithm with much less computation complexity.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_6: As extremely large-scale (XL) arrays advance, near-field (NF) communications have gained significant attention.With this shift, traditional far-field techniques are being revised for compatibility with new XL NF communication paradigms. This work presents NF hybrid beamforming (NF-HBF) approaches for XL-MIMO, focusing on challenges like near-field effects and spatial non-stationarity. First, it redefines the sparse recovery-based NF-HBF problem, shifting from angular- to polar-domain code-books, leading to direct greedy hybrid beamforming (DG-HBF). However, challenges such as high computational complexity, phase shifter (PS) resolution, and spatial non-stationarities persist. To overcome these, this study proposes stepwise-individual and stepwise-joint greedy HBF methods, namely SIG-HBF and SJG-HBF. These methods simplify the process by approximating spherical-wave beams with planar-wave beams, promising lower PS resolution needs, reduced complexity, and the ability to tackle spatially non-stationary channels. Moreover, by exploring conjugate symmetric sequency-ordered Hadamard transforms, NF-HBF can be efficiently achieved using 2-bit PSs with values in {1,−1, j,−j}, facilitated by the SJG-HBF and SJG-HBF methods. Numerical simulations on the proposed methods demonstrate that DG-HBF can approach NF fully-digital beamforming, while SIG-HBF and SJG-HBF highlight the feasibility of utilizing angular-domain codebooks with low PS cost and low memory storage for NF-HBF.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_7: In this work, we novelly exploit a deviation beamfocusing scheme to resolve the performance degradation from small elevation of received signal in near-field communications. Compared to conventional reconfigurable intelligent surface (RIS)-aided scheme in near-field communications, which only adjusts optimal signal phase shifts for transmitter-RIS-user link being applicable to the far-field communications scenario, our proposed scheme takes the angle impact into account and designs the beamfocusing point deviated from user in terms of the distance, elevation and azimuth to overcome such handicap. In addition, we study the factors affecting the elevation angular difference, aiming to enlarge the angular difference with a fixed beamfocusing gain. Simulation and analytical results demonstrate that our proposed scheme outperforms the conventional RIS-aided scheme in the case of incidence signal with small elevation. We also verify the merits of RIS construction in significant enhancing beamfocusing gain compared with the frequency-varying, proving its effectiveness to improve the overall achievable rate.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_8: As one of the key enabler technologies for future networks, terahertz communications (THzComs) are vulnerable to performance degradation and even outage in mobile and near-field scenarios. The emerging integrated sensing and communications (ISAC) technique paves a way to settle the challenges by estimating user position or channel state to achieve beam-alignment, but fails to content the needs of near-field beam-focusing in mobile scenarios. To this end, a closed-form model for terahertz near-field sensing-assisted mobile communications (TNF-SAMC) is provided. Firstly, the TNF-SAMC system models in temporal/frequency/spatial-division schemes are exhibited. Secondly, a near-field four-dimensional sensing framework characterized by Cramér-Rao Bound is put forward. Thirdly, considering the assistance of sensing, the closed-form expressions of Ergodic Shannon Capacity is derived. Next, the theoretical analysis is proceeded to reveal TNF-SAMC performance features in terms of Division Factor, Joint Cramér-Rao Bound, and Critical Point. Especially the simplified closed-form derivations of Critical Points are given, hinting the minimum resources that should be allocated for sensing in TNF-SAMC systems. Then, numerical results verify the conclusion that temporal-division scheme prefers low mobility scenario, spatial-division scheme performs better in beam alignment case, while frequency-division scheme is most superior in TNF-SAMC systems.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_9: This paper compares the sensing performance of a narrowband near-field system across several practical antenna array geometries and SIMO/MISO and MIMO configurations. For identical transmit and receive apertures, MIMO processing is equivalent to squaring the near-field array factor, resulting in improved beamdepth and sidelobe level. Analytical derivations, supported by simulations, show that the MIMO processing improves the maximum near-field sensing range and resolution by approximately a factor of 1.4 compared to a single-aperture system. Using a quadratic approximation of the mainlobe of the array factor, an analytical improvement factor of sqrt{2} is derived, validating the numerical results. Finally, MIMO is shown to improve the poor sidelobe performance observed in the near-field by a factor of two, due to squaring of the array factor.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_10: The large array aperture of extremely large-scale multiple-input multiple-output (XL-MIMO) systems necessitates the consideration of near-field (NF) spherical wavefront, which significantly increases the beam training codebook size and hence training overhead. In this paper, we propose a new effective low-dimensional NF codebook for uniform circular arrays (UCAs) that is designed to focus solely on the azimuth angle of beams, regardless of distance and elevation angle. Specifically, we first reveal that the NF spherical wavefront channel can be approximated by the product of far-field planar wavefront and a phase residual term. Then, a novel codebook is designed for NF UCA beam training by compensating for the residual NF phase. Numerical simulations and experimental measurements demonstrate that the beam pattern is approximately unaffected by the user distance and elevation angle. Furthermore, these assessments show that the proposed codebook can achieve close beamforming performance with that of existing NF codebooks while with a low dimension of codebook size.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_11: The performance of near-field sensing (NISE) in a legacy wideband multiple-input multiple-output (MIMO) orthogonal frequency-division multiplexing (OFDM) communication system is analyzed. The maximum likelihood estimates (MLE) for the target’s distance and angle relative to the antenna array are derived. To evaluate the estimation error, closedform analytical expressions of Cram´er-Rao bounds (CRBs) are derived for both uniform linear arrays (ULAs) and uniform circular arrays (UCAs). The asymptotic CRBs are then analyzed to reveal the scaling laws of CRBs with respect to key system parameters, including array size, bandwidth, and target distance. Our results reveal that 1) the mean-squared error achieved by MLEs approaches CRBs in the high signal-to-noise ratio regime; 2) a larger array aperture does not necessarily improve NISE performance, especially with ultra-large bandwidth; 3) large bandwidth sets an estimation error ceiling for NISE as target distance increases; 4) array aperture and bandwidth, rather than the number of antennas and subcarriers, are the key factors affecting wideband NISE performance; and 5) UCAs offer superior, angle-independent wideband NISE performance compared to ULAs with the same aperture.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_12: As sixth-generation (6G) wireless networks approach, leveraging the millimeter-wave (mmWave) and terahertz (THz) bands’ abundant spectrum becomes crucial, promising ultrahigh data rates and enabling immersive communication experiences. This transformation, characterized by the integration of ultramassive multi-in multi-out (UM-MIMO) systems, facilitates significant increases in throughput and capacity by utilizing densely packed antenna arrays. The resulting shift from traditional far-field communication, with its planar wavefronts, to near-field communication, where spherical wavefronts predominate, necessitates a reevaluation of conventional beamforming and channel estimation methods. Effective codebooks, based on the spherical wavefront phenomenon, are vital for both channel estimation and beam focusing, providing predefined sets of beam steering vectors or codewords needed to efficiently probe the channel and direct the beams toward desired directions. This article explores the development of polar-domain codebooks tailored for these near-field conditions. We reveal the exact boundaries of near-field communication, design polar codebooks based on our sparsity analysis findings, and demonstrate their efficacy in channel estimation and beam training without relying on user range information. These codebooks significantly reduce dimensionality, offering a practical solution to the challenges of minimal pilot overhead. Through two case studies, one on channel estimation and the other on beam training using the defined polar-domain codebooks, we illustrate the potential of these methodologies to enhance system performance and spectral efficiency in near-field wireless systems.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_13: The advent of extremely large antenna arrays (ELAAs) is crucial for meeting the performance demands of future sixth-generation (6G) wireless networks. However, ELAA introduces significant near-field communication (NFC) effects, characterized by spherical wavefront propagation, in contrast to the conventional planar waves observed in far-field models (FFMs). As NFC facilitates precise beamfocusing and spatial multiplexing, it inherently increases the risk of eavesdropping, making physical-layer security (PLS) a critical challenge for safeguarding confidential communication. This article investigates a multiple-input-multiple-output (MIMO) system in the near-field regime, utilizing NFC properties to enhance secrecy performance. Unlike FFMs that rely on the angular domain, our approach leverages both angular and distance domains to achieve robust PLS, even when an eavesdropper shares the same angular direction as a legitimate user. We propose a deep reinforcement learning (DRL)-based solution to optimize beamforming, power allocation, and antenna selection. By minimizing antenna use while maximizing secrecy rates, the approach avoids resource wastage and ensures superior security. Numerical simulations demonstrate significant secrecy rate improvements.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_14: With the further investigation and utilization of the extremely large-scale antenna array (ELAA) and Terahertz (THz) band, as well as the increasingly stringent standards of regulatory agencies, the near-field communications are evolving into the norm of intelligent transportation systems. In this work, we propose a novel beamfocusing scheme to improve the system performance for the reconfigurable intelligent surface (RIS)-aided hybrid near-and far-field communications with multi-unmanned aerial vehicle (UAV). Compared to conventional schemes without considering the near-field beam pattern with a finite depth, in our proposed scheme, the RIS only serves one UAV, while other UAVs are within the radiation range of the concentrated signal energy by beamfocusing. Specifically, we study the polar radius and angular deviations between UAVs with a given beamfocusing gain, aiming to reveal the impact of RIS structure on beamfocusing gain. We also make a fair comparison between the proposed scheme and allocated RIS scheme, deriving the feasible reference distance. In addition, we study the feasibility for different RIS structures focusing on the beamfocusing gain, which has the potential to improve the system performance. Simulation results demonstrate that the proposed scheme is always superior than that of the allocated RIS scheme within the feasible reference distance.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_15: In healthcare environments, accurate and real-time patient localization and monitoring are crucial for ensuring patient safety and improving operational efficiency. This article proposes DeepSense-Healthcare, a hybrid CNN-LSTM framework integrated with adaptive resource management to enhance near-field (NF) IoT-based patient localization and monitoring in healthcare facilities. By combining convolutional neural networks (CNNs) for spatial feature extraction with long short-term memory (LSTM) networks for temporal modeling, DeepSense-Healthcare captures complex spatial-temporal patterns in NF signals, achieving high localization accuracy. An adaptive resource management module is incorporated to optimize computational load, dynamically adjusting resource allocation based on patient activity levels, thereby improving energy efficiency and maintaining responsiveness. We evaluate the proposed framework against baseline models through extensive experiments on the SEED-VIG and ILM datasets across various activity levels. The results demonstrate that DeepSense-Healthcare outperforms conventional methods in localization accuracy, energy efficiency, and latency during various activity scenarios. These findings underscore the effectiveness of DeepSense-Healthcare as a robust and efficient solution for continuous patient monitoring in dynamic healthcare settings.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_16: All-metal array antennas have always been preferred due to their features, such as higher power capacity, higher structural strength, and lower passive intermodulation (PIM). A novel all-metal modular tightly coupled dipole array (TCDA) is proposed and realized for the first time in this article. The proposed TCDA has uniquely shaped planar dipole arms, effectively minimizing the antenna profile. Meanwhile, the novel element facilitates the modular assembly of arrays with arbitrary sizes and shapes, enhancing the ease of replacement in case of damage to some elements. The array edge truncation effect can, moreover, be alleviated by just using half-sized elements (HSEs), thereby effectively reducing the aperture sizes of traditional TCDAs with dummy elements. Simulation results show that the proposed TCDA is able to operate at 4.67:1 bandwidth (0.45–2.1 GHz) for broadside, and is able to scan up to 60° in the E-D-plane and 45° in the H-plane within 0.5–2 GHz, with a rather low profile of only 0.48λH at the highest operating frequency. The proposed all-metal TCDA of a 3×9 modular rectangular array, a ring-shaped array, and a cross-shaped array were realized by 3-D printing technique without any welding. Good agreement is achieved between the simulated and measured results.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_17: Utilizing device mobility to form virtual large-scale antenna arrays can provide accurate angle-of-arrival (AoA) information for robots. However, existing wireless localization systems that exploit device mobility are designed based on far-field channel assumptions and cannot directly provide range estimates. To address this problem, in this paper, we develop a novel near-field localization architecture for mobile robots by fusing the robot’s motion trajectory and channel state information (CSI) of a single antenna. Specifically, we first utilize channel reciprocity to multiply the uplink CSI and downlink CSI to eliminate the phase offset. Second, we further propose a two-stage localization algorithm that separates the line-of-sight (LoS) path from the multipath, and a multi-scale iterative scheme is employed to refine the estimation of AoA and distance of the LoS path. In addition, the range and AoA profiles for different trajectory shapes and the Cramer-Rao bounds for localization accuracy under squared channels are derived. Finally, the effectiveness of the proposed system is verified in a real environment. The simulation and experimental test results show that the proposed near-field localization system can operate in complex channel environments, and its localization accuracy outperforms the existing schemes.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_18: With the flourishing development of sixth-generation wireless networks, the demand of spectrum efficiency rapidly increases, in order to support the demands of high-quality data transmission and connections of massive users. Among various promising technologies, the technology of near-field communications provides a great potential to address such an issue due to the unique spherical-wave channels for electromagnetic (EM) propagation. In this article, multiple-input-single-output (MISO) near-field communications is comprehensively studied to clarify the influences of the shape and structure of uniform planar array (UPA) on the system performance, considering three cases with uniform linear array (ULA), rectangular UPA, and circular UPA. In particular, we reveal the properties of rapid deterioration for signal-to-noise ratio (SNR) from the reduced projection aperture in near-field communications and investigate the single spherical crown antenna design and spherical crown antenna array design in order to address this issue. Moreover, we also characterize the role of antenna projection aperture in detail, and theoretically analyze the shape of UPA, yielding the corresponding exact closed-form expressions for SNR and outage probability (OP). Based on these above analytical results, we find out that adjusting the spacing between adjacent antennas to control the relative angle between user and selected antennas is an efficient way to improve the projection aperture of antenna and SNR. Simulation results are shown to well match analytical results, which validate the correctness of our analysis, clarifying that our proposed designs outperform the conventional works and illustrating a better stability for angle variations.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_46_19: Localization is expected to play a significant role in future wireless networks, as positioning and situational awareness, navigation and tracking, are integral parts of 6G usage scenarios. Nevertheless, in many cases localization requires extra equipment, which interferes with communications systems, while also requiring additional resources. On the other hand, high frequency and highly directional communications offer a new framework of improved resolution capabilities in the angular and range domains. The implementation of integrated sensing and communications is being explored to unify the sensing and communications systems and promote a communicate-to-sense approach. To this end, a localization algorithm is presented that utilizes beam-forming and the emerging beam-focusing technique, to estimate the location of the receiver. The algorithm can be implemented with large antenna arrays, and large intelligent surfaces. The performance of the algorithm for static and mobile users is evaluated through Monte-Carlo simulations. The results are presented with the empirical CDF for both static and mobile users, and the probability of successful estimation for static users.","Main_46: Massive multiple-input multiple-output (MIMO) for 5G is evolving into the
extremely large-scale antenna array (ELAA) to increase the spectrum efficiency
by orders of magnitude for 6G communications. ELAA introduces
spherical-wave-based near-field communications, where channel capacity can be
significantly improved for single-user and multi-user scenarios. Unfortunately,
for the widely studied uniform linear array (ULA), the near-field regions at
large incidence angles will be reduced. Thus, many users randomly distributed
in a cell may fail to benefit from near-field communications. In this paper, we
leverage the rotational symmetry of uniform circular array (UCA) to provide
uniform and enlarged near-field region for all users in a cell, enabling more
users to benefit from near-field communications. Specifically, by exploiting
the geometrical relationship between UCA and user with the spherical-wave
model, the near-field beamforming technique for UCA is developed for the first
time. Based on the analysis of near-field beamforming, we reveal that UCA is
able to provide a larger near-field region than ULA in terms of the effective
Rayleigh distance. Moreover, based on the UCA beamforming property, a
concentric-ring codebook is designed to realize efficient beamforming in the
near-field region of UCA. In addition, we find out that UCA could generate
orthogonal near-field beams along the same direction, which has the potential
for further improvement of multi-user capacity compared with ULA. Simulation
results are provided to verify the feasibility of UCA to enable more users to
benefit from near-field communications by broadening the near-field region.
",0.95
"Cite_47_1: We propose a gauged 𝑈⁢(1)𝐵−𝐿 version of the light Dirac neutrino portal dark matter. The 𝑈⁢(1)𝐵−𝐿 symmetry provides a UV completion by naturally accommodating three right-handed neutrinos from anomaly cancellation requirements which, in combination with the left-handed neutrinos, form the sub-eV Dirac neutrinos after electroweak symmetry breaking. The particle content and the gauge charges are chosen in such a way that light neutrinos remain purely Dirac and dark matter, a gauge singlet Dirac fermion, remain stable. We consider both thermal and nonthermal production possibilities of dark matter and correlate the corresponding parameter space with the one within reach of future cosmic microwave background (CMB) experiments sensitive to enhanced relativistic degrees of freedom Δ⁢𝑁eff. The interplay of dark matter, CMB, structure formation and other terrestrial constraints keep the scenario very predictive leading the 𝑈⁢(1)𝐵−𝐿 parameter space into tight corners.","Main_47: A dark matter (DM) having feeble interaction with the visible sector can
thermalise via substantial interaction with a Weakly Interacting Massive
Particle (WIMP). Such DM candidates are categorised as pseudo-FIMP (pFIMP).
pFIMP can provide both direct and indirect search prospects via WIMP loop. This
work focuses into such possibilities. We provide all such one loop graphs
involving scalar, fermion and vector boson particles via which pFIMP can
interact with the Standard Model assuming both of them are stabilised via
$\mathbb{Z}_2\otimes \mathbb{Z}_2^{\prime}$ symmetries. We elaborate upon a
model where a fermion DM acts as WIMP and a scalar singlet acts as pFIMP having
negligible Higgs portal interaction and substantial conversion via Yukawa
interaction. We study in details the loop induced direct and indirect search
prospects of the pFIMP in the relic density allowed region of the model.
",0.15
"Cite_47_2: The dynamics and detection possibility of a pseudo-FIMP (pFIMP) dark matter (DM) in the presence of a thermal DM have been studied in different contexts. The pFIMP phenomenology largely depends on the WIMP-like partner DM, as pFIMP interacts with the standard model (SM) particles only via the partner DM loop. Introducing a lepton portal interaction, which connects DM directly to the SM lepton sector, improves its detection prospects. However, such possibilities are constrained strongly by the non-observation of lepton flavor-violating decays. Interestingly, this also makes it possible to probe such models in future low-energy experiments. In this article, we have tried to establish such connections and find parameter space which respects the limits from DM relic, direct, indirect, and lepton flavor violation (LFV). We also recast the constraints from di-lepton/di-tau plus missing energy signal at the LHC on our model and provide projections for HL-LHC and future lepton colliders. Although the LFV and collider limits mainly concern WIMPs, the parameter space for pFIMPs is also constrained due to its strong connection to WIMPs through DM relic density and detection prospects.","Main_47: A dark matter (DM) having feeble interaction with the visible sector can
thermalise via substantial interaction with a Weakly Interacting Massive
Particle (WIMP). Such DM candidates are categorised as pseudo-FIMP (pFIMP).
pFIMP can provide both direct and indirect search prospects via WIMP loop. This
work focuses into such possibilities. We provide all such one loop graphs
involving scalar, fermion and vector boson particles via which pFIMP can
interact with the Standard Model assuming both of them are stabilised via
$\mathbb{Z}_2\otimes \mathbb{Z}_2^{\prime}$ symmetries. We elaborate upon a
model where a fermion DM acts as WIMP and a scalar singlet acts as pFIMP having
negligible Higgs portal interaction and substantial conversion via Yukawa
interaction. We study in details the loop induced direct and indirect search
prospects of the pFIMP in the relic density allowed region of the model.
",0.15
"Cite_47_3: More than one dark sector particle transforming under the same symmetry provides one stable dark matter (DM) component which undergoes co-annihilation with the heavier particle(s) decaying to DM. Specific assumptions on the kinematics and on the coupling parameters may render the heavier component(s) stable and contribute as DM. The choices of the charges of the dark sector fields under transformation play a crucial role in the resultant phenomenology. In this paper, we systematically address the possibility of obtaining two scalar DM components under Z symmetry. We consider both the possibilities of DM being weakly interacting massive particle (WIMP) or pseudofeebly interacting massive particle (pFIMP). We elaborate upon Z symmetric model, confronting the relic density allowed parameter space with recent most direct and indirect search bounds and prospects. We also highlight the possible distinction of the allowed parameter space in single component and two component cases, as well as between WIMP-WIMP and WIMP-pFIMP scenarios.","Main_47: A dark matter (DM) having feeble interaction with the visible sector can
thermalise via substantial interaction with a Weakly Interacting Massive
Particle (WIMP). Such DM candidates are categorised as pseudo-FIMP (pFIMP).
pFIMP can provide both direct and indirect search prospects via WIMP loop. This
work focuses into such possibilities. We provide all such one loop graphs
involving scalar, fermion and vector boson particles via which pFIMP can
interact with the Standard Model assuming both of them are stabilised via
$\mathbb{Z}_2\otimes \mathbb{Z}_2^{\prime}$ symmetries. We elaborate upon a
model where a fermion DM acts as WIMP and a scalar singlet acts as pFIMP having
negligible Higgs portal interaction and substantial conversion via Yukawa
interaction. We study in details the loop induced direct and indirect search
prospects of the pFIMP in the relic density allowed region of the model.
",0.15
"Cite_48_1: The critical two-dimensional Stochastic Heat Flow (SHF) is the scaling limit of the directed polymers in random environments and the noise-mollified Stochastic Heat Equation (SHE), at the critical dimension of two and near the critical temperature. The work of Caravenna, Sun, and Zygouras (2023) proved that the discrete polymers converge to a universal (model-independent) limit, thereby identifying the limit as the SHF. In this work, we take a different approach: We formulate a set of axioms for the SHF, prove the uniqueness in law under these axioms, and also prove the existence by showing that the noise-mollified SHE converges to the SHF under this formulation. The set consists of three axioms commonly seen in stochastic flows and one axiom about the first four moments of the SHF over every time interval.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_2: A spin-glass system with a smooth or fractal outer surface is studied by renormalization-group theory, in bulk spatial dimension. Independently varying the surface and bulk random-interaction strengths, phase diagrams are calculated. The smooth surface does not have spin-glass ordering in the absence of bulk spin-glass ordering and always has spin-glass ordering when the bulk is spin-glass ordered. With fractal () surfaces, a sponge is obtained and has surface spin-glass ordering also in the absence of bulk spin-glass ordering. The phase diagram has the only-surface-spin-glass ordered phase, the bulk and surface spin-glass ordered phase, and the disordered phase, and a special multicritical point where these three phases meet. All spin-glass phases have distinct chaotic renormalization-group trajectories, with distinct Lyapunov and runaway exponents which we have calculated.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_3: A nematic phase, previously seen in the classical Heisenberg spin-glass system, occurs in the n-component cubic-spin spin-glass system, between the low-temperature spin-glass phase and the-high temperature disordered phase, for number of components, in spatial dimension, thus constituting a liquid-crystal phase in a dirty (quenched-disordered) magnet. This result is obtained from renormalization-group calculations that are exact on the hierarchical lattice and, equivalently, approximate on the cubic spatial lattice. The nematic phase completely intervenes between the spin-glass phase and the disordered phase. The Lyapunov exponents of the spin-glass chaos are calculated from up to and show odd-even oscillations with respect to .","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_4: A nematic phase, previously seen in the 𝑑=3 classical Heisenberg spin-glass system, occurs in the 𝑛-component cubic-spin spin-glass system, between the low-temperature spin-glass phase and the high-temperature disordered phase, for number of spin components 𝑛≥3, in spatial dimension 𝑑=3, thus constituting a liquid-crystal phase in a dirty (quenched-disordered) magnet. Furthermore, under application of a variety of uniform magnetic fields, a veritable plethora of phases is found. Under uniform magnetic fields, 17 different phases and two spin-glass phase diagram topologies (meaning the occurrences and relative positions of the many phases), qualitatively different from the conventional spin-glass phase diagram topology, are seen. The chaotic rescaling behaviors and their Lyapunov exponents are calculated in each of these spin-glass phase diagram topologies. These results are obtained from renormalization-group calculations that are exact on the 𝑑=3 hierarchical lattice and, equivalently, approximate on the cubic spatial lattice. Axial, planar-diagonal, or body-diagonal finite-strength uniform fields are applied to 𝑛=2 and 3 component cubic-spin spin-glass systems in 𝑑=3.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_5: The global phase diagrams of the Ashkin–Teller model are calculated in and 3 by renormalization-group theory that is exact on the hierarchical lattice and approximate on the recently improved Migdal–Kadanoff procedure. Three different ordered phases occur in the dimensionally distinct phase diagrams that reflect three-fold order-parameter permutation symmetry, a closed symmetry line, and a quasi-disorder line. First- and second-order phase boundaries are obtained. In , second-order phase transitions meeting at a bifurcation point are seen. In , first- and second-order phase transitions are separated by tricritical and critical endpoints.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_6: A model that merges the Potts, cubic, and clock models is studied in spatial dimension d=3 by renormalization-group theory. Effective vacancies are included in the renormalization-group initial conditions. In the global phase diagram, 5 different ordered phases, namely ferromagnetic, antiferromagnetic, ferrimagnetic, antiferrimagnetic, axial, and a disordered phase are found, separated by first- and second-order phase boundaries. 8 different phase diagram cross-sections occur. When the effective vacancies are suppressed, the global spinodal phase diagram is found: All disordering phase transitions become second order, the disordered phase recedes, and 17 different phase diagram cross-sections occur, spinodality thus much enriching ordering behavior. In the spinodal phase diagram, the ferrimagnetic and antiferrimagnetic phases have reentrance. The employed renormalization group transformation is exact on the d=3 dimensional hierarchical model and Migdal-Kadanoff approximate on the cubic lattice.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_7: The random-magnetic-field classical Heisenberg model is solved in spatial dimensions 𝑑≥2 using the recently developed Fourier-Legendre renormalization-group theory for 4⁢𝜋 steradians continuously orientable spins, with renormalization-group flows of 12 500 variables. The random-magnetic-field Heisenberg model is exactly solved in 10 hierarchical models, for 𝑑=2,2.26,2.46,2.58,2.63,2.77,2.89,3. For nonzero random fields, ferromagnetic order is seen for 𝑑>2. This ordering, at 𝑑=2.46,2.58,2.63,2.77,2.89,3, shows reentrance as a function of temperature.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_8: A spin system is studied with simultaneous permutation-symmetric Potts and spin-rotation-symmetric clock interactions in spatial dimensions 𝑑=2 and 3. The global phase diagram is calculated from the renormalization-group solution with the recently improved (spontaneous first-order detecting) Migdal-Kadanoff approximation or, equivalently, with hierarchical lattices with the inclusion of effective vacancies. Five different ordered phases are found: Conventionally ordered ferromagnetic, quadrupolar, antiferromagnetic phases and algebraically ordered antiferromagnetic, antiquadrupolar phases. These five different ordered phases and the disordered phase are mutually bounded by first- and second-order phase transitions, themselves delimited by multicritical points: Inverted bicritical, zero-temperature bicritical, tricritical, second-order bifurcation, and zero-temperature highly degenerate multicritical points. One rich phase diagram topology exhibits all of these phenomena.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_9: The phase diagram of the Ashkin-Tellerized XY model in spatial dimension d=3 is calculated by renormalization-group theory. In this system, each site has two spins, each spin being an XY spin, that is having orientation continuously varying in 2 pi radians. Nearest-neighbor sites are coupled by two-spin and four-spin interactions. The phase diagram has ordered phases that are ferromagnetic and antiferromagnetic in each of the spins, and phases that are ferromagnetic and antiferromagnetic in the multiplicative spin variable. The phase diagram exhibits two symmetrically situated reverse bifurcation points of the phase boundaries. The renormalization-group flows are in terms of the doubly composite Fourier coefficients of the exponentiated energy of nearest-neighbor spins.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_10: The phase diagram of the Ashkin-Tellerized XY model in spatial dimension is calculated by renormalization-group theory. In this system, each site has two spins, each spin being an XY spin, that is having orientation continuously varying in  radians. Nearest-neighbor sites are coupled by two-spin and four-spin interactions. The phase diagram has ordered phases that are ferromagnetic and antiferromagnetic in each of the spins, and phases that are ferromagnetic and antiferromagnetic in the multiplicative spin variable. The phase diagram distinctively exhibits a pair of symmetrically situated direct bifurcation points and a pair of symmetrically situated reverse bifurcation points of the phase boundaries. The renormalization-group flows are in terms of the doubly composite Fourier coefficients of the exponentiated energy of nearest-neighbor spins.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_11: A nematic phase, previously seen in the classical Heisenberg spin-glass system, occurs in the n-component cubic-spin spin-glass system, between the low-temperature spin-glass phase and the-high temperature disordered phase, for number of components, in spatial dimension, thus constituting a liquid-crystal phase in a dirty (quenched-disordered) magnet. This result is obtained from renormalization-group calculations that are exact on the hierarchical lattice and, equivalently, approximate on the cubic spatial lattice. The nematic phase completely intervenes between the spin-glass phase and the disordered phase. The Lyapunov exponents of the spin-glass chaos are calculated from up to and show odd-even oscillations with respect to n.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_48_12: A spin-glass system with a smooth or fractal outer surface is studied by renormalization-group theory, in bulk spatial dimension. Independently varying the surface and bulk random-interaction strengths, phase diagrams are calculated. The smooth surface does not have spin-glass ordering in the absence of bulk spin-glass ordering and always has spin-glass ordering when the bulk is spin-glass ordered. With fractal () surfaces, a sponge is obtained and has surface spin-glass ordering also in the absence of bulk spin-glass ordering. The phase diagram has the only-surface-spin-glass ordered phase, the bulk and surface spin-glass ordered phase, and the disordered phase, and a special multicritical point where these three phases meet. All spin-glass phases have distinct chaotic renormalization-group trajectories, with distinct Lyapunov and runaway exponents which we have calculated.","Main_48:   We study models for a directed polymer in a random environment (DPRE) in
which the polymer traverses a hierarchical diamond graph and the random
environment is defined through random variables attached to the vertices. For
these models, we prove a distributional limit theorem for the partition
function in a limiting regime wherein the system grows as the coupling of the
polymer to the random environment is appropriately attenuated. The sequence of
diamond graphs is determined by a choice of a branching number $b\in
\{2,3,\ldots\}$ and segmenting number $s\in \{2,3,\ldots\}$, and our focus is
on the critical case of the model where $b=s$. This extends recent work in the
critical case of analogous models with disorder variables placed at the edges
of the graphs rather than the vertices.
",0.6
"Cite_50_1: Stochastic optimal control and games have a wide range of applications, from finance and economics to social sciences, robotics, and energy management. Many real-world applications involve complex models that have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for solving stochastic control problems and games. In this review, we focus on deep learning methods that have unlocked the possibility of solving such problems, even in high dimensions or when the structure is very complex, beyond what traditional numerical methods can achieve. We consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network-based methods for solving high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough results. This paper provides an introduction to these methods and summarizes the state-of-the-art works at the crossroad of machine learning and stochastic control and games.","Main_50:   We propose a new probabilistic scheme which combines deep learning techniques
with high order schemes for backward stochastic differential equations
belonging to the class of Runge-Kutta methods to solve high-dimensional
semi-linear parabolic partial differential equations. Our approach notably
extends the one introduced in [Hure Pham Warin 2020] for the implicit Euler
scheme to schemes which are more efficient in terms of discrete-time error. We
establish some convergence results for our implemented schemes under classical
regularity assumptions. We also illustrate the efficiency of our method for
different schemes of order one, two and three. Our numerical results indicate
that the Crank-Nicolson schemes is a good compromise in terms of precision,
computational cost and numerical implementation.
",0.25
"Cite_50_2: Backward Stochastic Differential Equations (BSDEs) have been widely employed in various areas of social and natural sciences, such as the pricing and hedging of financial derivatives, stochastic optimal control problems, optimal stopping problems and gene expression. Most BSDEs cannot be solved analytically and thus numerical methods must be applied to approximate their solutions. There have been a variety of numerical methods proposed over the past few decades as well as many more currently being developed. For the most part, they exist in a complex and scattered manner with each requiring a variety of assumptions and conditions. The aim of the present work is thus to systematically survey various numerical methods for BSDEs, and in particular, compare and categorize them, for further developments and improvements. To achieve this goal, we focus primarily on the core features of each method based on an extensive collection of 333 references: the main assumptions, the numerical algorithm itself, key convergence properties and advantages and disadvantages, to provide an up-to-date coverage of numerical methods for BSDEs, with insightful summaries of each and a useful comparison and categorization.","Main_50:   We propose a new probabilistic scheme which combines deep learning techniques
with high order schemes for backward stochastic differential equations
belonging to the class of Runge-Kutta methods to solve high-dimensional
semi-linear parabolic partial differential equations. Our approach notably
extends the one introduced in [Hure Pham Warin 2020] for the implicit Euler
scheme to schemes which are more efficient in terms of discrete-time error. We
establish some convergence results for our implemented schemes under classical
regularity assumptions. We also illustrate the efficiency of our method for
different schemes of order one, two and three. Our numerical results indicate
that the Crank-Nicolson schemes is a good compromise in terms of precision,
computational cost and numerical implementation.
",0.25
"Cite_50_3: Recently, several deep learning (DL) methods for approximating high-dimensional partial differential equations (PDEs) have been proposed. The interest that these methods have generated in the literature is in large part due to simulations which appear to demonstrate that such DL methods have the capacity to overcome the curse of dimensionality (COD) for PDEs in the sense that the number of computational operations they require to achieve a certain approximation accuracy  grows at most polynomially in the PDE dimension  and the reciprocal of . While there is thus far no mathematical result that proves that one of such methods is indeed capable of overcoming the COD, there are now a number of rigorous results in the literature that show that deep neural networks (DNNs) have the expressive power to approximate PDE solutions without the COD in the sense that the number of parameters used to describe the approximating DNN grows at most polynomially in both the PDE dimension  and the reciprocal of the approximation accuracy . Roughly speaking, in the literature it is has been proved for every  that solutions , , of semilinear heat PDEs with Lipschitz continuous nonlinearities can be approximated by DNNs with ReLU activation at the terminal time in the -sense without the COD provided that the initial value functions , , can be approximated by ReLU DNNs without the COD. It is the key contribution of this work to generalize this result by establishing this statement in the -sense with  and by allowing the activation function to be more general covering the ReLU, the leaky ReLU, and the softplus activation functions as special cases.","Main_50:   We propose a new probabilistic scheme which combines deep learning techniques
with high order schemes for backward stochastic differential equations
belonging to the class of Runge-Kutta methods to solve high-dimensional
semi-linear parabolic partial differential equations. Our approach notably
extends the one introduced in [Hure Pham Warin 2020] for the implicit Euler
scheme to schemes which are more efficient in terms of discrete-time error. We
establish some convergence results for our implemented schemes under classical
regularity assumptions. We also illustrate the efficiency of our method for
different schemes of order one, two and three. Our numerical results indicate
that the Crank-Nicolson schemes is a good compromise in terms of precision,
computational cost and numerical implementation.
",0.25
"Cite_50_4: We analyze a stochastic optimal control problem for the PReP vaccine in a model for the spread of HIV. To do so, we use a stochastic model for HIV/AIDS with PReP, where we include jumps in the model. This generalizes previous works in the field. First, we prove that there exists a positive, unique, global solution to the system of stochastic differential equations which makes up the model. Further, we introduce a stochastic control problem for dynamically choosing an optimal percentage of the population to receive PReP. By using the stochastic maximum principle, we derive an explicit expression for the stochastic optimal control. Furthermore, via a generalized Lagrange multiplier method in combination with the stochastic maximum principle, we study two types of budget constraints. We illustrate the results by numerical examples, both in the fixed control case and in the stochastic control case.","Main_50:   We propose a new probabilistic scheme which combines deep learning techniques
with high order schemes for backward stochastic differential equations
belonging to the class of Runge-Kutta methods to solve high-dimensional
semi-linear parabolic partial differential equations. Our approach notably
extends the one introduced in [Hure Pham Warin 2020] for the implicit Euler
scheme to schemes which are more efficient in terms of discrete-time error. We
establish some convergence results for our implemented schemes under classical
regularity assumptions. We also illustrate the efficiency of our method for
different schemes of order one, two and three. Our numerical results indicate
that the Crank-Nicolson schemes is a good compromise in terms of precision,
computational cost and numerical implementation.
",0.25
"Cite_50_5: A deep BSDE approach is presented for the pricing and delta-gamma hedging of high-dimensional Bermudan options, with applications in portfolio risk management. Large portfolios of a mixture of multi-asset European and Bermudan derivatives are cast into the framework of discretely reflected BSDEs. This system is discretized by the One Step Malliavin scheme (Negyesi et al. [2024, 2025]) of discretely reflected Markovian BSDEs, which involves a Gamma process, corresponding to second-order sensitivities of the associated option prices. The discretized system is solved by a neural network regression Monte Carlo method, efficiently for a large number of underlyings. The resulting option Deltas and Gammas are used to discretely rebalance the corresponding replicating strategies. Numerical experiments are presented on both high-dimensional basket options and large portfolios consisting of multiple options with varying early exercise rights, moneyness and volatility. These examples demonstrate the robustness and accuracy of the method up to 100 risk factors. The resulting hedging strategies significantly outperform benchmark methods both in the case of standard delta- and delta-gamma hedging.","Main_50:   We propose a new probabilistic scheme which combines deep learning techniques
with high order schemes for backward stochastic differential equations
belonging to the class of Runge-Kutta methods to solve high-dimensional
semi-linear parabolic partial differential equations. Our approach notably
extends the one introduced in [Hure Pham Warin 2020] for the implicit Euler
scheme to schemes which are more efficient in terms of discrete-time error. We
establish some convergence results for our implemented schemes under classical
regularity assumptions. We also illustrate the efficiency of our method for
different schemes of order one, two and three. Our numerical results indicate
that the Crank-Nicolson schemes is a good compromise in terms of precision,
computational cost and numerical implementation.
",0.25
"Cite_51_1: Cervical cancer is one of the primary causes of death in women. It should be diagnosed early and treated according to the best medical advice, similar to other diseases, to ensure that its effects are as minimal as possible. Pap smear images are one of the most constructive ways for identifying this type of cancer. This study proposes a cross-attention-based Transfomer approach for the reliable classification of cervical cancer in pap smear images. In this study, we propose the CerviFormer-a model that depends on the Transformers and thereby requires minimal architectural assumptions about the size of the input data. The model uses a cross-attention technique to repeatedly consolidate the input data into a compact latent Transformer module, which enables it to manage very large-scale inputs. We evaluated our model on two publicly available pap smear datasets. For 3-state classification on the Sipakmed data, the model achieved an accuracy of 96.67%. For 2-state classification on the Herlev data, the model achieved an accuracy of 94.57%. Experimental results on two publicly accessible datasets demonstrate that the proposed method achieves competitive results when compared to contemporary approaches. The proposed method brings forth a comprehensive classification model to detect cervical cancer in pap smear images. This may aid medical professionals in providing better cervical cancer treatment, consequently, enhancing the overall effectiveness of the entire testing process.","Main_51: The energy transition towards photovoltaic solar energy has evolved to be a
viable and sustainable source for the generation of electricity. It has
effectively emerged as an alternative to the conventional mode of electricity
generation for developing countries to meet their energy requirement. Thus,
many solar power plants have been set up across the globe. However, in these
large-scale or remote solar power plants, monitoring and maintenance persist as
challenging tasks, mainly identifying faulty or malfunctioning cells in
photovoltaic (PV) panels. In this paper, we use an unsupervised deep-learning
image segmentation model for the detection of internal faults such as hot spots
and snail trails in PV panels. Generally, training or ground truth labels are
not available for large solar power plants, thus the proposed model is highly
recommended as it does not require any prior learning or training. It extracts
the features from the input image and segments out the faults in the image.
Here we use infrared thermal images of the PV panel as input, passed to a
convolutional neural network which assigns cluster labels to the pixels.
Further, optimize the pixel labels, features and model parameters using
backpropagation based on iterative stochastic gradient descent. Then, we
compute similarity loss and spatial continuity loss to assign the same label to
the pixel with similar features and spatial continuity to reduce noises in the
image segmentation process. The effectiveness of the proposed approach was
examined on an online available dataset for the recognition of snail trails and
hot spot failures in monocrystalline solar panels.
",0.1
"Cite_51_2: In recent years, renewable energy has become a high-priority choice in utility companies to reduce their carbon emission footprint and cut overall energy costs. As a result, worldwide utility companies are building solar farms to supply electricity for operation usage. Since solar PV panels are relatively new assets for the organization to manage, organizations are continuously seeking insightful understandings to develop and optimize their solar panel maintenance programs. For example, some organizations utilize Unmanned Aerial Vehicle (UAV) technologies combined with an AI-driven image processing technique for condition monitoring to improve the efficiency and safety of the solar maintenance program. While the emerging AI technologies sound promising, this research found that the existing solar panel maintenance programs could still be time-consuming, unsafe, and costly as they might require personnel to visually inspect these assets due to various issues associated with the AI technologies. Therefore, based on a case study, this research investigated the challenges and issues in the existing solar panel maintenance process from technology, organization, and people perspectives. These findings can contribute to a more effective and efficient commercial solar panel maintenance approach.","Main_51: The energy transition towards photovoltaic solar energy has evolved to be a
viable and sustainable source for the generation of electricity. It has
effectively emerged as an alternative to the conventional mode of electricity
generation for developing countries to meet their energy requirement. Thus,
many solar power plants have been set up across the globe. However, in these
large-scale or remote solar power plants, monitoring and maintenance persist as
challenging tasks, mainly identifying faulty or malfunctioning cells in
photovoltaic (PV) panels. In this paper, we use an unsupervised deep-learning
image segmentation model for the detection of internal faults such as hot spots
and snail trails in PV panels. Generally, training or ground truth labels are
not available for large solar power plants, thus the proposed model is highly
recommended as it does not require any prior learning or training. It extracts
the features from the input image and segments out the faults in the image.
Here we use infrared thermal images of the PV panel as input, passed to a
convolutional neural network which assigns cluster labels to the pixels.
Further, optimize the pixel labels, features and model parameters using
backpropagation based on iterative stochastic gradient descent. Then, we
compute similarity loss and spatial continuity loss to assign the same label to
the pixel with similar features and spatial continuity to reduce noises in the
image segmentation process. The effectiveness of the proposed approach was
examined on an online available dataset for the recognition of snail trails and
hot spot failures in monocrystalline solar panels.
",0.1
"Cite_52_1: Reliability of quantum-chemical calculations based upon the density functional theory and its 1-matrix counterpart hinges upon minimizing the extent of empirical parameterization in the approximate energy expressions of these formalisms while imposing as many rigorous constraints upon them as possible. The recently uncovered universal properties of the natural orbitals facilitate the construction of such constraints for the 1-matrix functionals. The benefits of their employment in the validation of these functionals are vividly demonstrated by a critical review of the three incarnations of the so-called Collins conjecture. Although the incorporation of rigorous definitions of the correlation energy and entropy, and the identification of individual potential energy hypersurfaces as probable domains of its applicability turn the originally published unsubstantiated claim into a proper conjecture, the resulting formalism is found to be merely a conduit for incorporation of static correlation effects in electronic structure calculations that is unlikely to allow attaining chemical accuracy.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_2: A major challenge for density functional theory (DFT) is its failure to treat static correlation, yielding errors in predicted charges, band gaps, van der Waals forces, and reaction barriers. Here we combine one- and two-electron reduced density matrix (1- and 2-RDM) theories with DFT to obtain a universal 𝑂⁡(𝑁3) generalization of DFT for static correlation. Using the lowest unitary invariant of the cumulant 2-RDM, we generate a 1-RDM functional theory that corrects the convexity of any DFT functional to capture static correlation in its fractional orbital occupations. Importantly, the unitary invariant yields a predictive theory by revealing the dependence of the correction’s strength upon the trace of the two-electron repulsion matrix. We apply the theory to the barrier to rotation in ethylene, the relative energies of the benzynes, as well as an 11-molecule, dissociation benchmark. By inheriting the computational efficiency of DFT without sacrificing the treatment of static correlation, the theory opens new possibilities for the prediction and interpretation of significant quantum molecular effects and phenomena.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_3: The unrestricted Hartree-Fock method is extended to correlation calculation within the density-matrix functional theory. The method is derived from an entropic cumulant functional for the correlation energy. The eigenvalue equations for the spin-orbitals are modified by the orbital occupation numbers. The Euler equation for the occupation numbers results in the Fermi-Dirac distribution, which is very efficient to update as soon as the orbital eigenvalue equations are solved. The method is demonstrated on the ground state of O.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_4: To advance the foundation of one-particle reduced density matrix functional theory (1RDMFT), we refine and relate some of its fundamental features and underlying concepts. We define by concise means the scope of a 1RDMFT, identify its possible natural variables, and explain how symmetries could be exploited. In particular, for systems with time-reversal symmetry, we explain why there exist six equivalent universal functionals, prove concise relations among them, and conclude that the important notion of v-representability is relative to the scope and choice of variable. All these fundamental concepts are then comprehensively discussed and illustrated for the Hubbard dimer and its generalization to arbitrary pair interactions W. For this, we derive by analytical means the pure and ensemble functionals with respect to both the real- and complex-valued Hilbert space. The comparison of various functionals allows us to solve the underlying v-representability problems analytically, and the dependence of its solution on the pair interaction is demonstrated. Intriguingly, the gradient of each universal functional is found to always diverge repulsively on the boundary of the domain. In that sense, this key finding emphasizes the universal character of the fermionic exchange force, recently discovered and proven in the context of translationally invariant one-band lattice models.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_5: For the ground-state properties of gas-phase nanomolecules with multi-reference character, thermally assisted occupation (TAO) density functional theory (DFT) has recently been found to outperform the widely used Kohn–Sham DFT when traditional exchange-correlation energy functionals are employed. Aiming to explore solvation effects on the ground-state properties of nanomolecules with multi-reference character at a minimal computational cost, we combined TAO-DFT with the PCM (polarizable continuum model). In order to show its usefulness, TAO-DFT-based PCM (TAO-PCM) was used to predict the electronic properties of linear acenes in three different solvents (toluene, chlorobenzene, and water). According to TAO-PCM, in the presence of these solvents, the smaller acenes should have nonradical character, and the larger ones should have increasing polyradical character, revealing striking similarities to the past findings in the gas phase.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_6: For an electronic system, given a mean field method and a distribution of orbital occupation numbers that are close to the natural occupations of the correlated system, we provide formal evidence and computational support to the hypothesis that the entropy (or more precisely −σS, where σ is a parameter and S is the entropy) of such a distribution is a good approximation to the correlation energy. Underpinning the formal evidence are mild assumptions: the correlation energy is strictly a functional of the occupation numbers, and the occupation numbers derive from an invertible distribution. Computational support centers around employing different mean field methods and occupation number distributions (Fermi–Dirac, Gaussian, and linear), for which our claims are verified for a series of pilot calculations involving bond breaking and chemical reactions. This work establishes a formal footing for those methods employing entropy as a measure of electronic correlation energy (e.g., i-DMFT [Wang and Baerends, Phys. Rev. Lett. 128, 013001 (2022)] and TAO-DFT [J.-D. Chai, J. Chem. Phys. 136, 154104 (2012)]) and sets the stage for the widespread use of entropy functionals for approximating the (static) electronic correlation.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_7: The self-consistent-field method proposed recently [Wang et al., Phys. Rev. Lett. 128, 013001 (2022)] is discussed in more detail. The method leads to self-consistent eigenvalue equations for the natural spin orbitals and Fermi-Dirac distribution for the orbital occupation numbers. The entropic functional contains two parameters (𝜅 and 𝑏), which can be fitted to various experimental or theoretical data such as the dissociation energy, entropy, and total energy at a given geometry, etc. Calculations are demonstrated on the square H4 and hydrogen chains of H50, which are representative cases of degenerate or nondynamically correlated systems.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_8: We explore the hypercomplex Kohn-Sham (HCKS) formalism and develop a third exact functional theory beyond the popular Kohn-Sham density functional theory (KS-DFT) and reduced density matrix functional theory (RDMFT), called 1-HCKS. The orbitals of hierarchical correlation introduced in 1-HCKS are interesting because their occupations are neither necessarily integers, as in KS-DFT, nor overly flexible, as in RDMFT; rather, they can capture strong correlation with fractional occupations, governed by stringent constraints. This feature allows 1-HCKS to combine the advantages of KS-DFT and RDMFT in dealing with dynamic correlation and strong correlation, while eliminating the issues of computational convergence and basis set dependence encountered in RDMFT. Thus, 1-HCKS is a promising alternative for implementing DFT beyond KS-DFT, especially for the treatment of strongly correlated systems.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_9: Reduced density matrix functional theory (RDMFT) calculations are usually implemented in a decoupled manner, where the orbital and occupation optimizations are repeated alternately. Typically, orbital updates are performed using the unitary optimization method, while occupations are optimized through the explicit-by-implicit (EBI) method. The EBI method addresses explicit constraints by incorporating implicit functions, effectively transforming constrained optimization scenarios into unconstrained minimizations. Although the unitary and EBI methods individually achieve robust performance in optimizing orbitals and occupations, respectively, the decoupled optimization methods often suffer from slow convergence and require dozens of alternations between the orbital and occupation optimizations. To address this issue, this work proposes a coupled optimization method that combines unitary and EBI optimizations to update orbitals and occupations simultaneously at each step. To achieve favorable convergence in coupled optimization using a simple first-order algorithm, an effective and efficient preconditioner and line search are further introduced. The superiority of the new method is demonstrated through numerous tests on different molecules, random initial guesses, different basis sets, and different functionals. It outperforms all decoupled optimization methods in terms of convergence speed, convergence results, and convergence stability. Even a large system like C60 can converge to 10–8 au in 154 iterations, which shows that the coupled optimization method can make RDMFT more practical and facilitate its wider application and further development.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_10: A critical challenge for density-functional theory (DFT) in practice is its limited ability to treat static electron correlation, leading to errors in its prediction of charges, multiradicals, and reaction barriers. Recently, we combined one- and two-electron reduced density-matrix theories with DFT to obtain a universal 𝑂⁡(𝑁3) generalization of DFT for static correlation. In this Letter, we enhance the theory's treatment of large molecules by renormalizing the trace of the two-electron identity matrix in the correction using Cauchy-Schwarz inequalities of the electron-electron repulsion matrix. We apply the resulting functional theory to linear hydrogen chains as well as the prediction of the singlet-triplet gap and equilibrium geometries of a series of acenes. This renormalization of the generalized DFT retains the 𝑂⁡(𝑁3) computational scaling of DFT while enabling the accurate treatment of static correlation for a broad range of molecules and materials.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_11: The recently proposed i-DMFT method [Wang and Baerends, Phys. Rev. Lett. 128, 013001 (2022)] has been proven to be ideally suited to recover strong static correlation in dissociating covalent bonds. Here, the application to van der Waals bonding is investigated using the prototype van der Waals systems triplet H2 and ground-state He2. It is demonstrated that the i-DMFT orbitals are in this case essentially different from the natural orbitals, and the i-DMFT occupations differ substantially from the NO occupations. This is shown to lead to rather deficient interaction potential curves, even if a reasonable well depth may be obtained by fitting of parameters. If the basis set is extended, however, it may no longer be possible to generate van der Waals bonding at all. The linear behavior of the two-electron cumulant energy Ecum as a function of the “entropy” S along a dissociation coordinate, which was the basis of i-DMFT, is distinctly poorer in the case of van der Waals bonding than for covalent bonding.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_12: The potential energy curves for dihalogens (F2, Cl2, and Br2) are calculated with the i-DMFT method proposed recently [Wang and Baerends, Phys. Rev. Lett. 128, 013001]. All electrons are correlated in a set of self-consistent-field eigenvalue equations, with the orbital occupation numbers obeying the Fermi–Dirac distribution. The only input is the dissociation energies of the molecules, which are usually available from an experimental database. The quality of the computed potential energy curve is examined by extracting spectroscopic parameters and rotation–vibration energy levels, which are compared with experiment data and other theoretical calculations.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_13: A hybrid Kohn–Sham Density Functional Theory (KS-DFT) and 1-electron Reduced Density Matrix Functional Theory (1-RDMFT) has recently been developed to describe strongly correlated systems at mean-field computational cost. This approach relies on combining a Reduced Density Matrix Functional to capture strong correlation effects with existing exchange correlation (XC) functionals to capture the remaining dynamical correlation effects. In this work, we systematically benchmark the performance of nearly 200 different XC functionals available within LibXC in this DFA 1-RDMFT framework, contrasting it with their performance in unrestricted KS-DFT. We identify optimal XC functionals for use within DFA 1-RDMFT and elucidate fundamental trends in the response of different XC functionals to strong correlation in both DFA 1-RDMFT and UKS-DFT.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_14: This work presents a detailed mathematical derivation of the hierarchically correlated orbital functional theory (HCOFT), a framework based on hypercomplex orbitals. Recent study [Phys. Rev. Lett. 133, 206402 (2024)] has demonstrated that hypercomplex orbitals in a determinant are equivalent to a set of real-valued orbitals that allow fractional occupations, making them desirable fundamental descriptors for many-electron systems. The algebraic properties of Clifford algebra are rigorously applied to derive key quantities within HCOFT, addressing the complexities introduced by the hypercomplex representation. It is shown that, despite this added complexity, the resulting density and kinetic energy remain physically meaningful and satisfy essential properties, including the Pauli exclusion principle. To establish the uniqueness of HCOFT, alternative definitions of hypercomplex orbitals within Clifford algebra are explored. These alternatives can lead to the loss of physical meaning in fundamental quantities for many-electron systems. Overall, this work demonstrates that HCOFT not only preserves the desired physical properties but also provides a single-determinant framework capable of describing multireference systems.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_15: The information entropy based on the occupation numbers has been found to play a central role in a description of electron correlation within the density-matrix functional theory [i-DMFT, see Phys. Rev. Lett. 2022, 128, 013001]. In this article, the i-DMFT method is applied to predict potential energy curves, equilibrium bond lengths, and harmonic vibrational frequencies for the hydrogen halides: HF, HCl, and HBr. The results are compared with other theoretical calculations and experimental spectroscopic data.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_16: Common one-electron reduced density matrix (1-RDM) functionals that depend on Coulomb and exchange-only integrals tend to underestimate dynamic correlation, preventing reduced density matrix functional theory (RDMFT) from achieving comparable accuracy to density functional theory in main-group thermochemistry and thermochemical kinetics. The recently developed ωP22 functional introduces a semi-local density functional to screen the erroneous short-range portion of 1-RDM functionals without double-counting correlation, potentially providing a better treatment of dynamic correlation around equilibrium geometries. Herein, we systematically evaluate the performance of this functional model, which consists of two parameters, on main-group thermochemistry, thermochemical kinetics, nonbonded interactions, and more. Tests on atomization energies, vibrational frequencies, and reaction barriers reveal that the ωP22 functional model can reliably predict properties at equilibrium and slightly away from equilibrium geometries. In particular, it outperforms commonly used density functionals in the prediction of reaction barriers, nonbonded interactions, and singlet diradicals, thus enhancing the predictive power of RDMFT for routine calculations of thermochemistry and thermochemical kinetics around equilibrium geometries. Further development is needed in the future to refine short- and long-range approximations in the functional model in order to achieve an excellent description of properties both near and far from equilibrium geometries.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_52_17: An ab initio approach formulated under an entropy-inspired repartitioning of the electronic Hamiltonian is presented. This ansatz produces orbital eigenvalues each shifted by entropic contributions expressed as subsets of scaled pair correlation energy terms present in second-order Moller-Plesset (MP) perturbation theory. Under the auspices of Collins' conjecture, which suggests that the electron correlation energy is approximately proportional to the Jaynes entropy of the one-electron density matrix, we introduce a parameter that controls the accuracy of the resultant one-electron density at the MP2 level. By tuning the density in a somewhat automated way, we achieve one-electron densities on par with those from full configuration interaction for single-bond dissociation. This parameter can then be used to add a Collins'-like static correlation correction to the energy functional, capturing both dynamical and nondynamical correlation effects in many-electron systems. The performance of the proposed method and its related variants approaches the accuracy of generalized valence bond theory for estimating single bond dissociation energies (BDEs) for set of small, closed-shell molecules composed of first and second row elements. Our results hold implications for reincorporating the missing (static) correlation energy in regularized perturbation theories that is typically discarded. Finally, we propose generic BDE parameters (accurate to within 7% on average) that could be used for strongly-correlated systems in general.","Main_52: Density functional theory (DFT), one of the most widely utilized methods
available to computational chemistry, fails to describe systems with statically
correlated electrons. To address this shortcoming, in previous work we
transformed DFT into a one-electron reduced density matrix theory (1-RDMFT) via
the inclusion of a quadratic one-electron reduced density matrix (1-RDM)
correction. Here, we combine our 1-RDMFT approach with different DFT
functionals as well as Hartree-Fock to elucidate the method's dependence on the
underlying functional selection. Furthermore, we generalize the information
density matrix functional theory (iDMFT), recently developed as a correction to
the Hartree-Fock method, by incorporating density functionals in place of the
Hartree-Fock functional. We relate iDMFT mathematically to our approach and
benchmark the two with a common set of functionals and systems.
",0.85
"Cite_53_1: We define the logarithmic tautological rings of the moduli spaces of Deligne–Mumford stable curves (together with a set of additive generators lifting the decorated strata classes of the standard tautological rings). While these algebras are infinite dimensional, a connection to polyhedral combinatorics via a new theory of homological piecewise polynomials allows an effective study. A complete calculation is given in genus 0 via the algebra of piecewise polynomials on the cone stack of the associated Artin fan (lifting Keel's presentation of the Chow ring of ). Counterexamples to the simplest generalizations in genus 1 are presented. We show, however, that the structure of the log tautological rings is determined by the complete knowledge of all relations in the standard tautological rings of the moduli spaces of curves. In particular, Pixton's conjecture concerning relations in the standard tautological rings lifts to a complete conjecture for relations in the log tautological rings of the moduli spaces of curves. Several open questions are discussed. We develop the entire theory of logarithmic tautological classes in the context of arbitrary smooth normal crossings pairs  with explicit formulas for intersection products. As a special case, we give an explicit set of additive generators of the full logarithmic Chow ring of in terms of Chow classes on the strata of X and piecewise polynomials on the cone stack.","Main_53: We prove that the pullbacks of the virtual fundamental classes of the
Brill-Noether loci under any Abel-Jacobi section lie in the tautological ring
of the moduli space of stable curves. This resolves a conjecture of Pagani,
Ricolfi and van Zelm, and is part of a broader program to understand the
logarithmic intersection theory of $\Mgn$.
",0.15
"Cite_53_2: We explain how logarithmic structures select principal components in an intersection of schemes. These manifest in Chow homology and can be understood using strict transforms under logarithmic blowups. Our motivation comes from Gromov–Witten theory. The toric contact cycles in the moduli space of curves parameterize curves that admit a map to a fixed toric variety with prescribed contact orders. We show that they are intersections of virtual strict transforms of double ramification cycles in blowups of the moduli space of curves. We supply a calculation scheme for the virtual strict transforms, and deduce that toric contact cycles lie in the tautological ring of the moduli space of curves. This is a higher-dimensional analogue of a result of Faber and Pandharipande. The operational Chow rings of Artin fans play a basic role, and are shown to be isomorphic to rings of piecewise polynomials on associated cone complexes. The ingredients in our analysis are Fulton’s blowup formula, Aluffi’s formulas for Segre classes of monomial schemes, piecewise polynomials, and degeneration methods. A model calculation in toric intersection theory is treated without logarithmic methods and may be read independently.","Main_53: We prove that the pullbacks of the virtual fundamental classes of the
Brill-Noether loci under any Abel-Jacobi section lie in the tautological ring
of the moduli space of stable curves. This resolves a conjecture of Pagani,
Ricolfi and van Zelm, and is part of a broader program to understand the
logarithmic intersection theory of $\Mgn$.
",0.15
"Cite_53_3: We construct a derived pushforward of the r-th root of the universal line bundle over the Picard stack of genus g prestable curves carrying a line bundle. We prove a number of basic properties, and give a formula in terms of standard tautological generators. After pullback, our formula recovers formulae of Mumford, of the first-named author, and of Pagani--Ricolfi--van Zelm. We apply these constructions to prove a conjecture expressing the coefficients of higher powers of r in the so-called `Chiodo classes' to the double ramification cycle, and to give a formula for the r-spin logarithmic double ramification cycle.","Main_53: We prove that the pullbacks of the virtual fundamental classes of the
Brill-Noether loci under any Abel-Jacobi section lie in the tautological ring
of the moduli space of stable curves. This resolves a conjecture of Pagani,
Ricolfi and van Zelm, and is part of a broader program to understand the
logarithmic intersection theory of $\Mgn$.
",0.15
"Cite_54_1: We propose a novel approach to classify sets of Global Navigation Satellite System (GNSS) permanent stations as benchmarks for hydrogeodesy. Benchmarks are trusted sets of GNSS stations whose displacements are classified as significantly and positively correlated with hydrospheric changes and identified in a three temporal-scales: short-term, seasonal and long-term. We use 63 vertical displacement time series processed at the Nevada Geodetic Laboratory for the period 1998–2021 from stations located within Amazon basin and show that estimates of trends and annual signals, including the annual phase maximum, are very coherent with water surface levels provided by altimetry missions. We compute vertical displacements from Gravity Recovery and Climate Experiment (GRACE) and GRACE Follow-On gravity missions and predict those also from Global Land Water Storage (GLWS) v2.0 data set which values are produced by assimilation of GRACE into WaterGAP Global Hydrological Model (WGHM). We divide vertical displacements from the three data sets into the pre-defined temporal-scales of short-term, seasonal and long-term, using non-parametric wavelet analysis. For each temporal-scale, correlation coefficients are computed between GNSS-measured and GRACE-derived/GLWS-predicted displacements. We present the benefits of applying high-resolution GRACE-assimilating hydrology model to benchmark GNSS stations, which are particularly evident when using spherical harmonic coefficients higher than 120. Their increase causes the number of stations included in the benchmarks to rise by up to 15% for short-term. Benchmarking allows hydrogeodesy to take advantage of a broader set of GNSS stations that were previously omitted, such as earthquake-affected sites and those where a possible poroelastic response is observed.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_2: The Earth's climate is changing rapidly and unexpectedly, causing more frequent, longer and more severe droughts, with lasting impacts on plants, ecosystems, communities and people. Consequently, this is leading to an increased importance of monitoring the climate and water storage trends in different regions. This information on a global scale is already commonly derived using satellite-based geodetic techniques such as the Global Positioning System (GPS) and the Gravity Recovery and Climate Experiment (GRACE). The use of both techniques has significant advantages, especially in regions where changes in the hydrosphere are notable, such as the Amazon basin, where 25 GPS stations were lately classified as benchmarks for hydrogeodesy. We show that the vertical displacements obtained from GPS and GRACE have good spatio-temporal agreement with the Standardized Precipitation and Standardized Precipitation Evapotranspiration indices, abbreviated respectively as SPI and SPEI, for all these stations. Drought severity index (DSI) estimated separately from GPS-observed and GRACE-derived vertical displacements on a station-by-station basis is capable to identify dry and wet events previously reported for the Amazon basin. However, due to the weaknesses of both techniques, such as technique-related systematic errors or coarse spatial resolution, a few extreme hydrological events may not be properly captured by GPS-DSI and/or GRACE-DSI. To take full advantage of both techniques and overcome their weaknesses, we introduce a completely new methodology to combine individual GPS-DSI and GRACE-DSI indices. As a novelty, both indices are estimated using short-term changes (<9 months) of monthly vertical displacements observed by GPS permanent stations and those derived by GRACE for GPS locations. Then, to capture and detect drought events that either both geodetic techniques metrics missed or incorrectly depicted, the Multivariate Drought Severity Index (MDSI) is estimated through the concept of Frank copulas. We demonstrate that the MDSI captures more hydroclimatic events reported in previous studies, which are not identified by individual series of GPS-DSI or GRACE-DSI indices, and is temporally consistent with Standardized Streamflow Index (SSI) based on the in-situ river discharge changes.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_3: Slow slip events (SSEs) are geophysical phenomena primarily occurring in subduction zones. These events are often associated with seismic activity and can be detected by Global Positioning System (GPS). However, the relationship between SSEs and seismic activity remains unclear. To further investigate SSEs associated with seismic activity, we conducted SSE detection and inversion for the period from 2019 to 2022 on New Zealand’s North Island, where both SSEs and seismic activity frequently occur. By modeling daily GPS coordinate time series from 40 GPS stations and applying the Network Inversion Filter (NIF) method, we obtain surface displacements, cumulative slips, and slip rates for eight shallow SSEs. Subsequently, we conduct a statistical analysis of seismic activity concerning its spatial distribution and frequency before, during, and after SSE occurrences. The results indicate that SSE1 and SSE7 exhibited larger cumulative slips, at 14.35 and 7.20 cm, and surface displacements, at 4.97 and 2.53 cm, respectively. During their occurrences, the seismic frequency noticeably increased to 6.5 and 5.6 events per day in the Eastern Coastal Region (ECR) of New Zealand’s North Island. However, the other six SSEs, characterized by cumulative slips of less than 6 cm and maximum surface displacements of less than 2 cm, did not lead to a noticeable increase in seismic frequency during their occurrences in the ECR. In the Main Slip Regions (MSR) of these eight SSEs, a significant upward trend in seismic frequency was observed during their occurrences. Therefore, it can be inferred that in the ECR of New Zealand’s North Island, all SSEs result in an increased seismic frequency within their respective MSRs, but only significant SSEs impact the seismic frequency of the ECR. Monitoring shallow SSEs may contribute to the identification and recording of seismic activity.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_4: Global Positioning System (GPS) daily position time-series have a standard precision of a few millimetres. However, GPS position series contain large temporal correlations that impede the observation of subtle interannual Earth deformation. We show that the specific configuration of the GPS constellation, compared to other Global Navigation Satellite Systems (GNSS), contributes to the temporal correlation. Based on the analysis of observed and simulated GPS, Galileo, GLONASS and BeiDou orbits, we determine that the GPS orbital dynamics are more prone to interannual drifts caused by their higher sensitivity to the lunisolar gravitational resonance. This leads to substantial changes in the observation geometry over time, which, combined with mismodelled station-dependent systematic errors, results in a larger temporal correlation for GPS position time-series. Improving the weighting of the GPS observations may mitigate the effect of geometry, which is absent in other GNSS constellations.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_5: We investigate the occurrence patterns of SSEs along the shallow ( < 15 km) portion of the Hikurangi subduction zone. First, we build a manual catalog constraining timing and length of 92 SSEs between 2006 and 2024. Then, we investigate SSE occurrence patterns by fitting a renewal process, using Bayesian inference to obtain the posterior distribution of model parameters. Our results show that SSE recurrence intervals vary along the Hikurangi margin; less frequent SSEs occur in the southern part of the margin. The periodicity of SSEs also changes along strike. SSEs in the northern part of the margin occur more regularly than those at the central part. Finally, we do not find conclusive evidence that 2016 M w 7.8 Kaikōura earthquake had a lasting effect on SSE occurrence patterns.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_6: Detections of slow slip events (SSEs) are now common along most plate boundary fault systems at the global scale. However, no such event has been described in the south Peru - north Chile subduction zone so far, except for the early preparatory phase of the 2014 Iquique earthquake. We use geodetic template matching on GNSS-derived time series of surface motion in Northern Chile to extract SSEs hidden within the geodetic noise. We detect 33 events with durations ranging from 9 to 40 days and magnitudes from Mw 5.6 to 6.2. The moment released by these aseismic events seems to scale with the cube of their duration, suggesting a dynamic comparable to that of earthquakes. We compare the distribution of SSEs with the distribution of coupling along the megathrust derived using Bayesian inference on GNSS- and InSAR-derived interseismic velocities. From this comparison, we obtain that most SSEs occur in regions of intermediate coupling where the megathrust transitions from locked to creeping or where geometrical complexities of the interplate region have been proposed. We finally discuss the potential role of fluids as a triggering mechanism for SSEs in the area.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_7: Various slow slip events (SSEs) with distinct characteristics have been detected globally, particularly in regions with dense Global Navigation Satellite Systems (GNSS) networks. In the Hikurangi subduction zone of New Zealand, SSEs frequently occur alongside seismic activity, especially in the Manawatu and Kapiti regions. This study analyzes the 2021–2023 Kapiti-Manawatu long-term SSE using daily displacement data (2019–2023) from 53 GPS stations. The network inversion filter (NIF) method is applied to extract slow slip signals, revealing spatial migration with alternating slip between Kapiti and Manawatu, characterized by distinct phases of acceleration and deceleration. Manawatu exhibits higher slip rates, exceeding 4 cm/month, with greater cumulative slip and surface displacement than Kapiti. A moderate temporal correlation (coefficient 0.59) between seismic activity in the region and slip acceleration in Manawatu suggests that seismic events may contribute to the slip, while no significant correlation is observed in Kapiti.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_54_8: Detections of slow slip events (SSEs) are now common along most plate boundary fault systems globaly. However, no such event has been described in the south Peru - north Chile subduction zone so far, except for the early preparatory phase of the 2014 Iquique earthquake.We use geodetic template matching on GNSS-derived time series of surface motion in Southern Peru - Northern Chile to extract SSEs hidden within geodetic noise. We detect 24 events with dura18 tions ranging from 17 to 36 days and magnitudes from Mw 5.4 to 6.2. Our events, analyzed from a moment-duration scaling perspective, reveal values consistent with observations reported in other subduction zones. We compare the distribution of SSEs with the distribution of coupling along the megathrust derived using Bayesian inference on GNSS- and InSAR-derived interseismic velocities. From this comparison, we obtain that most SSEs occur in regions of intermediate coupling where the megathrust transitions from locked to creeping or where geometrical complexities of the in terplate region have been proposed. We finally discuss the potential role of fluids as a triggering mechanism for SSEs in the area.","Main_54:   In many places, tectonic tremor is observed in relation to slow slip and can
be used as a proxy to study slow slip events of moderate magnitude where
surface deformation is hidden in Global Navigation Satellite System (GNSS)
noise. However, when no clear relationship between tremor and slow slip
occurrence is observed, these methods cannot be applied, and we need other
methods to be able to better detect and quantify slow slip. Wavelets methods
such as the Discrete Wavelet Transform (DWT) and the Maximal Overlap Discrete
Wavelet Transform (MODWT) are mathematical tools for analyzing time series
simultaneously in the time and the frequency domain by observing how weighted
differences of a time series vary from one period to the next. We use wavelet
methods to analyze GNSS time series of slow slip events in Cascadia. We use
detrended GNSS data, apply the MODWT transform and stack the wavelet details
over several nearby GNSS stations. As an independent check on the timing of
slow slip events, we also compute the cumulative number of tremor in the
vicinity of the GNSS stations, detrend this signal, and apply the MODWT
transform. In both time series, we can then see simultaneous waveforms whose
timing corresponds to the timing of slow slip events. We assume that there is a
slow slip event whenever there is a positive peak followed by a negative peak
in the wavelet signal. We verify that there is a good agreement between slow
slip events detected with only GNSS data, and slow slip events detected with
only tremor data. The wavelet-based detection method effectively detects events
of magnitude higher than 6 as determined by independent event catalogs. As a
demonstration of using the wavelet analysis in a region without significant
tremor, we also analyze GNSS data from New Zealand and detect slow slip events
that are spatially and temporally close to those detected previously by other
studies.
",0.4
"Cite_55_1: Implicit neural representations (INRs) are the subject of extensive research, particularly in their application to modeling complex signals by mapping spatial and temporal coordinates to corresponding values. When handling videos, mapping compact inputs to entire frames or spatially partitioned patch images is an effective approach. This strategy better preserves spatial relationships, reduces computational overhead, and improves reconstruction quality compared to coordinate-based mapping. However, predicting entire frames often limits the reconstruction of high-frequency visual details. Additionally, conventional patch-based approaches based on uniform spatial partitioning tend to introduce boundary discontinuities that degrade spatial coherence. We propose a neural video representation method based on Structure-Preserving Patches (SPPs) to address such limitations. Our method separates each video frame into patch images of spatially aligned frames through a deterministic pixel-based splitting similar to PixelUnshuffle. This operation preserves the global spatial structure while allowing patch-level decoding. We train the decoder to reconstruct these structured patches, enabling a global-to-local decoding strategy that captures the global layout first and refines local details. This effectively reduces boundary artifacts and mitigates distortions from naive upsampling. Experiments on standard video datasets demonstrate that our method achieves higher reconstruction quality and better compression performance than existing INR-based baselines.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_2: Neural Video Representations (NVRs) have recently been proposed as a novel approach to the video compression problem. NVRs consist of one or multiple small neural networks that are overfitted on one specific video sequence, thereby encoding the video within the weights and biases of the network(s). In contrast to other learned video coding approaches, NVR-based codecs do not rely on large datasets and have a relatively low decoding complexity. Many works have focused on improving the compression performance of NVR-based codecs by enhancing the overall codec design, devising more performant and parameter-efficient model architectures, and incorporating more advanced model compression schemes such as weight pruning, quantization, and entropy minimization. We provide a systematic overview of representative work in the field and discuss common weaknesses and opportunities for future work, with a focus on practical deployment for video streaming. This paper serves as both an introduction for newcomers and a reference for existing researchers, highlighting the potential of neural video representations as an alternative to traditional codecs in video compression.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_3: This paper presents a novel scheme to efficiently compress Light Detection and Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives, and such archives pave the way for a detailed understanding of the corresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight format for representing 3D LiDAR observations. Although conventional image compression techniques can be adapted to improve compression efficiency for RIs, their practical performance is expected to be limited due to differences in bit precision and the distinct pixel value distribution characteristics between natural images and RIs. We propose a novel implicit neural representation~(INR)--based RI compression method that effectively handles floating-point valued pixels. The proposed method divides RIs into depth and mask images and compresses them using patch-wise and pixel-wise INR architectures with model pruning and quantization, respectively. Experiments on the KITTI dataset show that the proposed method outperforms existing image, point cloud, RI, and INR-based compression methods in terms of 3D reconstruction and detection quality at low bitrates and decoding latency.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_4: Existing Implicit Neural Representation (INR) video compression techniques have opened up new avenues in the field of video compression. NeRV maps the temporal coordinates to high-resolution images using neural networks, providing a more flexible and efficient encoding method for video data. However, NeRV implicitly stores all video information in the network, requiring post network compression techniques such as pruning. To integrate explicit compression and implicit representation into an end-to-end framework, this study proposes a novel neural representation-based video compression paradigm called Latent code based Neural representation video compression (LNeRV). Specifically, LNeRV consists of hierarchy feature grids, synthesis network and entropy coding network. With a single-stage training process, LNeRV achieves video compression and dynamically allocates bits according to video complexity, better fitting dynamic videos. We provide a comprehensive compression-to-decompression workflow for our approach. Extensive experimental results verify the effectiveness of our LNeRV.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_5: Implicit Neural Representations (INRs) have demonstrated significant potential in video compression by representing videos as neural networks. However, as the number of frames increases, the memory consumption for training and inference increases substantially, posing challenges in resource-constrained scenarios. Inspired by the success of traditional video compression frameworks, which process video frame by frame and can efficiently compress long videos, we adopt this modeling strategy for INRs to decrease memory consumption, while aiming to unify the frameworks from the perspective of timeline-based autoregressive modeling. In this work, we present a novel understanding of INR models from an autoregressive (AR) perspective and introduce a Unified AutoRegressive Framework for memory-efficient Neural Video Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural video compression under a unified autoregressive paradigm. It partitions videos into several clips and processes each clip using a different INR model instance, leveraging the advantages of both compression frameworks while allowing seamless adaptation to either in form. To further reduce temporal redundancy between clips, we design two modules to optimize the initialization, training, and compression of these model parameters. UAR-NVC supports adjustable latencies by varying the clip length. Extensive experimental results demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to resource-constrained environments and significantly improve performance compared to different baseline models.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_6: Recent advances in video compression introduce implicit neural representation (INR) based methods, which effectively capture global dependencies and characteristics of entire video sequences. Unlike traditional and deep learning based approaches, INR-based methods optimize network parameters from a global perspective, resulting in superior compression potential. However, most current INR methods utilize a fixed and uniform network architecture across all frames, limiting their adaptability to dynamic variations within and between video sequences. This often leads to suboptimal compression outcomes as these methods struggle to capture the distinct nuances and transitions in video content. To overcome these challenges, we propose Content Adaptive Neural Representation for Video Compression (CANeRV), an innovative INR-based video compression network that adaptively conducts structure optimisation based on the specific content of each video sequence. To better capture dynamic information across video sequences, we propose a dynamic sequence-level adjustment (DSA). Furthermore, to enhance the capture of dynamics between frames within a sequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to effectively capture spatial structural information within video frames, thereby enhancing the detail restoration capabilities of CANeRV, we devise a structure level hierarchical structural adaptation (HSA).} Experimental results demonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art INR-based video compression techniques across diverse video datasets.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_7: Neural Video Representations (NVR) have emerged as a novel method for video compression, which encode video content into network parameters, transforming video compression into a model compression issue. In this paper, we attempt to introduce Lottery Ticket Hypothesis (LTH) into NVR, aiming to identify optimal sub-networks (, winning tickets) capable of effective representing corresponding videos. Firstly, we validate the existence of winning tickets within NVR and reveal that these winning tickets exhibit non-transferability across different videos. To balance heavy searching cost with training efficiency, we devise an Identifying Winning Tickets (IWT) method tailored for NVR. Additionally, leveraging the non-transferability of winning tickets and video frames redundancy, we design a Progressive Cyclic Learning (PCL) strategy to accelerate the searching winning tickets phase. Finally, comprehensive experiments are conducted to evaluate the performance and general properties of winning tickets across various NVR architectures and videos. The results demonstrate that our proposed method significantly outperforms the original pruning approach, achieving performance gains of 1.81 dB, 4.19 dB, and 3.5 dB at 90 % sparsity in NeRV, E-NeRV, and HNeRV, respectively.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_8: In the field of video compression, the pursuit for better quality at lower bit rates remains a long-lasting goal. Recent developments have demonstrated the potential of Implicit Neural Representation (INR) as a promising alternative to traditional transform-based methodologies. Video INRs can be roughly divided into frame-wise and pixel-wise methods according to the structure the network outputs. While the pixel-based methods are better for upsampling and parallelization, frame-wise methods demonstrated better performance. We introduce CoordFlow, a novel pixel-wise INR for video compression. It yields state-of-the-art results compared to other pixel-wise INRs and on-par performance compared to leading frame-wise techniques. The method is based on the separation of the visual information into visually consistent layers, each represented by a dedicated network that compensates for the layer's motion. When integrated, a byproduct is an unsupervised segmentation of video sequence. Objects motion trajectories are implicitly utilized to compensate for visual-temporal redundancies. Additionally, the proposed method provides inherent video upsampling, stabilization, inpainting, and denoising capabilities.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_9: Point clouds have gained prominence in numerous applications due to their ability to accurately depict 3D objects and scenes. However, compressing unstructured, high-precision point cloud data effectively remains a significant challenge. In this paper, we propose NeRC^{	extbf{3}}, a novel point cloud compression framework leveraging implicit neural representations to handle both geometry and attributes. Our approach employs two coordinate-based neural networks to implicitly represent a voxelized point cloud: the first determines the occupancy status of a voxel, while the second predicts the attributes of occupied voxels. By feeding voxel coordinates into these networks, the receiver can efficiently reconstructs the original point cloud's geometry and attributes. The neural network parameters are quantized and compressed alongside auxiliary information required for reconstruction. Additionally, we extend our method to dynamic point cloud compression with techniques to reduce temporal redundancy, including a 4D spatial-temporal representation termed 4D-NeRC^{	extbf{3}}. Experimental results validate the effectiveness of our approach: for static point clouds, NeRC^{	extbf{3}} outperforms octree-based methods in the latest G-PCC standard. For dynamic point clouds, 4D-NeRC^{	extbf{3}} demonstrates superior geometry compression compared to state-of-the-art G-PCC and V-PCC standards and achieves competitive results for joint geometry and attribute compression.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_10: Generalized models dominate neural video compression methods to compress arbitrary video with the same model. However, obtaining a universal neural codec with high compression efficiency on all videos is challenging. Existing work attempts to adapt the decoder-side model per video content through full parameter tuning, but this requires a large Group of Pictures (GOP) to compensate for the cost of transmitting updated parameters. To tackle this challenge, we propose to tune generalized video codecs per video content in a parameter-efficient manner. The per-content tuned parameters are further compressed with entropy coding using adaptive distribution estimations. This allows for enhancing the compression efficiency while maintaining a normal GOP size for random access capabilities. To validate the generality and validity of our approach, we apply it to two representative methods: CNN-based, DCVC-HEM, and Transformer-based, VCT. Our results demonstrate that introducing content-specific representation leads to a notable improvement in compression efficiency compared to the original methods.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_11: Inspired by the success of generative image models, recent work on learned image compression increasingly focuses on better probabilistic models of the natural image distribution, leading to excellent image quality. This, however, comes at the expense of a computational complexity that is several orders of magnitude higher than today's commercial codecs, and thus prohibitive for most practical applications. With this paper, we demonstrate that by focusing on modeling visual perception rather than the data distribution, we can achieve a very good trade-off between visual quality and bit rate similar to generative compression models such as HiFiC, while requiring less than 1% of the multiply-accumulate operations (MACs) for decompression. We do this by optimizing C3, an overfitted image codec, for Wasserstein Distortion (WD), and evaluating the image reconstructions with a human rater study, showing that WD clearly outperforms LPIPS as an optimization objective. The study also reveals that WD outperforms other perceptual metrics such as LPIPS, DISTS, and MS-SSIM as a predictor of human ratings, remarkably achieving over 94% Pearson correlation with Elo scores.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_12: The delivery of free viewpoint videos (FVVs) is gaining popularity because of their ability to provide freely switchable perspectives to remote users as immersive experiences. While smooth view switching is crucial for enhancing user's experiences, FVV delivery faces a significant challenge in balancing traffic and decoding latency. The typical approach sends limited viewpoints and synthesizes the remainings on the user, reducing traffic, but increasing decoding delays. Alternatively, sending more viewpoints reduces the delay, but requires more bandwidth for transmission. In this paper, we propose a novel FVV representation format, FV-NeRV~(Free Viewpoint-Neural Representation for Videos), to address this dilemma in FVV delivery. FV-NeRV reduces both traffic and decoding delay even for content with a large number of virtual viewpoints by overfitting compact neural networks to all viewpoints and pruning and quantizing the trained model. Experiments using FVVs show that FV-NeRV achieves a comparable or even superior traffic reduction with faster decoding speed compared to existing FVV codecs and NeRV formats.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_13: 3D Gaussian splatting has surpassed neural radiance field methods in novel view synthesis by achieving lower computational costs and real-time high-quality rendering. Although it produces a high-quality rendering with a lot of input views, its performance drops significantly when only a few views are available. In this work, we address this by proposing a depth-aware Gaussian splatting method for few-shot novel view synthesis. We use monocular depth prediction as a prior, along with a scale-invariant depth loss, to constrain the 3D shape under just a few input views. We also model color using lower-order spherical harmonics to avoid overfitting. Further, we observe that removing splats with lower opacity periodically, as performed in the original work, leads to a very sparse point cloud and, hence, a lower-quality rendering. To mitigate this, we retain all the splats, leading to a better reconstruction in a few view settings. Experimental results show that our method outperforms the traditional 3D Gaussian splatting methods by achieving improvements of 10.5% in peak signal-to-noise ratio, 6% in structural similarity index, and 14.1% in perceptual similarity, thereby validating the effectiveness of our approach.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_14: Neural representation for video (NeRV), which employs a neural network to parameterize video signals, introduces a novel methodology in video representations. However, existing NeRV-based methods have difficulty in capturing fine spatial details and motion patterns due to spectral bias, in which a neural network learns high-frequency (HF) components at a slower rate than low-frequency (LF) components. In this paper, we propose spectra-preserving NeRV (SNeRV) as a novel approach to enhance implicit video representations by efficiently handling various frequency components. SNeRV uses 2D discrete wavelet transform (DWT) to decompose video into LF and HF features, preserving spatial structures and directly addressing the spectral bias issue. To balance the compactness, we encode only the LF components, while HF components that include fine textures are generated by a decoder. Specialized modules, including a multi-resolution fusion unit (MFU) and a high-frequency restorer (HFR), are integrated into a backbone to facilitate the representation. Furthermore, we extend SNeRV to effectively capture temporal correlations between adjacent video frames, by casting the extension as additional frequency decomposition to a temporal domain. This approach allows us to embed spatio-temporal LF features into the network, using temporally extended up-sampling blocks (TUBs). Experimental results demonstrate that SNeRV outperforms existing NeRV models in capturing fine details and achieves enhanced reconstruction, making it a promising approach in the field of implicit video representations. The codes are available at https://github.com/qwertja/SNeRV.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_15: Implicit neural representations (INR) has found successful applications across diverse domains. To employ INR in real-life, it is important to speed up training. In the field of INR for video applications, the state-of-the-art approach [25] employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors [6]. However, the grid usage, which does not consider the video’s dynamic nature, leads to redundant use of trainable parameters. As a result, it has significantly lower parameter efficiency and higher bitrate compared to NeRV-style methods [5, 6, 27] that do not use a parametric encoding. To address the problem, we propose Neural Video representation with Temporally coherent Modulation (NVTM), a novel framework that can capture dynamic characteristics of video. By decomposing the spatio-temporal 3D video data into a set of 2D grids with flow information, NVTM enables learning video representation rapidly and uses parameter efficiently. Our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the NeRV-style method, with a speed increase of over 3 times. Also, it remarks an average of 1.54dB/0.019 improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters) and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic), compared to previous grid-type works. By expanding this to compression tasks, we demonstrate comparable performance to video compression standards (H.264, HEVC) and recent INR approaches for video compression. Additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. S. Shin and S. Kim—Equally contributed.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_16: We present FCNR, a fast compressive neural representation for tens of thousands of visualization images under varying viewpoints and timesteps. The existing NeRVI solution, albeit enjoying a high compression ratio, incurs slow speeds in encoding and decoding. Built on the recent advances in stereo image compression, FCNR assimilates stereo context modules and joint context transfer modules to compress image pairs. Our solution significantly improves encoding and decoding speed while maintaining high reconstruction quality and satisfying compression ratio. To demonstrate its effectiveness, we compare FCNR with state-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI, and ECSIC. The source code can be found at https://github.com/YunfeiLu0112/FCNR.","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_55_17: Neural fields also known as implicit neural representations (INR) have recently been shown to be quite effective at representing video content. However, video content typically contains audio and existing works on INR do not represent both video and audio. Since INR for audio is not well-explored, we propose a novel neural representation for representing audio (NeRA) that can represent audio using a neural network. We also propose a novel neural representation for jointly representing both videos and audios (NeRVA) using a neural network. We represent multimedia as a neural network that takes timestamp as input and outputs the corresponding RGB image frame and the audio samples. The proposed neural network architecture uses a combination of Multi-Layer Perceptron (MLP) and convolutional blocks. We also demonstrate that our joint representation of multimedia content has better performance than individually representing the components of multimedia (An improvement of +2dB PSNR for video and 10× FAD for audio).","Main_55:   Implicit Neural Representations (INR) have recently shown to be powerful tool
for high-quality video compression. However, existing works are limiting as
they do not explicitly exploit the temporal redundancy in videos, leading to a
long encoding time. Additionally, these methods have fixed architectures which
do not scale to longer videos or higher resolutions. To address these issues,
we propose NIRVANA, which treats videos as groups of frames and fits separate
networks to each group performing patch-wise prediction. This design shares
computation within each group, in the spatial and temporal dimensions,
resulting in reduced encoding time of the video. The video representation is
modeled autoregressively, with networks fit on a current group initialized
using weights from the previous group's model. To further enhance efficiency,
we perform quantization of the network parameters during training, requiring no
post-hoc pruning or quantization. When compared with previous works on the
benchmark UVG dataset, NIRVANA improves encoding quality from 37.36 to 37.70
(in terms of PSNR) and the encoding speed by 12X, while maintaining the same
compression rate. In contrast to prior video INR works which struggle with
larger resolution and longer videos, we show that our algorithm is highly
flexible and scales naturally due to its patch-wise and autoregressive designs.
Moreover, our method achieves variable bitrate compression by adapting to
videos with varying inter-frame motion. NIRVANA achieves 6X decoding speed and
scales well with more GPUs, making it practical for various deployment
scenarios.
",0.85
"Cite_56_1: Diffusion Models (DMs) have recently emerged as a highly effective category of deep generative models, achieving exceptional results in various domains, including image synthesis, video generation, and molecule design. This survey provides a comprehensive analysis of the expanding body of research on this topic. The primary objective of this study is to investigate the architecture and requirements of generative artificial intelligence systems. Initially, an analysis of the prerequisites and frontier ideas for the implementation of generative AI systems is performed. To clarify the operational mechanisms of the methodology, the design choices of DMs are thoroughly examined, covering aspects such as refinement, parallel generation, editing, in-painting, and cross-domain generation. This study extensively reviews fundamental DMs and their diverse applications in fields such as computer vision (CV), natural language processing (NLP), image synthesis, and interdisciplinary applications (scene generation, 3D vision, video modeling, medical image diagnosis, time-series analysis, audio generation, 3D molecule generation etc.) in other scientific domains. A comparative study for all the works that use generative AI methods for various downstream tasks in each domain is performed. A comprehensive study on datasets is also carried out. Finally, it discusses the limitations of current methods, as well as the need for additional techniques and future directions in order to make meaningful progress in this area.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_2: Medical imaging tasks require an understanding of subtle and localized visual features due to the inherently detailed and area-specific nature of pathological patterns, which are crucial for clinical diagnosis. Although recent advances in medical vision-language pre-training (VLP) enable models to learn clinically relevant visual features by leveraging both medical images and their associated radiology reports, current medical VLP methods primarily focus on aligning images with entire reports. This focus hinders the learning of dense (pixel-level) visual features and is suboptimal for dense prediction tasks (e.g., medical image segmentation).To address this challenge, we propose a novel medical VLP framework, named Global to Dense level representation learning (G2D), which aims to learn global and dense visual features simultaneously using only image-text pairs without extra annotations. In particular, G2D designs a Pseudo Segmentation (PS) task, which enables the model to learn dense visual features during VLP. Notably, generating PS masks can be performed on the fly during VLP, which does not incur extra trainable parameters. With this simple yet effective idea, G2D achieves superior performance across 5 medical imaging tasks and 25 diseases. Particularly, in the segmentation task which requires dense visual features, G2D surpasses existing models even with just 1% of the training data for finetuning, compared to 100% used by other models. ","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
Cite_56_3: We present an approach for analyzing grouping information contained within a neural network's activations permitting extraction of spatial layout and semantic segmentation from the behavior of large pre-trained vision models. Unlike prior work our method conducts a wholistic analysis of a network's activation state leveraging features from all layers and obviating the need to guess which part of the model contains relevant information. Motivated by classic spectral clustering we formulate this analysis in terms of an optimization objective involving a set of affinity matrices each formed by comparing features within a different layer. Solving this optimization problem using gradient descent allows our technique to scale from single images to dataset-level analysis including in the latter both intra- and inter-image relationships. Analyzing a pre-trained generative transformer provides insight into the computational strategy learned by such models. Equating affinity with key-query similarity across attention layers yields eigenvectors encoding scene spatial layout whereas defining affinity by value vector similarity yields eigenvectors encoding object identity. This result suggests that key and query vectors coordinate attentional information flow according to spatial proximity (a `where' pathway) while value vectors refine a semantic category representation (a `what' pathway).,"Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_4: Recent advancements in diffusion models have significantly impacted the trajectory of generative machine learning re-search, with many adopting the strategy of fine-tuning pre-trained models using domain-specific text-to-image datasets. Notably, this method has been readily employed for medical applications, such as X-ray image synthesis, leveraging the plethora of associated radiology reports. Yet, a prevailing concern is the lack of assurance on whether these models genuinely comprehend their generated content. With the evolution of text conditional image generation, these models have grown potent enough to facilitate object localization scrutiny. Our research underscores this advancement in the critical realm of medical imaging, emphasizing the crucial role of interpretability. We further unravel a consequential trade-off between image fidelity – as gauged by conventional metrics – and model interpretability in generative diffusion models. Specifically, the adoption of learnable text encoders when fine-tuning results in diminished interpretability. Our in-depth exploration uncovers the underlying factors responsible for this divergence. Consequently, we present a set of design principles for the development of truly interpretable generative models.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_5: With the increasing demand for pork, improving pig health and welfare management productivity has become a priority. However, it is impractical for humans to manually monitor all pigsties in commercial-scale pig farms, highlighting the need for automated health monitoring systems. In such systems, object detection is essential. However, challenges such as insufficient training data, low computational performance, and generalization issues in diverse environments make achieving high accuracy in unseen environments difficult. Conventional RGB-based object detection models face performance limitations due to brightness similarity between objects and backgrounds, new facility installations, and varying lighting conditions. To address these challenges, this study proposes a DOG (Depth-Oriented Gray) image generation method using various foundation models (SAM, LaMa, Depth Anything). Without additional sensors or retraining, the proposed method utilizes depth information from the testing environment to distinguish between foreground and background, generating depth background images and establishing an approach to define the Region of Interest (RoI) and Region of Uninterest (RoU). By converting RGB input images into the HSV color space and combining HSV-Value, inverted HSV-Saturation, and the generated depth background images, DOG images are created to enhance foreground object features while effectively suppressing background information. Experimental results using low-cost CPU and GPU systems demonstrated that DOG images improved detection accuracy (AP50) by up to 6.4% compared to conventional gray images. Moreover, DOG image generation achieved real-time processing speeds, taking 3.6 ms on a CPU, approximately 53.8 times faster than the GPU-based depth image generation time of Depth Anything, which requires 193.7 ms.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_6: Phrase grounding, i.e., mapping natural language phrases to specific image regions, holds significant potential for disease localization in medical imaging through clinical reports. While current state-of-the-art methods rely on discriminative, self-supervised contrastive models, we demonstrate that generative text-to-image diffusion models, leveraging cross-attention maps, can achieve superior zero-shot phrase grounding performance. Contrary to prior assumptions, we show that fine-tuning diffusion models with a frozen, domain-specific language model, such as CXR-BERT, substantially outperforms domain-agnostic counterparts. This setup achieves remarkable improvements, with mIoU scores doubling those of current discriminative methods. These findings highlight the underexplored potential of generative models for phrase grounding tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM), a novel post-processing technique that aligns text and image biases to identify regions of high certainty. BBM refines cross-attention maps, achieving even greater localization accuracy. Our results establish generative approaches as a more effective paradigm for phrase grounding in the medical imaging domain, paving the way for more robust and interpretable applications in clinical practice. ","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_7: This paper proposes a foreground-background separation (FBS) method with a novel foreground model based on convolutional sparse representation (CSR). In order to analyze the dynamic and static components of videos acquired under undesirable conditions, such as hardware, environmental, and power limitations, it is essential to establish an FBS method that can handle videos with low frame rates and various types of noise. Existing FBS methods have two limitations that prevent us from accurately separating foreground and background components from such degraded videos. First, they only capture either data-specific or general features of the components. Second, they do not include explicit models for various types of noise to remove them in the FBS process. To this end, we propose a robust FBS method with a CSR-based foreground model. This model can adaptively capture specific spatial structures scattered in imaging data. Then, we formulate FBS as a constrained multiconvex optimization problem that incorporates CSR, functions that capture general features, and explicit noise characterization functions for multiple types of noise. Thanks to these functions, our method captures both data-specific and general features to accurately separate the components from various types of noise even under low frame rates. To obtain a solution of the optimization problem, we develop an algorithm that alternately solves its two convex subproblems by newly established algorithms. Experiments demonstrate the superiority of our method over existing methods using two types of degraded videos: infrared and microscope videos.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_8: Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. ","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_9: The text-to-image diffusion models have been applied to image segmentation, demonstrating the potential of diffusion models in segmentation. However, texts often struggle to accurately describe objects, particularly when it comes to fine-grained details. Sketches, on the other hand, can address the issue to some extent. We observed that the intermediate features of the diffusion model guided by sketch contain more effective semantic information for segmentation compared to those guided by text. Therefore, we propose Sketch2Seg, a sketch-based image segmentation method with a diffusion model. By extracting intermediate features of a sketch-to-image diffusion model, only a simple pixel classifier needs to be trained. Quantitatively, our method reaches 78.47% and 81.07% mIOU on the PASCAL VOC and SketchySeg datasets on zero-shot setups, respectively. To investigate fine-grained sketch segmentation and detection, we contribute the SketchyCOCOSeg dataset which contains segmentation annotations for images corresponding to the SketchyCOCO dataset.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_10: Diffusion models have made great success in image generation and own much potential in other vision tasks like image segmentation. With respect to medical image segmentation (MIS), model interpretability is indeed as important as model accuracy. This paper introduces the fundamental situation of MIS and typical diffusion model-related frameworks for MIS. Interpretability issues are also discussed in earnest.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_56_11: Image generation through disentangling object representations is a critical area of research with significant potential. Disentanglement involves separating the representation of objects and their attributes, enabling greater control over the generated output. However, existing approaches are limited to disentangling only the objects’ attributes and generating images with selected combinations of attributes. This study explores learning object-level disentanglement of semantically rich latent representation using von-Mises-Fisher (vMF) distributions. The proposed approach aims to disentangle compressed representations into object and background classes. The approach is tested on surgical scenes for disentanglement of tools and background information using the Cholec80 dataset. Achieving tool-background disentanglement provides an opportunity to generate rare and custom surgical scenes. However, the proposed method learns to disentangle representations based on pixel intensities. This study uncovers the challenges and shortfalls in achieving object-level disentanglement of the compressed representations using vMF distributions.","Main_56:   Curating datasets for object segmentation is a difficult task. With the
advent of large-scale pre-trained generative models, conditional image
generation has been given a significant boost in result quality and ease of
use. In this paper, we present a novel method that enables the generation of
general foreground-background segmentation models from simple textual
descriptions, without requiring segmentation labels. We leverage and explore
pre-trained latent diffusion models, to automatically generate weak
segmentation masks for concepts and objects. The masks are then used to
fine-tune the diffusion model on an inpainting task, which enables fine-grained
removal of the object, while at the same time providing a synthetic foreground
and background dataset. We demonstrate that using this method beats previous
methods in both discriminative and generative performance and closes the gap
with fully supervised training while requiring no pixel-wise object labels. We
show results on the task of segmenting four different objects (humans, dogs,
cars, birds).
",0.55
"Cite_57_1: In this paper, an interactive learning experience is proposed, aiming to involve museum visitors in a personalized experience of the transmittal of cultural knowledge in an active and creative way. The proposed system, called HapticSOUND, consists of three subsystems: (a) the Information, where visitors are informed about the traditional musical instruments; (b) the Entertainment, where visitors are entertained by playing serious games to virtually assemble traditional musical instruments by a set of 3D objects; and (c) the Interaction, where visitors interact with a digital musical instrument which is an exact 3D-printed replica of a traditional musical instrument, where cameras have been placed to capture user gestures and machine learning algorithms have been implemented for gesture recognition. The museum visitor can interact with the lifelike replica to tactilely and aurally explore the instrument’s abilities, producing sounds guided by the system and receiving real-time visual and audio feedback. Emphasis is given to the Interaction Subsystem, where a pilot study was conducted to evaluate the usability of the subsystem. Preliminary results were promising since the usability was satisfactory, indicating that it is an innovative approach that utilizes sensorimotor learning and machine learning techniques in the context of playing sounds based on real-time gesture and fingering recognition.","Main_57:   With the rise in pervasive computing solutions, interactive surfaces have
gained a large popularity across multi-application domains including smart
boards for education, touch-enabled kiosks for smart retail and smart mirrors
for smart homes. Despite the increased popularity of such interactive surfaces,
existing platforms are mostly limited to custom built surfaces with attached
sensors and hardware, that are expensive and require complicated design
considerations. To address this, we design a low-cost, intuitive system called
MuTable that repurposes any flat surface (such as table tops) into a live
musical instrument. This provides a unique, close to real-time instrument
playing experience to the user to play any type of musical instrument. This is
achieved by projecting the instrument's shape on any tangible surface, sensor
calibration, user taps detection, tap position identification, and associated
sound generation. We demonstrate the performance of our working system by
reporting an accuracy of 83% for detecting softer taps, 100% accuracy for
detecting the regular taps, and a precision of 95.7% for estimating hand
location.
",0.15
"Cite_57_2: This paper introduces a tangible user interface based on projector-camera system. The interface is encoded by stripes in an imperceptible form. We make the first attempt to employ a neural network for extracting hand information from structured light images. The lightweight network of only 0.24 MB parameters infers hand existence, hand segmentation and fingertip locations. These results are utilized for decoding the imperceptible stripes. Experimental results show that the proposed network outperforms state-of-the-art methods, achieving a high IOU of hand segmentation (0.996), and fingertip localization precision (98.98%) within a 10-pixel range. The interactive system performs a touch detection precision of 98.78%, with an average fingertip localization error of 3.33 pixels. It supports real-time interaction with an average execution time is 24.87ms.","Main_57:   With the rise in pervasive computing solutions, interactive surfaces have
gained a large popularity across multi-application domains including smart
boards for education, touch-enabled kiosks for smart retail and smart mirrors
for smart homes. Despite the increased popularity of such interactive surfaces,
existing platforms are mostly limited to custom built surfaces with attached
sensors and hardware, that are expensive and require complicated design
considerations. To address this, we design a low-cost, intuitive system called
MuTable that repurposes any flat surface (such as table tops) into a live
musical instrument. This provides a unique, close to real-time instrument
playing experience to the user to play any type of musical instrument. This is
achieved by projecting the instrument's shape on any tangible surface, sensor
calibration, user taps detection, tap position identification, and associated
sound generation. We demonstrate the performance of our working system by
reporting an accuracy of 83% for detecting softer taps, 100% accuracy for
detecting the regular taps, and a precision of 95.7% for estimating hand
location.
",0.15
"Cite_57_3: This paper presents an empirical study of 8051 in sound based applications by generating an octave of sound using the 8051 microcontroller. The 8051 microcontroller is an ideal choice for various applications due to its simple architecture, versatility, and low cost. In this paper, the focus is on the musical application of the 8051 microcontroller and its ability to generate an octave of notes using a buzzer and push-button interface. The paper presents the design and implementation of the project, including the hardware and software aspects and evaluates the performance of the 8051 microcontroller in terms of its ability to generate accurate musical notes and explores its limitations in this application.","Main_57:   With the rise in pervasive computing solutions, interactive surfaces have
gained a large popularity across multi-application domains including smart
boards for education, touch-enabled kiosks for smart retail and smart mirrors
for smart homes. Despite the increased popularity of such interactive surfaces,
existing platforms are mostly limited to custom built surfaces with attached
sensors and hardware, that are expensive and require complicated design
considerations. To address this, we design a low-cost, intuitive system called
MuTable that repurposes any flat surface (such as table tops) into a live
musical instrument. This provides a unique, close to real-time instrument
playing experience to the user to play any type of musical instrument. This is
achieved by projecting the instrument's shape on any tangible surface, sensor
calibration, user taps detection, tap position identification, and associated
sound generation. We demonstrate the performance of our working system by
reporting an accuracy of 83% for detecting softer taps, 100% accuracy for
detecting the regular taps, and a precision of 95.7% for estimating hand
location.
",0.15
"Cite_58_1: We introduce an interactive decision model capturing the functioning of echo chambers in society. Our starting point is an environment of uncertainty where individuals have preferences over Savage acts. All individuals have core beliefs, but these do not fully determine their behavior. We distinguish a decision maker’s core beliefs from her behavioral beliefs, with the latter incorporating the former but, at the same time, being influenced by the behavioral beliefs of others within the echo chamber. Therefore, echo chambers feature interactive behavioral beliefs and, accordingly, interactive decision-making. We provide conditions on the model set-up that characterize the exact identification of the model parameters from observed behavior. Specifically, under these conditions, we can uniquely identify the composition of the echo chambers in society, the core beliefs of individuals, and the degree to which they are immune to influence. Further, we behaviorally characterize the version of the model that permits exact identification. We relate our model to leading empirical themes on echo chambers like elite influence, the stickiness of beliefs within echo chambers, and the polarization of beliefs across such segments of society","Main_58:   When learning from others, people often focus their attention on those with
similar views. This is often attributed to flawed reasoning, and thought to
slow learning and polarize beliefs. However, we show that echo chambers are a
rational response to uncertain information quality, and can improve learning
and reduce disagreement. Furthermore, broadening the range of views someone is
exposed to can backfire, slowing their learning by making them less responsive
to the views of others. A Bayesian decision maker learns about the world by
first selecting a set of information sources and then observing a signal from
one of them. If some sources are more accurate than others but it's not clear
which, sampling the signals close to one's prior expectation is more
informative, as they are more likely high quality. The optimal echo chamber
balances the credibility of views similar to one's own against the usefulness
of those further away.
",0.05
"Cite_59_1: Gait and balance have a direct impact on patients’ independence and quality of life. Due to a higher life expectancy, the number of patients suffering neurological disorders has increased exponentially, with gait and balance impairments being the main side effects. In this context, the use of rehabilitation robotic devices arises as an effective and complementary tool to recover gait and balance functions. Among rehabilitation devices, end effectors present some advantages and have shown encouraging outcomes. The objective of this study is twofold: to propose a general classification of devices for gait and balance rehabilitation and to provide a review of the existing end effectors for such purposes. We classified the devices into five groups: treadmills, exoskeletons, patient-guided systems, perturbation platforms, and end effectors. Overall, 55 end effectors were identified in the literature, of which 16 were commercialized. We found a disproportionate number of end effectors capable of providing both types of rehabilitation (2/55) and those focused on either balance (21/55) or gait (32/55). The analysis of their features from a mechanical standpoint (degrees of freedom, topology, and training mode) allowed us to identify the potential of parallel manipulators as driving mechanisms of end effector devices and to suggest several future research directions.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_59_2: Neurological diseases are observed in approximately 1 billion people worldwide. A further increase is foreseen at the global level as a result of population growth and aging. Individuals with neurological disorders often experience cognitive, motor, sensory, and lower extremity dysfunctions. Thus, the possibility of falling and balance problems arise due to the postural control deficiencies that occur as a result of the deterioration in the integration of multi-sensory information. We propose a novel rehabilitation framework, Integrated Balance Rehabilitation (I-BaR), to improve the effectiveness of the rehabilitation with objective assessment, individualized therapy, convenience with different disability levels and adoption of assist-as-needed paradigm and, with integrated rehabilitation process as whole, that is, ankle-foot preparation, balance, and stepping phases, respectively. Integrated Balance Rehabilitation allows patients to improve their balance ability by providing multi-modal feedback: visual via utilization of virtual reality; vestibular via anteroposterior and mediolateral perturbations with the robotic platform; proprioceptive via haptic feedback.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_59_3: The increasing interest in balance and cognitive rehabilitation leads to the use of cutting-edge technologies to improve individual well-being. Traditional rehabilitation therapies, such as balance boards, have paved the way for platform-based therapies assisted by robotic platforms. This paper presents the analysis of a four-legged platform with three degrees of freedom, which is already widespread in the gaming and motorsport markets for driving simulations and not yet in rehabilitation applications. A kinematic analysis is performed, pointing out the presence of movements along the not controlled degrees of freedom (i.e. cross talk) of the device. As a second step, dynamic analysis shows how, in the machine under analysis, the actual values of cross-talk also depends on the dynamic response of the platform, governed by friction at the contact between the actuators’ ends and the ground.Finally, the paper explores a design solution, based on spring constraint system, aimed at reducing the dominance of the role played by friction in determining the dynamic response of the system, thus improving the predictability and the deterministic evaluation of cross-talk values.Results indicate promising prospects for the proposed platform in rehabilitation applications, in which the exploitation of cross-talks could turn into an added value to modulate the difficulty levels of the therapy with a compact machine.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_59_4: This study presents a novel Forward Dynamic approach that integrates a Genetic Algorithm (GA) to generate and optimize gait patterns with Inverse Dynamics analysis, tailored to individual users in the robot-assisted training platform. The resulting trajectories provide gait-trajectory planning using the developed gait-pattern simulator, enabling medical personnel to select customized tasks and training trajectories for users based on their training goals, subsequently transferring user-specific parameters to the exoskeletal robot for rehabilitation/training. By leveraging biomechanical data as reference (joint torque and force) from the Inverse Dynamic analysis, the method foregoes the necessity for experimental data, directly predicting joint angles and positions. To ensure the validity of the proposed method, we used a combined approach of numerical analysis and comparison with motion-capture data. This evaluation aimed to assess how closely the simulated results resembled real human walking motion for the lower-extremity joints.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_59_5: In this study, we present the design and development evaluation of BalanSENS toward the realization of the Integrated Balance Rehabilitation (I-BaR) framework. BalanSENS is designed to encourage active participation by integrating multi-sensory information with the co-improvement of sensory and motor functions. Moreover, it can offer individual rehabilitation design with the integration of three phases. The first phase provides foot-ankle muscle activation and movement sensation development. In the second phase, sensory weighting skills and upper extremities independence can be improved by using multi-sensory input. In the last/stepping phase, walking parameters are aimed to be improved with modulated distance. The parallel manipulator is designed through simulations to determine actuation properties and analyze the load-bearing capacity and feasibility of the materials. Drawing from simulation outcomes, an operational 3 DoF platform is constructed to demonstrate their design suitability for the I-BaR framework. Furthermore, design evaluations demonstrated promising results in quantifying force and real-time data monitoring during the passive ankle preparation phase.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
Cite_59_6: A Stewart platform was used to generate translational sinusoidal perturbations in the antero-posterior direction. Eighteen individuals with chronic ankle instability and concurrent ankle pain were recruited. They were instructed to assume a central stance on the support surface with open eyes both before and 30 min after local analgesia. Data of center of pressure and electromyography of the tibialis anterior and medial gastrocnemius were recorded. Statistical analysis was performed to make comparisons pre- and post-analgesia using two-tailed paired t-test for the continuous variables.,"Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_59_7: Multiple sclerosis (MS) is an autoimmune disease that affects more than 1 million people worldwide. Since there is no definitive treatment for the disease, the treatment plan for each patient should be updated regularly according to the current level of the disease. There are standard clinical tests, each with its own scoring scale, used to monitor the deterioration of upper and lower extremity functions of MS patients under the supervision of a neurologist and physiotherapist. However, non-objective scoring based on the opinion of the physiotherapist is open to erroneous assessments and may vary from person to person. In addition, clinical tests do not provide detailed information about the functional impairment of the patient. Unfortunately, an objective evaluation system has not yet been implemented all over the world, and the treatment plan is still determined according to the disease in neurological-based disabilities, such as MS, which is of vital importance. personal assessment. To address the aforementioned problem, the design and experimental evaluation of a wearable thimble-like device that can be substituted for the standard clinical tests to assess the follow-up of MS are presented. The device provides the measurement of high sensitivity and opportunity for objective assessment and allows patients of all ages to use it in any desired place during their treatment phase.","Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
Cite_59_8: Impedance Control for Ankle Preparation Phase in BALANSens Platform Introduction Implementation and Results Conclusion Reference Page 1 Impedance Control for Ankle,"Main_59:   The central nervous system exploits anticipatory (APAs) and compensatory
(CPAs) postural adjustments to maintain the balance.The postural adjustments
comprising stability of the center of mass (CoM) and the pressure distribution
of the body influence each other if there is a lack of performance in either of
them.Any predictable or sudden perturbation may pave the way for the divergence
of CoM from equilibrium and inhomogeneous pressure distribution of the
body.Such a situation is often observed in the daily lives of MS patients due
to their poor APAs and CPAs and induces their falls.The way of minimizing the
risk of falls in neurological patients is by utilizing perturbation-based
rehabilitation, as it is efficient in the recovery of the balance disorder. In
light of the findings, we present the design, implementation, and experimental
evaluation of a novel 3 DoF parallel manipulator to treat the balance disorder
of MS.The robotic platform allows angular motion of the ankle based on its
anthropomorphic freedom.Moreover, the end-effector endowed with upper and lower
platforms is designed to evaluate both the pressure distribution of each foot
and the CoM of the body, respectively.Data gathered from the platforms are
utilized to both evaluate the performance of the patients and used in
high-level control of the robotic platform to regulate the difficulty level of
tasks. In the study, kinematic and dynamic analyses of the robot are derived
and validated in the simulation environment.Low-level control of the first
prototype is also successfully implemented via the PID controller.The capacity
of each platform is evaluated with a set of experiments considering the
assessment of pressure distribution and CoM of the foot-like objects on the
end-effector.The experimental results indicate that such a system well-address
the need for balance skill training and assessment through the APAs and CPAs.
",0.4
"Cite_61_1: We focus on two recently completed NCO studies that assessed the comparability of outcome risk for patients initiating different osteoporosis medications and lipid-lowering therapies, illustrating several ways in which confounding may result. In these studies, NCO methods were implemented in claims-based data sources, with the results used to guide the decision to proceed with comparative effectiveness or safety analyses.","Main_61:   Studies of vaccine efficacy often record both the incidence of
vaccine-targeted virus strains (primary outcome) and the incidence of
non-targeted strains (secondary outcome). However, standard estimates of
vaccine efficacy on targeted strains ignore the data on non-targeted strains.
Assuming non-targeted strains are unaffected by vaccination, we regard the
secondary outcome as a negative control outcome and show how using such data
can (i) increase the precision of the estimated vaccine efficacy against
targeted strains in randomized trials, and (ii) reduce confounding bias of that
same estimate in observational studies. For objective (i), we augment the
primary outcome estimating equation with a function of the secondary outcome
that is unbiased for zero. For objective (ii), we jointly estimate the
treatment effects on the primary and secondary outcomes. If the bias induced by
the unmeasured confounders is similar for both types of outcomes, as is
plausible for factors that influence the general risk of infection, then we can
use the estimated efficacy against the secondary outcomes to remove the bias
from estimated efficacy against the primary outcome. We demonstrate the utility
of these approaches in studies of HPV vaccines that only target a few highly
carcinogenic strains. In this example, using non-targeted strains increased
precision in randomized trials modestly but reduced bias in observational
studies substantially.
",0.15
"Cite_61_2: We estimated HPV vaccine efficacy (VE) from a single arm by comparing 2 ratios: the ratio of the rate of persistent incident infection with vaccine-targeted HPV 16 and 18 (HPV 16/18) and cross-protected types HPV 31, 33, and 45 (HPV 31/33/45) to vaccine-unaffected types HPV 35, 39, 51, 52, 56, 58, 59, and 66 (HPV 35/39/51/52/56/58/59/66) vs the ratio of prevalence of these types at the time of trial enrollment. We compare VE estimates using only data from the bivalent HPV 16/18 vaccine arm of the Costa Rica Vaccine Trial with published VE estimates that used both the vaccine and control arms. Our single-arm approach among 3727 women yielded VE estimates against persistent HPV 16/18 infections similar to published 2-arm estimates from the trial (according-to-protocol cohort: 91.0% , 95% CI = 82.9% to 95.3% [single-arm] vs 90.9% , 95% CI = 82.0% to 95.9% [2-arm]; intention-to-treat cohort: 41.7%, 95% CI = 32.4% to 49.8% [single-arm] vs 49.0% , 95% CI = 38.1% to 58.1% [2-arm]). VE estimates were also similar in analytic subgroups (number of doses received; baseline HPV serology status).","Main_61:   Studies of vaccine efficacy often record both the incidence of
vaccine-targeted virus strains (primary outcome) and the incidence of
non-targeted strains (secondary outcome). However, standard estimates of
vaccine efficacy on targeted strains ignore the data on non-targeted strains.
Assuming non-targeted strains are unaffected by vaccination, we regard the
secondary outcome as a negative control outcome and show how using such data
can (i) increase the precision of the estimated vaccine efficacy against
targeted strains in randomized trials, and (ii) reduce confounding bias of that
same estimate in observational studies. For objective (i), we augment the
primary outcome estimating equation with a function of the secondary outcome
that is unbiased for zero. For objective (ii), we jointly estimate the
treatment effects on the primary and secondary outcomes. If the bias induced by
the unmeasured confounders is similar for both types of outcomes, as is
plausible for factors that influence the general risk of infection, then we can
use the estimated efficacy against the secondary outcomes to remove the bias
from estimated efficacy against the primary outcome. We demonstrate the utility
of these approaches in studies of HPV vaccines that only target a few highly
carcinogenic strains. In this example, using non-targeted strains increased
precision in randomized trials modestly but reduced bias in observational
studies substantially.
",0.15
"Cite_61_3: Adjustment for prognostic baseline variables can reduce bias due to covariate imbalance and increase efficiency in randomized trials. While the use of covariate adjustment in late-phase trials is justified by favorable large-sample properties, it is seldom used in small, early-phase studies, due to uncertainty in which variables are prognostic and the potential for precision loss, type I error rate inflation, and undercoverage of confidence intervals. To address this problem, we consider adjustment for a valid negative control outcome (NCO), or an auxiliary post-randomization outcome believed to be completely unaffected by treatment but more highly correlated with the primary outcome than baseline covariates. We articulate the assumptions that permit adjustment for NCOs without producing post-randomization selection bias, and describe plausible data-generating models where NCO adjustment can improve upon adjustment for baseline covariates alone. In numerical experiments, we illustrate performance and provide practical recommendations regarding model selection and finite-sample variance corrections. We apply our methods to the reanalysis of two early-phase vaccine trials in HIV exposed uninfected (HEU) infants, where we demonstrate that adjustment for auxiliary post-baseline immunological parameters can enhance the precision of vaccine effect estimates relative to standard approaches that avoid adjustment or adjust for baseline covariates alone.","Main_61:   Studies of vaccine efficacy often record both the incidence of
vaccine-targeted virus strains (primary outcome) and the incidence of
non-targeted strains (secondary outcome). However, standard estimates of
vaccine efficacy on targeted strains ignore the data on non-targeted strains.
Assuming non-targeted strains are unaffected by vaccination, we regard the
secondary outcome as a negative control outcome and show how using such data
can (i) increase the precision of the estimated vaccine efficacy against
targeted strains in randomized trials, and (ii) reduce confounding bias of that
same estimate in observational studies. For objective (i), we augment the
primary outcome estimating equation with a function of the secondary outcome
that is unbiased for zero. For objective (ii), we jointly estimate the
treatment effects on the primary and secondary outcomes. If the bias induced by
the unmeasured confounders is similar for both types of outcomes, as is
plausible for factors that influence the general risk of infection, then we can
use the estimated efficacy against the secondary outcomes to remove the bias
from estimated efficacy against the primary outcome. We demonstrate the utility
of these approaches in studies of HPV vaccines that only target a few highly
carcinogenic strains. In this example, using non-targeted strains increased
precision in randomized trials modestly but reduced bias in observational
studies substantially.
",0.15
"Cite_62_1: In the present power system scenario, wide use of harmonics generated nonlinear loads leads to a challenging job in overcoming the power quality (PQ) challenges like voltage sag and swell, voltage flickering etc. Therefore, much advancement is done in designing various custom power devices (CPD) for compensating harmonics. Further, researches are made based on the renewable energy sources (RES) connected to the grid through the CPD. Several controlling techniques are implemented for switching the CPD at point of common coupling (PCC). This paper focuses on the basic fundamentals of harmonics and its effect in the supply system, comparison of various CPDs and its role in various grid-based applications. Also this paper focuses the research study on the role of CPDs in enhancing the PQ and stability with the grid-connected distributed generation sources or the RES. Recent advancements or trends in PQ improvement are discussed in this literature. Therefore, this literature would helpful for the engineers and researchers to improve PQ in various domestic, industrial and commercial sectors. For convenient reference, a volume of periodicals with over 180 papers on the subject are listed.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_2: With the rapid development of distributed energy resources and electric vehicles, the operational complexity of distribution networks, particularly low-voltage distribution networks (LVDNs), has significantly increased. Due to limited observability, high stochastic load variations, and complex network topologies, achieving real-time voltage situational awareness and dynamic control in LVDNs remains a major challenge. Traditional methods struggle to quickly converge and provide comprehensive and accurate situational awareness under such conditions. This paper proposes a Transformer and SAC-based method for voltage situational awareness and control in LVDNs. The Transformer network dynamically captures the spatiotemporal dependencies of voltage states, enhancing the perception and regulation of critical nodes, while the Soft Actor-Critic (SAC) algorithm ensures adaptive voltage control in stochastic environments, improving real-time responsiveness and accuracy. Simulation results demonstrate that the proposed method exhibits strong robustness and fast convergence across various complex LVDN scenarios, effectively capturing voltage situational changes and significantly reducing voltage fluctuations and violation risks.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_3: The widespread integration of photovoltaics (PVs) presents significant challenges to the operation and control of distribution systems, particularly in maintaining voltage stability at nodes with PV connections. To address these challenges, this paper proposes a voltage control algorithm based on distributed model-free adaptive control (MFAC). The control objective is to achieve real-time reactive-power-voltage coordination under constraints including PV power output limitations, voltage safety ranges, and the communication network topology. The proposed method estimates dynamic linearization parameters that represent the voltage control characteristics of the distribution systems by utilizing real-time data from distributed PVs and enabling communication between adjacent nodes. Rather than relying on a precise network model, the algorithm achieves robust voltage control by estimating these parameters from historical and real-time sampling data, employing a data-driven approach to iteratively update control strategies. Multi-scenario simulations of a 32-bus power system demonstrated the effectiveness and robustness of the algorithm across diverse operating conditions.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_4: Volt-VAR optimization (VVO) is essential for maintaining secure voltage levels and reactive power balance in electrical power systems, but the integration of distributed solar energy sources introduces significant variability and intermittency that cause voltage fluctuations. Existing VVO techniques often rely on static models and centralized optimization algorithms, which are computationally intensive and lack real-time adaptability. They also struggle to enforce critical power flow constraints in active distribution networks (ADNs), failing to guarantee the minimum number of voltage violations and power losses across different states of the ADN. To address these limitations, we propose a Deep Reinforcement Learning (DRL) framework utilizing the Importance Weighted Actor-Learner Architecture (IMPALA). This approach excels in handling VVO constraints by integrating power flow constraints directly into the state representation and reward function, enabling the agent to learn adaptive control policies that inherently respect these constraints while optimizing reactive power flow. DRL-IMPALA is particularly well-suited for this application due to its scalability, real-time decision-making capabilities, and efficient distributed computing, achieving a tenfold reduction in computational time compared to traditional methods. We validate our approach through simulations on the IEEE-123 bus system and a real distribution feeder in the Midwest U.S., demonstrating that our DRL-based solution effectively minimizes voltage violations and power losses, adapts rapidly to network changes, and overcomes the shortcomings of existing VVO techniques, making it a robust and efficient solution for power systems with high solar penetration.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_5: The increasing focus on cyber-physical security in Smart Grids (SGs) has catalyzed a surge in research over recent years. This paper comprehensively reviews SG cyber-physical security advancements, diverging from conventional studies that concentrate on specific attack types. It begins with a structured overview of SGs, delineating their cyber and physical layers and analyzing the key processes: generation, transmission, distribution, and consumption. Subsequent sections critique existing survey studies, identifying gaps and underscoring overlooked aspects in the current literature, particularly concerning the challenges faced. The review progresses to analyze current research trends in SG security, evaluating methodologies across both layers and categorizing them into Machine Learning-based, data-driven, and model-based approaches. The analysis includes a detailed classification of research focused on Control, Monitoring, and Protection across each component and stage of SGs. Additionally, the paper examines emerging cyberattack strategies in SGs that have not been extensively reviewed in existing literature. In conclusion, the paper reflects on significant gaps and challenges in SG cyber-physical security research, underscoring the need for further exploration and innovation in this domain. Thus, this review serves as a critical roadmap for future research, delineating the current state and potential directions in the rapidly evolving field of SG security.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_6: In recent years, the integration of distributed generation, particularly solar sources, into the distribution network has increased. However, the current power system control equipment lacks the capability to manage overvoltages at the common coupling point, thereby requiring modifications in control modes through smart inverters to address voltage stability concerns. This study evaluates the effectiveness of Volt-Watt (VW) and Volt-Var (VV) curves in mitigating voltage stability issues in distribution networks with high PV system integration using simulations on the IEEE 34 bus medium voltage distribution system by using OpenDSS and Python. Results indicate that VV curves outperform VW curves, preventing voltage violations while maintaining PV system output close to maximum capacity.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_62_7: Active distribution network (ADN) is faced with significant challenges, including frequent and fast voltage violations, due to the increased integration of intermittent renewable energy resources. This paper proposes a two-stage multi-mode voltage control strategy based on a deep reinforcement learning (DRL) algorithm, designed to alleviate voltage violations in ADN and minimize network power loss. In the first stage, a DRL algorithm, the soft actor-critic (SAC), is introduced to determine the hourly dispatch of on-load tap changers and capacitor banks, ensuring voltage security during the day-ahead stage. A multi-mode voltage regulation strategy is then proposed to obtain real-time dispatch of PV inverters, aiming to save energy and enforce voltage constraints under various conditions. The real-time voltage regulation problem is formulated as a Markov decision process and solved using a multi-agent SAC integrated with an attention mechanism. All agents undergo centralized offline training to learn the optimal coordinated voltage control strategy, then make decentralized online decisions based on locally available information only. The effectiveness of the proposed approach is confirmed through extensive testing on the IEEE 33-bus distribution system, with simulation results conclusively demonstrating its ability to address voltage violation challenges.","Main_62:   Volt-VAR and Volt-Watt functionality in photovoltaic (PV) smart inverters
provide mechanisms to ensure system voltage magnitudes and power factors remain
within acceptable limits. However, these control functions can become unstable,
introducing oscillations in system voltages when not appropriately configured
or maliciously altered during a cyberattack. In the event that Volt-VAR and
Volt-Watt control functions in a portion of PV smart inverters in a
distribution grid are unstable, the proposed adaptation scheme utilizes the
remaining and stably-behaving PV smart inverters and other Distributed Energy
Resources to mitigate the effect of the instability. The adaptation mechanism
is entirely decentralized, model-free, communication-free, and requires
virtually no external configuration. We provide a derivation of the adaptive
control approach and validate the algorithm in experiments on the IEEE 37 and
8500 node test feeders.
",0.35
"Cite_63_1: Recent advances in multimodal techniques have led to significant progress in Medical Visual Question Answering (Med-VQA). However, most existing models focus on global image features rather than localizing disease-specific regions crucial for diagnosis. Additionally, current research tends to emphasize answer accuracy at the expense of the reasoning pathway, yet both are crucial for clinical decision-making. To address these challenges, we propose From Vision to Text Chain-of-Thought (V2T-CoT), a novel approach that automates the localization of preference areas within biomedical images and incorporates this localization into region-level pixel attention as knowledge for Vision CoT. By fine-tuning the vision language model on constructed R-Med 39K dataset, V2T-CoT provides definitive medical reasoning paths. V2T-CoT integrates visual grounding with textual rationale generation to establish precise and explainable diagnostic results. Experimental results across four Med-VQA benchmarks demonstrate state-of-the-art performance, achieving substantial improvements in both performance and interpretability.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_2: Recently, vision-language pretraining has emerged as a transformative technique that integrates the strengths of both visual and textual modalities, resulting in powerful vision-language models (VLMs). Leveraging web-scale pretraining data, these models exhibit strong zero-shot capabilities. However, their performance often deteriorates when confronted with domain-specific or specialized generalization tasks. To address this, a growing body of research focuses on transferring or generalizing the rich knowledge embedded in VLMs to various downstream applications. This survey aims to comprehensively summarize the generalization settings, methodologies, benchmarking and results in VLM literatures. Delving into the typical VLM structures, current literatures are categorized into prompt-based, parameter-based and feature-based methods according to the transferred modules. The differences and characteristics in each category are furthered summarized and discussed by revisiting the typical transfer learning (TL) settings, providing novel interpretations for TL in the era of VLMs. Popular benchmarks for VLM generalization are further introduced with thorough performance comparisons among the reviewed methods. Following the advances in large-scale generalizable pretraining, this survey also discusses the relations and differences between VLMs and up-to-date multimodal large language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the surging literatures in vision-language research from a novel and practical generalization prospective, this survey contributes to a clear landscape of current and future multimodal researches.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_3: The remarkable success of transformers in the field of natural language processing has sparked interest in their potential for mod- elling long-range dependencies within speech sequences. Transformers have gained prominence across various speech-related do- mains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. However, the integration of transformers in speech processing comes with significant challenges such as managing the high computational costs, handling the complexity of speech variability, and addressing the data scarcity for certain speech tasks. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_4: Crossmodal knowledge distillation (KD) aims to enhance a unimodal student using a multimodal teacher model. In particular, when the teacher's modalities include the student's, additional complementary information can be exploited to improve knowledge transfer. In supervised image classification, image datasets typically include class labels that represent high-level concepts, suggesting a natural avenue to incorporate textual cues for crossmodal KD. However, these labels rarely capture the deeper semantic structures in real-world visuals and can lead to label leakage if used directly as inputs, ultimately limiting KD performance. To address these issues, we propose a multi-teacher crossmodal KD framework that integrates CLIP image embeddings with learnable WordNet-relaxed text embeddings under a hierarchical loss. By avoiding direct use of exact class names and instead using semantically richer WordNet expansions, we mitigate label leakage and introduce more diverse textual cues. Experiments show that this strategy significantly boosts student performance, whereas noisy or overly precise text embeddings hinder distillation efficiency. Interpretability analyses confirm that WordNet-relaxed prompts encourage heavier reliance on visual features over textual shortcuts, while still effectively incorporating the newly introduced textual cues. Our method achieves state-of-the-art or second-best results on six public datasets, demonstrating its effectiveness in advancing crossmodal KD.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_5: Visual Language Models such as CLIP excel in image recognition due to extensive image-text pre-training. However, applying the CLIP inference in zero-shot classification, particularly for medical image diagnosis, faces challenges due to: 1) the inadequacy of representing image classes solely with single category names; 2) the modal gap between the visual and text spaces generated by CLIP encoders. Despite attempts to enrich disease descriptions with large language models, the lack of class-specific knowledge often leads to poor performance. In addition, empirical evidence suggests that existing proxy learning methods for zero-shot image classification on natural image datasets exhibit instability when applied to medical datasets. To tackle these challenges, we introduce the Knowledge Proxy Learning (KPL) to mine knowledge from CLIP. KPL is designed to leverage CLIP's multimodal understandings for medical image classification through Text Proxy Optimization and Multimodal Proxy Learning. Specifically, KPL retrieves image-relevant knowledge descriptions from the constructed knowledge-enhanced base to enrich semantic text proxies. It then harnesses input images and these descriptions, encoded via CLIP, to stably generate multimodal proxies that boost the zero-shot classification performance. Extensive experiments conducted on both medical and natural image datasets demonstrate that KPL enables effective zero-shot image classification, outperforming all baselines. These findings highlight the great potential in this paradigm of mining knowledge from CLIP for medical image classification and broader areas.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_6: Composed image retrieval is designed to more accurately retrieve target images that align with user intentions by using a combination of reference images and descriptive modification texts. However, existing methods primarily focus on designing complex feature fusion networks while neglecting the prevalent issues of noise and inconsistent sample quality in training data, leading to insufficient cross-modal semantic alignment and sample relevance modeling. To address this, we propose an innovative Tripartite Alignment Network (TAN) that introduces a momentum distillation mechanism, leveraging the historical knowledge of a teacher network as additional super-vision to guide the optimization of the student network. During the feature encoder fine-tuning stage, we design response-based knowledge distillation and feature-based knowledge distillation techniques, explicitly strengthening modal alignment through composed-target contrastive learning and implicitly promoting modal fusion via composed-target matching learning. In the combiner training stage, we incorporate a lightweight combiner network and employ a cross-entropy-based matching loss function, encouraging high matching scores for relevant image-text pairs and low scores for irrelevant pairs. Extensive experiments on the FashionIQ and Shoes datasets demonstrate that TAN exhibits superior performance compared to existing state-of-the-art methods, with notable improvements in R@10 of +14.03% and +13.09%, respectively. These results affirm the effectiveness of momentum distillation in multimodal learning. ","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_7: We propose BridgeCLIP, an innovative framework designed to harness the power of vision-language models for bridge inspection from images. BridgeCLIP is a CLIP-based multi-label classifier that finds multiple damages in a single bridge image. Pre-trained vision-language models learn the relationships between general objects by millions of text-image pairs, of which descriptions are not precise enough for domain-specific problems. Following the concept that humans normally learn the visual appearance of bridge damage by reading a manual, we introduce a novel Description Attention Module (DAM) to incorporate the domain-specific knowledge extracted from the professional descriptions in bridge inspection manuals. By utilizing both general knowledge of the pre-trained CLIP and professional knowledge of bridge inspection, BridgeCLIP comprehensively learns the inter-class relationships of different damages. Experimental results on bridge inspection datasets show that BridgeCLIP outperforms the state-of-the-art multi-label classifiers.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_8: Knowledge Distillation is the mechanism by which the insights gained from a larger teacher model are transferred to a smaller student model. However, the transfer suffers when the teacher model is significantly larger than the student. To overcome this, prior works have proposed training intermediately sized models, Teacher Assistants (TAs) to help the transfer process. However, training TAs is expensive, as training these models is a knowledge transfer task in itself. Further, these TAs are larger than the student model and training them especially in large data settings can be computationally intensive. In this paper, we propose a novel framework called Controlled Information Flow for Knowledge Distillation (CIFD) consisting of two components. First, we propose a significantly smaller alternatives to TAs, the Rate-Distortion Module (RDM) which uses the teacher's penultimate layer embedding and a information rate-constrained bottleneck layer to replace the Teacher Assistant model. RDMs are smaller and easier to train than TAs, especially in large data regimes, since they operate on the teacher embeddings and do not need to relearn low level input feature extractors. Also, by varying the information rate across the bottleneck, RDMs can replace TAs of different sizes. Secondly, we propose the use of Information Bottleneck Module in the student model, which is crucial for regularization in the presence of a large number of RDMs. We show comprehensive state-of-the-art results of the proposed method over large datasets like Imagenet. Further, we show the significant improvement in distilling CLIP like models over a huge 12M image-text dataset. It outperforms CLIP specialized distillation methods across five zero-shot classification datasets and two zero-shot image-text retrieval datasets.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_9: Vision-and-language navigation (VLN) is a challenging task that requires an agent to navigate an indoor environment using natural language instructions. Traditional VLN employs cross-modal feature fusion, where visual and textual information are combined to guide the agent’s navigation. However, incomplete use of perceptual information, scarcity of domain-specific training data, and diverse image and language inputs result in suboptimal performance. Herein, we propose a cross-modal feature fusion VLN history-aware information, that leverages an agent’s past experiences to make more informed navigation decisions. The regretful model and self-monitoring models are added, and the advantage actor critic(A2C) reinforcement learning algorithm is employed to improve the navigation success rate, reduce action redundancy, and shorten navigation paths. Subsequently, a data augmentation method based on speaker data is introduced to improve the model generalizability. We evaluate the proposed algorithm on the room-to-room (R2R) and room-for-room (R4R) benchmarks, and the experimental results demonstrate that, by comparison, the proposed algorithm outperforms state-of-the-art methods.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_10: In this paper, we explore a novel Text-supervised Egocentic Semantic Segmentation (TESS) task that aims to assign pixel-level categories to egocentric images weakly supervised by texts from image-level labels. In this task with prospective potential, the egocentric scenes contain dense wearer-object relations and inter-object interference. However, most recent third-view methods leverage the frozen Contrastive Language-Image Pre-training (CLIP) model, which is pre-trained on the semantic-oriented third-view data and lapses in the egocentric view due to the “relation insensitive” problem. Hence, we propose a Cognition Transferring and Decoupling Network (CTDN) that first learns the egocentric wearer-object relations via correlating the image and text. Besides, a Cognition Transferring Module (CTM) is developed to distill the cognitive knowledge from the large-scale pre-trained model to our model for recognizing egocentric objects with various semantics. Based on the transferred cognition, the Foreground-background Decoupling Module (FDM) disentangles the visual representations to explicitly discriminate the foreground and background regions to mitigate false activation areas caused by foreground-background interferential objects during egocentric relation learning. Extensive experiments on four TESS benchmarks demonstrate the effectiveness of our approach, which outperforms many recent related methods by a large margin.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_11: Prompt learning has surfaced as an effective approach to enhance the performance of Vision-Language Models (VLMs) like CLIP when applied to downstream tasks. However, current learnable prompt tokens are primarily used for the single phase of adapting to tasks (i.e., adapting prompt), easily leading to overfitting risks. In this work, we propose a novel Cascade Prompt Learning (CasPL) framework to enable prompt learning to serve both generic and specific expertise (i.e., boosting and adapting prompt) simultaneously. Specifically, CasPL is a new learning paradigm comprising two distinct phases of learnable prompts: the first boosting prompt is crafted to extract domain-general knowledge from a senior larger CLIP teacher model by aligning their predicted logits using extensive unlabeled domain images. The second adapting prompt is then cascaded with the frozen first set to fine-tune the downstream tasks, following the approaches employed in prior research. In this manner, CasPL can effectively capture both domain-general and task-specific representations into explicitly different gradual groups of prompts, thus potentially alleviating overfitting issues in the target domain. It’s worth noting that CasPL serves as a plug-and-play module that can seamlessly integrate into any existing prompt learning approach. CasPL achieves a significantly better balance between performance and inference speed, which is especially beneficial for deploying smaller VLM models in resource-constrained environments. Compared to the previous state-of-the-art method PromptSRC, CasPL shows an average improvement of 1.85for base classes, 3.44  for novel classes, and 2.72 for the harmonic mean over 11 image classification datasets. ","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_12: Prompt learning is an effective method to customize Vision-Language Models (VLMs) for various downstream tasks, involving tuning very few parameters of input prompt tokens. Recently, prompt pretraining in large-scale dataset (e.g., ImageNet-21K) has played a crucial role in prompt learning for universal visual discrimination. However, we revisit and observe that the limited learnable prompts could face underfitting risks given the extensive images during prompt pretraining, simultaneously leading to poor generalization. To address the above issues, in this paper, we propose a general framework termed Revisiting Prompt Pretraining (RPP), which targets at improving the fitting and generalization ability from two aspects: prompt structure and prompt supervision. For prompt structure, we break the restriction in common practice where query, key, and value vectors are derived from the shared learnable prompt token. Instead, we introduce unshared individual query, key, and value learnable prompts, thereby enhancing the model's fitting capacity through increased parameter diversity. For prompt supervision, we additionally utilize soft labels derived from zero-shot probability predictions provided by a pretrained Contrastive Language Image Pretraining (CLIP) teacher model. These soft labels yield more nuanced and general insights into the inter-class relationships, thereby endowing the pretraining process with better generalization ability. RPP produces a more resilient prompt initialization, enhancing its robust transferability across diverse visual recognition tasks. Experiments across various benchmarks consistently confirm the state-of-the-art (SOTA) performance of our pretrained prompts. Codes and models will be made available soon.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_13: Contrastive Language-Image Pre-training (CLIP) has achieved excellent performance over a wide range of tasks. However, the effectiveness of CLIP heavily relies on a substantial corpus of pre-training data, resulting in notable consumption of computational resources. Although knowledge distillation has been widely applied in single modality models, how to efficiently expand knowledge distillation to vision-language foundation models with extensive data remains relatively unexplored. In this paper, we introduce CLIP-CID, a novel distillation mechanism that effectively transfers knowledge from a large vision-language foundation model to a smaller model. We initially propose a simple but efficient image semantic balance method to reduce transfer learning bias and improve distillation efficiency. This method filters out 43.7% of image-text pairs from the LAION400M while maintaining superior performance. After that, we leverage cluster-instance discrimination to facilitate knowledge transfer from the teacher model to the student model, thereby empowering the student model to acquire a holistic semantic comprehension of the pre-training data. Experimental results demonstrate that CLIP-CID achieves state-of-the-art performance on various downstream tasks including linear probe and zero-shot classification.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_63_14: Composed image retrieval, aimed at enhancing user searches by accurately capturing intent, involves semantically aligning and fusing image and text features. We propose the Semantic-guided Hierarchical Alignment and Fusion network (SHAF), which is specifically designed to combine information from both visual and textual modalities across various network layers. SHAF employs attention mechanisms to progressively align text and image features from low to high levels, effectively bridging the semantic gap between these modalities. The network integrates complementary information from images and text fragments in the query through dynamic weight allocation and feature enhancement mechanisms. This process generates a composite feature within a unified embedding space. Extensive experiments on the FashionIQ and Shoes (+7.15 and +7.58 in R@10) datasets show that SHAF performs better than the state-of-the-art models in composed image retrieval tasks.","Main_63:   Contrastive language-image pretraining (CLIP) links vision and language
modalities into a unified embedding space, yielding the tremendous potential
for vision-language (VL) tasks. While early concurrent works have begun to
study this potential on a subset of tasks, important questions remain: 1) What
is the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in
low-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches
without impacting inference or pretraining complexity? In this work, we seek to
answer these questions through two key contributions. First, we introduce an
evaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual
Entailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of
data availability constraints and conditions of domain shift. Second, we
propose an approach, named CLIP Targeted Distillation (CLIP-TD), to
intelligently distill knowledge from CLIP into existing architectures using a
dynamically weighted objective applied to adaptively selected tokens per
instance. Experiments demonstrate that our proposed CLIP-TD leads to
exceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to
71.3%) conditions of VCR, while simultaneously improving performance under
standard fully-supervised conditions (up to 2%), achieving state-of-art
performance on VCR compared to other single models that are pretrained with
image-text data only. On SNLI-VE, CLIP-TD produces significant gains in
low-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On
VQA, CLIP-TD provides improvement in low-shot (up to 9%), and in
fully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works
utilizing CLIP for finetuning, as well as baseline naive distillation
approaches. Code will be made available.
",0.7
"Cite_64_1: In this paper, we study an efficient algorithm for constructing point sets underlying quasi-Monte Carlo integration rules for weighted Korobov classes. The algorithm presented is a reduced fast component-by-component digit-by-digit (CBC-DBD) algorithm, which is useful for situations where the weights in the function space show a sufficiently fast decay. The advantage of the algorithm presented here is that the computational effort can be independent of the dimension of the integration problem to be treated if suitable assumptions on the integrand are met. By considering a reduced digit-by-digit construction, we allow an integration algorithm to be less precise with respect to the number of bits in those components of the problem that are considered less important. The new reduced CBC-DBD algorithm is designed to work for the construction of lattice point sets, and the corresponding integration rules (so-called lattice rules) can be used to treat functions in different kinds of function spaces. We show that the integration rules constructed by our algorithm satisfy error bounds of almost optimal convergence order. Furthermore, we give details on an efficient implementation such that we obtain a considerable speed-up of a previously known CBC-DBDalgorithmthat has been studied in the paper Digit-by-digit and component-by-component constructions of lattice rules for periodic functions with unknown smoothness by Ebert, Kritzer, Nuyens, and Osisiogu, published in the Journal of Complexity in 2021. This improvement is illustrated by numerical results.","Main_64:   Lattice rules are among the most prominently studied quasi-Monte Carlo
methods to approximate multivariate integrals.
  A rank-$1$ lattice rule to approximate an $s$-dimensional integral is fully
specified by its \emph{generating vector} $\boldsymbol{z} \in \mathbb{Z}^s$ and
its number of points~$N$.
  While there are many results on the existence of ``good'' rank-$1$ lattice
rules, there are no explicit constructions of good generating vectors for
dimensions $s \ge 3$.
  This is why one usually resorts to computer search algorithms. In a recent
paper by Ebert et al. in the Journal of Complexity, we showed a
component-by-component digit-by-digit (CBC-DBD)
  construction for good generating vectors of rank-1 lattice rules for
integration of functions in weighted Korobov classes. However, the result in
that paper was
  limited to product weights. In the present paper, we shall generalize this
result to arbitrary positive weights, thereby answering an open question posed
  in the paper of Ebert et al. We also include a short section on how the
algorithm can be implemented in the case of POD weights, by which we see that
the CBC-DBD construction is competitive with the classical CBC construction.
",0.05
Cite_65_1: ,"Main_65:   We generalize the familiar notion of periodicity in sequences to a new kind
of pseudoperiodicity, and we prove some basic results about it. We revisit the
results of a 2012 paper of Shevelev and reprove his results in a simpler and
more unified manner, and provide a complete answer to one of his previously
unresolved questions. We consider finding words with specific pseudoperiods and
having the smallest possible critical exponent. Finally, we consider the
problem of determining whether a finite word is pseudoperiodic of a given size,
and show that it is NP-complete.
",0.15
"Cite_65_2: We revisit the topic of power-free morphisms, focusing on the properties of the class of complementary morphisms. Such morphisms are defined over a 2-letter alphabet, and map the letters 0 and 1 to complementary words. We prove that every prefix of the famous Thue–Morse word t gives a complementary morphism that is -free and hence α-free for every real number . We also describe, using a 4-state binary finite automaton, the lengths of all pre.","Main_65:   We generalize the familiar notion of periodicity in sequences to a new kind
of pseudoperiodicity, and we prove some basic results about it. We revisit the
results of a 2012 paper of Shevelev and reprove his results in a simpler and
more unified manner, and provide a complete answer to one of his previously
unresolved questions. We consider finding words with specific pseudoperiods and
having the smallest possible critical exponent. Finally, we consider the
problem of determining whether a finite word is pseudoperiodic of a given size,
and show that it is NP-complete.
",0.15
"Cite_65_3: We present a novel perspective on the NFA canonization problem, which introduces intermediate minimization steps to reduce the exploration space on-the-fly. Essential to our approach are so-called equivalence registries which manage information about equivalent states and allow for incorporating further optimization techniques such as convexity closures or simulation to boost performance. Due to the generality of our approach, these concepts can be embedded in classic subset construction or Brzozowski's approach. We evaluate our approach on a set of real-world examples from automatic sequences and observe that we are able to improve especially worst-case scenarios. We implement our approach in an open-source library for users to experiment with.","Main_65:   We generalize the familiar notion of periodicity in sequences to a new kind
of pseudoperiodicity, and we prove some basic results about it. We revisit the
results of a 2012 paper of Shevelev and reprove his results in a simpler and
more unified manner, and provide a complete answer to one of his previously
unresolved questions. We consider finding words with specific pseudoperiods and
having the smallest possible critical exponent. Finally, we consider the
problem of determining whether a finite word is pseudoperiodic of a given size,
and show that it is NP-complete.
",0.15
"Cite_67_1: We consider the rich dynamics and bifurcations exhibited by a three-dimensional quadratic map. Torus doubling bifurcations are central to bifurcation theory. Such bifurcations can only occur in a higher dimensional map of dimensions greater than or equal to three. After subsequent doublings, formation of Shilnikov attractors and hyperchaotic attractors are usually observed. It is shown that the map under consideration shows both resonant and ergodic torus doubling bifurcation. We show that the system exhibits resonant torus doubling bifurcation in which the doubled mode-locked periodic orbits lies on a Möbius strip. Additionally, we also illustrate the doubling of ergodic tori and analyze the bifurcation via the use of the second Poincaré section and the method of Lyapunov bundles. The analysis involves the techniques of construction of one-dimensional manifolds, a one-parameter continuation of saddle periodic orbits, and multi-dimensional Newton–Raphson method to locate the saddle periodic orbits.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_2: his paper deals with various routes to hyperchaos with all three positive Lyapunov exponents in a three-dimensional quadratic map. The map under consideration displays strong hyperchaoticity in the sense that in a wider range of parameter space, the system showcases three positive Lyapunov exponents. It is shown that the saddle periodic orbits eventually become repellers at this hyperchaotic regime. By computing the distance of the repellers to the attractors as a function of parameters, it is shown that the hyperchaotic attractors absorb the repelling periodic orbits. First, we discuss a route from stable fixed point undergoing period-doubling bifurcations to chaos and then hyperchaos and role of saddle periodic orbits. We then illustrate a route from doubling bifurcation of quasiperiodic closed invariant curves to hyperchaotic attractors. Finally, the presence of weak hyperchaotic flow-like attractors is discussed.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_3: Stabilization by a periodic pulsed force of trajectories running away to infinity in the three-dimensional Rössler system at a threshold of a saddle-node bifurcation, birth of equilibrium states is studied. It is shown that the external pulsed action stabilizes dynamical regimes in a fairly wide range of external signal parameters. Stabilized regimes can be periodic, quasi-periodic, or chaotic. Three types of chaotic oscillations are revealed depending on the spectrum of Lyapunov exponents: the simplest (classical) and multi-dimensional (hyperchaos and chaos with additional zero Lyapunov exponent). Scenarios for the development of multi-dimensional chaos have been studied in detail. The universality of the observed behavior when changing the direction of the external force is investigated. It is shown that the effects are universal, when force acts in a plane corresponding to focal behavior of the trajectories, stabilization is not observed if the direction of force is perpendicular to the plane. The universality of the obtained picture is studied when the autonomous dynamics of the model change, it is shown that for small periods of external action, the picture is determined by a transient processes of autonomous model and remains characteristic for stabilization. With an increase in the period of external force, the properties of an autonomous model appear.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_4: The emergence of collective oscillations and synchronization is a widespread phenomenon in complex systems. While widely studied in the setting of dynamical systems, this phenomenon is not well understood in the context of out-of-equilibrium phase transitions in many-body systems. Here we consider three classical lattice models, namely the Ising, the Blume-Capel, and the Potts models, provided with a feedback among the order and control parameters. With the help of the linear response theory we derive low-dimensional nonlinear dynamical systems for mean-field cases. These dynamical systems quantitatively reproduce many-body stochastic simulations. In general, we find that the usual equilibrium phase transitions are taken over by more complex bifurcations where nonlinear collective self-oscillations emerge, a behavior that we illustrate by the feedback Landau theory. For the case of the Ising model, we obtain that the bifurcation that takes over the critical point is nontrivial in finite dimensions. Namely, we provide numerical evidence that in two dimensions the most probable value of a cycle's amplitude follows the Onsager law for slow feedback. We illustrate multistability for the case of discontinuously emerging oscillations in the Blume-Capel model, whose tricritical point is substituted by the Bautin bifurcation. For the Potts model with 𝑞=3 colors we highlight the appearance of two mirror stable limit cycles at a bifurcation line and characterize the onset of chaotic oscillations that emerge at low temperature through either the Feigenbaum cascade of period doubling or the Afraimovich-Shilnikov scenario of a torus destruction. We also demonstrate that entropy production singularities as a function of the temperature correspond to qualitative change in the spectrum of Lyapunov exponents. Our results show that mean-field collective behavior can be described by the bifurcation theory of low-dimensional dynamical systems, which paves the way for the definition of universality classes of collective oscillations.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_5: A non-autonomous model of the Anishchenko–Astakhov generator in the regime of periodic and chaotic self-oscillations is considered. A periodic sequence of short pulses is considered as an external force. It is shown that the synchronization picture is close in structure to the classical synchronization picture observed in a two-dimensional system, but the pulse action leads to the excitation of chaotic oscillations, including those characterized by a different spectrum of Lyapunov exponents. In particular, it is shown appearance of hyperchaos and chaos with additional close to zero Lyapunov exponent. Phenomenological scenarios for the development of multi-dimensional chaos related to destruction of two-frequency tori are described. Hyperchaos is formed via hierarchy of discrete Shilnikov attractors arise as a result of sequence of Neimark-Sacker bifurcations. Chaos with additional close to zero Lyapunov exponent occurs as impact of saddle tori appeared via sequence of torus-doubling bifurcations.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_6: The problem of high-dimensional chaos with additional zero Lyapunov exponents is discussed. A review of both early and modern publications is presented. Specific examples of systems of different nature and model systems are considered. The review is supplemented with illustrations on the example of a discrete version of the Lorenz-84 system and a flow system consisting of subsystems with multi-frequency quasiperiodicity and chaos. Related effects such as quasi-periodic resonant tongues, quasi-periodic windows in chaos, and quasi-periodic shrimps are also discussed.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_7: We study hyperchaotic attractors characterized by three positive Lyapunov exponents in numerical experiments. In order to possess this property, periodic orbits belonging to the attractor should have a three-dimensional unstable invariant manifold. Starting with a stable fixed point we describe several bifurcation scenarios that create such periodic orbits inside the attractor. These scenarios include cascades of alternating period-doubling and Neimark – Sacker bifurcations which, as we show, naturally appear near the cascade of codimension-2 period-doubling bifurcations, when periodic orbits along the cascade have multipliers . The proposed scenarios are illustrated by examples of the three-dimensional Kaneko endomorphism and a four-dimensional Hénon map.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_8: We study nonlinear dynamics in a model of three interacting encapsulated gas bubbles in a liquid. The model is a system of three coupled nonlinear oscillators with an external periodic force. Such bubbles have numerous applications, for instance, they are used as contrast agents in ultrasound visualization. Certain types of bubbles oscillations may be beneficial or undesirable depending on a given application and, hence, the dependence of the regimes of bubbles oscillations on the control parameters is worth studying. We demonstrate that there is a wide variety of types of dynamics in the model by constructing a chart of dynamical regimes in the control parameters space. Here we focus on hyperchaotic attractors characterized by three positive Lyapunov exponents and strange attractors with one or two positive Lyapunov exponents possessing an additional zero Lyapunov exponent, which have not been observed previously in the context of bubbles oscillations. We also believe that we provide a first example of a hyperchaotic attractor with additional zero Lyapunov exponent. Furthermore, the mechanisms of the onset of these types of attractors are still not well studied. We identify two-parametric regions in the control parameter space where these hyperchaotic and chaotic attractors appear and study one-parametric routes leading to them. We associate the appearance of hyperchaotic attractors with three positive Lyapunov exponents with the inclusion of a periodic orbit with a three-dimensional unstable manifold, while the onset of chaotic oscillations with an additional zero Lyapunov exponent is connected to the partial synchronization of bubbles oscillations. We propose several underlying bifurcation mechanisms that explain the emergence of these regimes. We believe that these bifurcation scenarios are universal and can be observed in other systems of coupled oscillators.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_9: The dynamics of two coupled neuron models, the Hindmarsh – Rose systems, are studied. Their interaction is simulated via a chemical coupling that is implemented with a sigmoid function. It is shown that the model may exhibit complex behavior: quasi-periodic, chaotic and hyperchaotic oscillations. A phenomenological scenario for the formation of hyperchaos associated with the appearance of a discrete Shilnikov attractor is described. It is shown that the formation of these attractors leads to the appearance of in-phase bursting oscillations.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_67_10: The dynamics of two coupled neuron models, the Hindmarsh – Rose systems, are studied. Their interaction is simulated via a chemical coupling that is implemented with a sigmoid function. It is shown that the model may exhibit complex behavior: quasi-periodic, chaotic and hyperchaotic oscillations. A phenomenological scenario for the formation of hyperchaos associated with the appearance of a discrete Shilnikov attractor is described. It is shown that the formation of these attractors leads to the appearance of in-phase bursting oscillations.","Main_67:   We study bifurcation mechanisms for the appearance of hyperchaotic attractors
in three-dimensional diffeomorphisms, i.e., such attractors whose orbits have
two positive Lyapunov exponents in numerical experiments. In order to possess
this property periodic orbits belonging to the attractor should have
two-dimensional unstable invariant manifolds. For realization of this
possibility, we propose several bifurcation scenarios that include cascades of
both supercritical period-doubling bifurcations with saddle periodic orbits and
supercritical Neimark-Sacker bifurcations with stable periodic orbits, as well
as various combinations of these cascades. In the paper, these scenarios are
illustrated by an example of the three-dimensional Mir\'a map.
",0.5
"Cite_68_1: Detecting small drones, often indistinguishable from birds, is crucial for modern surveillance. This work introduces a drone detection methodology built upon the medium-sized YOLOv11 object detection model. To enhance its performance on small targets, we implemented a multi-scale approach in which the input image is processed both as a whole and in segmented parts, with subsequent prediction aggregation. We also utilized a copy-paste data augmentation technique to enrich the training dataset with diverse drone and bird examples. Finally, we implemented a post-processing technique that leverages frame-to-frame consistency to mitigate missed detections. The proposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird Detection Grand Challenge, held at the 2025 International Joint Conference on Neural Networks (IJCNN), showcasing its capability to detect drones in complex environments effectively.","Main_68:   Public datasets have played a key role in advancing the state of the art in
License Plate Recognition (LPR). Although dataset bias has been recognized as a
severe problem in the computer vision community, it has been largely overlooked
in the LPR literature. LPR models are usually trained and evaluated separately
on each dataset. In this scenario, they have often proven robust in the dataset
they were trained in but showed limited performance in unseen ones. Therefore,
this work investigates the dataset bias problem in the LPR context. We
performed experiments on eight datasets, four collected in Brazil and four in
mainland China, and observed that each dataset has a unique, identifiable
""signature"" since a lightweight classification model predicts the source
dataset of a license plate (LP) image with more than 95% accuracy. In our
discussion, we draw attention to the fact that most LPR models are probably
exploiting such signatures to improve the results achieved in each dataset at
the cost of losing generalization capability. These results emphasize the
importance of evaluating LPR models in cross-dataset setups, as they provide a
better indication of generalization (hence real-world performance) than
within-dataset ones.
",0.2
"Cite_68_2: Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help with this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection and an OCR model to extract license plate numbers with impressive accuracy.","Main_68:   Public datasets have played a key role in advancing the state of the art in
License Plate Recognition (LPR). Although dataset bias has been recognized as a
severe problem in the computer vision community, it has been largely overlooked
in the LPR literature. LPR models are usually trained and evaluated separately
on each dataset. In this scenario, they have often proven robust in the dataset
they were trained in but showed limited performance in unseen ones. Therefore,
this work investigates the dataset bias problem in the LPR context. We
performed experiments on eight datasets, four collected in Brazil and four in
mainland China, and observed that each dataset has a unique, identifiable
""signature"" since a lightweight classification model predicts the source
dataset of a license plate (LP) image with more than 95% accuracy. In our
discussion, we draw attention to the fact that most LPR models are probably
exploiting such signatures to improve the results achieved in each dataset at
the cost of losing generalization capability. These results emphasize the
importance of evaluating LPR models in cross-dataset setups, as they provide a
better indication of generalization (hence real-world performance) than
within-dataset ones.
",0.2
"Cite_68_3: This paper introduces a layout-independent and efficient automatic number plate detection system based on the YOLO world. The system employs a unified approach to both number plate detection and layout classification, to enhance recognition outcomes by applying normalization and denormalization rules. By estimating and enhancing various models, the system is conceptualized to attain the optimal speed-to-accuracy ratio at each stage. In addition to being trained on images from multiple datasets, the networks are also fortified with a variety of data augmentation techniques to ensure their robustness under a variety of circumstances. In the Automobile, Stanford cars, CCPD dataset, used cars, and real-time datasets, it outperformed both prior works and commercial systems. Furthermore, the author annotated 5,000 images from public datasets with 10,000 bounding boxes manually using Roboflow. These annotations were then made available to the research community at large.","Main_68:   Public datasets have played a key role in advancing the state of the art in
License Plate Recognition (LPR). Although dataset bias has been recognized as a
severe problem in the computer vision community, it has been largely overlooked
in the LPR literature. LPR models are usually trained and evaluated separately
on each dataset. In this scenario, they have often proven robust in the dataset
they were trained in but showed limited performance in unseen ones. Therefore,
this work investigates the dataset bias problem in the LPR context. We
performed experiments on eight datasets, four collected in Brazil and four in
mainland China, and observed that each dataset has a unique, identifiable
""signature"" since a lightweight classification model predicts the source
dataset of a license plate (LP) image with more than 95% accuracy. In our
discussion, we draw attention to the fact that most LPR models are probably
exploiting such signatures to improve the results achieved in each dataset at
the cost of losing generalization capability. These results emphasize the
importance of evaluating LPR models in cross-dataset setups, as they provide a
better indication of generalization (hence real-world performance) than
within-dataset ones.
",0.2
"Cite_68_4: Despite significant advancements in License Plate Recognition (LPR) through deep learning, most improvements rely on high-resolution images with clear characters. This scenario does not reflect real-world conditions where traffic surveillance often captures low-resolution and blurry images. Under these conditions, characters tend to blend with the background or neighboring characters, making accurate LPR challenging. To address this issue, we introduce a novel loss function, Layout and Character Oriented Focal Loss (LCOFL), which considers factors such as resolution, texture, and structural details, as well as the performance of the LPR task itself. We enhance character feature learning using deformable convolutions and shared weights in an attention module and employ a GAN-based training approach with an Optical Character Recognition (OCR) model as the discriminator to guide the super-resolution process. Our experimental results show significant improvements in character reconstruction quality, outperforming two state-of-the-art methods in both quantitative and qualitative measures. ","Main_68:   Public datasets have played a key role in advancing the state of the art in
License Plate Recognition (LPR). Although dataset bias has been recognized as a
severe problem in the computer vision community, it has been largely overlooked
in the LPR literature. LPR models are usually trained and evaluated separately
on each dataset. In this scenario, they have often proven robust in the dataset
they were trained in but showed limited performance in unseen ones. Therefore,
this work investigates the dataset bias problem in the LPR context. We
performed experiments on eight datasets, four collected in Brazil and four in
mainland China, and observed that each dataset has a unique, identifiable
""signature"" since a lightweight classification model predicts the source
dataset of a license plate (LP) image with more than 95% accuracy. In our
discussion, we draw attention to the fact that most LPR models are probably
exploiting such signatures to improve the results achieved in each dataset at
the cost of losing generalization capability. These results emphasize the
importance of evaluating LPR models in cross-dataset setups, as they provide a
better indication of generalization (hence real-world performance) than
within-dataset ones.
",0.2
"Cite_69_1: This article studies experimental design in settings where the experimental units are large aggregate entities (e.g., markets), and only one or a small number of units can be exposed to the treatment. In such settings, randomization of the treatment may result in treated and control groups with very different characteristics at baseline, inducing biases. We propose a variety of experimental non-randomized synthetic control designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) that select the units to be treated, as well as the untreated units to be used as a control group. Average potential outcomes are estimated as weighted averages of the outcomes of treated units for potential outcomes with treatment, and weighted averages the outcomes of control units for potential outcomes without treatment. We analyze the properties of estimators based on synthetic control designs and propose new inferential techniques. We show that in experimental settings with aggregate units, synthetic control designs can substantially reduce estimation biases in comparison to randomization of the treatment.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_69_2: This article investigates the economic consequences of EU membership for the countries that established the EU in 1992. The Synthetic Control method is used in the frame of an algorithmic methodology that aims to guide the choice of donor pool countries and predictors by testing different combinations. The algorithm allows to judge the appropriateness of the research design by computing a set of synthetic countries able to improve the performance of the benchmark case (i.e., a synthetic country computed using an initial large basket of donor pool units and predictors). According to the results, the algorithm has been able to improve the counterfactual scenarios' precision and robustness for all tested countries. It shows that the economic effect of the EU membership has significantly varied among countries. Results suggest that the European integration process has not prevented persistent divergence and heterogeneity of growth paths among member countries.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_69_3: Policy researchers using synthetic control methods typically choose a donor pool in part by using policy domain expertise so the untreated units are most like the treated unit in the pre intervention period. This potentially leaves estimation open to biases, especially when researchers have many potential donors. We compare how functional principal component analysis synthetic control, forward-selection, and the original synthetic control method select donors. To do this, we use Gaussian Process simulations as well as policy case studies from West German Reunification, a hotel moratorium in Barcelona, and a sugar-sweetened beverage tax in San Francisco. We then summarize the implications for policy research and provide avenues for future work.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_69_4: We propose a Synthetic Instrumental Variables (SIV) estimator for panel data that combines the strengths of instrumental variables and synthetic controls to address unmeasured confounding. We derive conditions under which SIV is consistent and asymptotically normal, even when the standard IV estimator is not. Motivated by the finite sample properties of our estimator, we introduce an ensemble estimator that simultaneously addresses multiple sources of bias and provide a permutation-based inference procedure. We demonstrate the effectiveness of our methods through a calibrated simulation exercise, two shift-share empirical applications, and an application in digital economics that includes both observational data and data from a randomized control trial. In our primary empirical application, we examine the impact of the Syrian refugee crisis on Turkish labor markets. Here, the SIV estimator reveals significant effects that the standard IV does not capture. Similarly, in our digital economics application, the SIV estimator successfully recovers the experimental estimates, whereas the standard IV does not.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_69_5: We propose a Synthetic Instrumental Variables (SIV) estimator for panel data that combines the strengths of instrumental variables and synthetic controls to address unmeasured confounding. We derive conditions under which SIV is consistent and asymptotically normal, even when the standard IV estimator is not. Motivated by the finite sample properties of our estimator, we introduce an ensemble estimator that simultaneously addresses multiple sources of bias and provide a permutation-based inference procedure. We demonstrate the effectiveness of our methods through a calibrated simulation exercise, two shift-share empirical applications, and an application in digital economics that includes both observational data and data from a randomized control trial. In our primary empirical application, we examine the impact of the Syrian refugee crisis on Turkish labor markets. Here, the SIV estimator reveals significant effects that the standard IV does not capture. Similarly, in our digital economics application, the SIV estimator successfully recovers the experimental estimates, whereas the standard IV does not.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_69_6: Difference-in-differences designs build counterfactuals by invoking a parallel trend assumption, but this may be violated in the presence of invalid control units. Thus, selecting a control group is vital to ensure proper identification. We introduce fdid based on (Li, Kathleen T. ”A Simple Forward Difference-inDifferences Method.” Marketing Science 43, no. 2 (2024): 267-279). We discuss estimation and inference, document fdid’s syntax, and apply it empirically.","Main_69:   Synthetic control methods often rely on matching pre-treatment
characteristics (called predictors) of the treated unit. The choice of
predictors and how they are weighted plays a key role in the performance and
interpretability of synthetic control estimators. This paper proposes the use
of a sparse synthetic control procedure that penalizes the number of predictors
used in generating the counterfactual to select the most important predictors.
We derive, in a linear factor model framework, a new model selection
consistency result and show that the penalized procedure has a faster mean
squared error convergence rate. Through a simulation study, we then show that
the sparse synthetic control achieves lower bias and has better post-treatment
performance than the un-penalized synthetic control. Finally, we apply the
method to revisit the study of the passage of Proposition 99 in California in
an augmented setting with a large number of predictors available.
",0.3
"Cite_70_1: A standard ML model is commonly generated by a single method that specifies aspects such as architecture, initialization, training data and hyperparameters configuration. The presented work introduces a novel methodology allowing to define multiple methods as distinct agents. Agents can collaborate and compete to generate and improve ML models for a given tasks. The proposed methodology is demonstrated with the generation and extension of a dynamic modular multitask ML system solving more than one hundred image classification tasks. Diverse agents can compete to produce the best performing model for a task by reusing the modules introduced to the system by competing agents. The presented work focuses on the study of agents capable of: 1) reusing the modules generated by concurrent agents, 2) activating in parallel multiple modules in a frozen state by connecting them with trainable modules, 3) condition the activation mixture on each data sample by using a trainable router module. We demonstrate that this simple per-sample parallel routing method can boost the quality of the combined solutions by training a fraction of the activated parameters.","Main_70:   The traditional ML development methodology does not enable a large number of
contributors, each with distinct objectives, to work collectively on the
creation and extension of a shared intelligent system. Enabling such a
collaborative methodology can accelerate the rate of innovation, increase ML
technologies accessibility and enable the emergence of novel capabilities. We
believe that this novel methodology for ML development can be demonstrated
through a modularized representation of ML models and the definition of novel
abstractions allowing to implement and execute diverse methods for the
asynchronous use and extension of modular intelligent systems. We present a
multiagent framework for the collaborative and asynchronous extension of
dynamic large-scale multitask systems.
",0.05
"Cite_71_1: Neutrinos are known to play important roles in many astrophysical scenarios from the early period of the big bang to current stellar evolution being a unique messenger of the fusion reactions occurring in the center of our sun. In particular, neutrinos are crucial in determining the dynamics and the composition evolution in explosive events such as core-collapse supernovae and the merger of two neutron stars. In this paper, we review the current understanding of supernovae and binary neutron star mergers by focusing on the role of neutrinos therein. Several recent improvements on the theoretical modeling of neutrino interaction rates in nuclear matter as well as their impact on the heavy element nucleosynthesis in the supernova neutrino-driven wind are discussed, including the neutrino–nucleon opacity at the mean field level taking into account the relativistic kinematics of nucleons, the effect due to the nucleon–nucleon correlation, and the nucleon–nucleon bremsstrahlung. We also review the framework used to compute the neutrino–nucleus interactions and the up-to-date yield prediction for isotopes from neutrino nucleosynthesis occurring in the outer envelope of the supernova progenitor star during the explosion. Here improved predictions of energy spectra of supernova neutrinos of all flavors have had significant impact on the nucleosynthesis yields. Rapid progresses in modeling the flavor oscillations of neutrinos in these environments, including several novel mechanisms for collective neutrino oscillations and their potential impacts on various nucleosynthesis processes are summarized.","Main_71:   The existence of eV-mass sterile neutrinos is not ruled out because of
persistent experimental anomalies. Upcoming multi-messenger detections of
neutron-star merger remnants could provide indirect constraints on the
existence of these particles. We explore the active-sterile flavor conversion
phenomenology in a two-flavor scenario (1 active + 1 sterile species) as a
function of the sterile neutrino mixing parameters, neutrino emission angle
from the accretion torus, and temporal evolution of the merger remnant. The
torus geometry and the neutron richness of the remnant are responsible for the
occurrence of multiple resonant active-sterile conversions. The number of
resonances strongly depends on the neutrino emission direction above or inside
the remnant torus and leads to large production of sterile neutrinos (and no
antineutrinos) in the proximity of the polar axis as well as more sterile
antineutrinos than neutrinos in the equatorial region. As the black hole torus
evolves in time, the shallower baryon density is responsible for more adiabatic
flavor conversion, leading to larger regions of the mass-mixing parameter space
being affected by flavor mixing. Our findings imply that the production of
sterile states could have indirect implications on the disk cooling rate, its
outflows, and related electromagnetic observables which remain to be assessed.
",0.2
"Cite_71_2: The metastable hypermassive neutron star produced in the coalescence of two neutron stars can copiously produce axions that radiatively decay into 𝒪⁡(100)  MeV photons. These photons can form a fireball with characteristic temperature smaller than 1 MeV. By relying on x-ray observations of GW170817/GRB 170817A with CALET CGBM, Konus-Wind, and Insight-HXMT/HE, we present new bounds on the axion-photon coupling for axion masses in the range 1–400 MeV. We exclude couplings down to 5×10−11  GeV−1, complementing and surpassing existing constraints. Our approach can be extended to any feebly interacting particle decaying into photons.","Main_71:   The existence of eV-mass sterile neutrinos is not ruled out because of
persistent experimental anomalies. Upcoming multi-messenger detections of
neutron-star merger remnants could provide indirect constraints on the
existence of these particles. We explore the active-sterile flavor conversion
phenomenology in a two-flavor scenario (1 active + 1 sterile species) as a
function of the sterile neutrino mixing parameters, neutrino emission angle
from the accretion torus, and temporal evolution of the merger remnant. The
torus geometry and the neutron richness of the remnant are responsible for the
occurrence of multiple resonant active-sterile conversions. The number of
resonances strongly depends on the neutrino emission direction above or inside
the remnant torus and leads to large production of sterile neutrinos (and no
antineutrinos) in the proximity of the polar axis as well as more sterile
antineutrinos than neutrinos in the equatorial region. As the black hole torus
evolves in time, the shallower baryon density is responsible for more adiabatic
flavor conversion, leading to larger regions of the mass-mixing parameter space
being affected by flavor mixing. Our findings imply that the production of
sterile states could have indirect implications on the disk cooling rate, its
outflows, and related electromagnetic observables which remain to be assessed.
",0.2
"Cite_71_3: Neutron star merger remnants are unique sites for exploring neutrino flavor conversion in dense media. Because of the natural excess of ν̅e over νe, the neutrino-neutrino potential can cancel the matter potential, giving rise to matter-neutrino resonant flavor conversion. Under the assumption of two (anti)neutrino flavors and spatial homogeneity, we solve the neutrino quantum kinetic equations to investigate the occurrence of the matter-neutrino resonance within a multi-angle framework. We find that isotropy is broken spontaneously, regardless of the mass ordering. Relying on a hydrodynamical simulation of a binary neutron star merger remnant with a black hole of 3 M⊙ and an accretion torus of 0.3 M⊙, we find that complete flavor conversion caused by the matter-neutrino resonance is unlikely, although the matter and neutrino potentials cancel at various locations above the disk. Importantly, the matter-neutrino resonant flavor conversion crucially depends on the shape of the neutrino angular distributions. Our findings suggest that an accurate modeling of the neutrino angular distributions is necessary to understand flavor conversion physics in merger remnants, its implications on the disk physics and synthesis of the elements heavier than iron.","Main_71:   The existence of eV-mass sterile neutrinos is not ruled out because of
persistent experimental anomalies. Upcoming multi-messenger detections of
neutron-star merger remnants could provide indirect constraints on the
existence of these particles. We explore the active-sterile flavor conversion
phenomenology in a two-flavor scenario (1 active + 1 sterile species) as a
function of the sterile neutrino mixing parameters, neutrino emission angle
from the accretion torus, and temporal evolution of the merger remnant. The
torus geometry and the neutron richness of the remnant are responsible for the
occurrence of multiple resonant active-sterile conversions. The number of
resonances strongly depends on the neutrino emission direction above or inside
the remnant torus and leads to large production of sterile neutrinos (and no
antineutrinos) in the proximity of the polar axis as well as more sterile
antineutrinos than neutrinos in the equatorial region. As the black hole torus
evolves in time, the shallower baryon density is responsible for more adiabatic
flavor conversion, leading to larger regions of the mass-mixing parameter space
being affected by flavor mixing. Our findings imply that the production of
sterile states could have indirect implications on the disk cooling rate, its
outflows, and related electromagnetic observables which remain to be assessed.
",0.2
"Cite_71_4: Light sterile neutrinos, 𝜈𝑠, are often introduced to explain an anomalous deficit in the electron antineutrino flux from nuclear reactors. If they exist, sterile neutrinos would also be produced in collapsing massive stars through the active-sterile neutrino oscillation. In order to investigate the impacts of sterile neutrinos on supernova dynamics, we perform two-dimensional neutrino-radiation hydrodynamic simulations of stellar core-collapse coupled with the active-sterile oscillation through the Mikheyev–Smirnov–Wolfenstein effect. As the initial condition of our simulations, we adopt a blue supergiant model that is tuned to reproduce observational features of the SN 1987A progenitor to compare our models with observations of the event. It is found that the active-sterile oscillation reduces the 𝜈𝑒 and  fluxes and decreases the explosion energy. We also find that, if the mixing angle 𝜃 and the mass difference 𝛿⁢𝑚2 s between 𝜈𝑒 and 𝜈𝑠 are large enough, the star fails to explode. This suggests that these mixing parameters relevant to sterile neutrinos could be constrained by supernova explodability, though other uncertainties in supernova theory need to be addressed to refine them. In addition, we predict neutrino signals from a nearby supernova event and find that the neutrino event number can significantly decrease because the 𝜈𝑒 and fluxes are reduced. In particular, DUNE observations of 𝜈𝑒 will be useful to search for a signature of sterile neutrinos with a tiny mixing angle because a smaller mixing angle leads to a larger effect on the 𝜈𝑒 flux.","Main_71:   The existence of eV-mass sterile neutrinos is not ruled out because of
persistent experimental anomalies. Upcoming multi-messenger detections of
neutron-star merger remnants could provide indirect constraints on the
existence of these particles. We explore the active-sterile flavor conversion
phenomenology in a two-flavor scenario (1 active + 1 sterile species) as a
function of the sterile neutrino mixing parameters, neutrino emission angle
from the accretion torus, and temporal evolution of the merger remnant. The
torus geometry and the neutron richness of the remnant are responsible for the
occurrence of multiple resonant active-sterile conversions. The number of
resonances strongly depends on the neutrino emission direction above or inside
the remnant torus and leads to large production of sterile neutrinos (and no
antineutrinos) in the proximity of the polar axis as well as more sterile
antineutrinos than neutrinos in the equatorial region. As the black hole torus
evolves in time, the shallower baryon density is responsible for more adiabatic
flavor conversion, leading to larger regions of the mass-mixing parameter space
being affected by flavor mixing. Our findings imply that the production of
sterile states could have indirect implications on the disk cooling rate, its
outflows, and related electromagnetic observables which remain to be assessed.
",0.2
"Cite_72_1: The classic paper of Shapley and Shubik [SS71] characterized the core of the assignment game. We observe that a sub-coalition consisting of one player (or a set of players from the same side of the bipartition) can make zero profit, and therefore its profit under a core imputation can be an arbitrary amount. Hence an arbitrary core imputation makes no fairness guarantee at the level of individual agents. Can this deficiency be addressed by picking a “good” core imputation? To arrive at an appropriate solution concept, we give specific criteria for picking a special core imputation, and we undertake a detailed comparison of four solution concepts. Leximin and leximax core imputations come out as clear winners; we define these to be equitable core imputations. These imputations achieve “fairness” in different ways: whereas leximin tries to make poor agents more rich, leximax tries to make rich agents less rich. We give combinatorial strongly polynomial algorithms for computing these imputations via a novel adaptation of the classical primal-dual paradigm. The “engine” driving them involves insights into core imputations obtained via complementarity. It will not be surprising if our work leads to new uses of this powerful technique. Furthermore, we expect more work on computing the leximin and leximax core imputations of other natural games, in addition to the recent follow-up work [GGSV24].","Main_72:   We give new characterizations of core imputations for the following games:
  * The assignment game.
  * Concurrent games, i.e., general graph matching games having non-empty core.
  * The unconstrained bipartite $b$-matching game (edges can be matched
multiple times).
  * The constrained bipartite $b$-matching game (edges can be matched at most
once).
  The classic paper of Shapley and Shubik \cite{Shapley1971assignment} showed
that core imputations of the assignment game are precisely optimal solutions to
the dual of the LP-relaxation of the game. Building on this, Deng et al.
\cite{Deng1999algorithms} gave a general framework which yields analogous
characterizations for several fundamental combinatorial games. Interestingly
enough, their framework does not apply to the last two games stated above. In
turn, we show that some of the core imputations of these games correspond to
optimal dual solutions and others do not. This leads to the tantalizing
question of understanding the origins of the latter. We also present new
characterizations of the profits accrued by agents and teams in core
imputations of the first two games. Our characterization for the first game is
stronger than that for the second; the underlying reason is that the
characterization of vertices of the Birkhoff polytope is stronger than that of
the Balinski polytope.
",0.15
"Cite_72_2: B-Matching is a special case of matching problems where nodes can join multiple matchings with the degree of each node constrained by an upper bound, the node's B-value. The core solution of a bipartite B-matching is both a matching between the nodes respecting the upper bound constraint and an allocation of the weights of the edges among the nodes such that no group of nodes can deviate and collectively gain higher allocation. We present two learning dynamics that converge to the core of the bipartite B-matching problems. The first dynamics are centralized dynamics in the nature of the Hungarian method, which converge to the core in a polynomial time. The second dynamics are distributed dynamics, which converge to the core with probability one. For the distributed dynamics, a node maintains only a state consisting of (i) the aspiration levels for all of its possible matches and (ii) the matches, if any, to which it belongs. The node does not keep track of its history nor is it aware of the environment state. In each stage, a randomly activated node proposes to form a new match and changes its aspiration based on the success or failure of its proposal. At this stage, the proposing node inquires about the aspiration of the node it wants to match with to calculate the feasibility of the match. The environment matching structure changes whenever a proposal succeeds. A state is absorbing for the distributed dynamics if and only if it is in the core of the B-matching.","Main_72:   We give new characterizations of core imputations for the following games:
  * The assignment game.
  * Concurrent games, i.e., general graph matching games having non-empty core.
  * The unconstrained bipartite $b$-matching game (edges can be matched
multiple times).
  * The constrained bipartite $b$-matching game (edges can be matched at most
once).
  The classic paper of Shapley and Shubik \cite{Shapley1971assignment} showed
that core imputations of the assignment game are precisely optimal solutions to
the dual of the LP-relaxation of the game. Building on this, Deng et al.
\cite{Deng1999algorithms} gave a general framework which yields analogous
characterizations for several fundamental combinatorial games. Interestingly
enough, their framework does not apply to the last two games stated above. In
turn, we show that some of the core imputations of these games correspond to
optimal dual solutions and others do not. This leads to the tantalizing
question of understanding the origins of the latter. We also present new
characterizations of the profits accrued by agents and teams in core
imputations of the first two games. Our characterization for the first game is
stronger than that for the second; the underlying reason is that the
characterization of vertices of the Birkhoff polytope is stronger than that of
the Balinski polytope.
",0.15
"Cite_72_3: The core is a dominant solution concept in economics and cooperative game theory; it is used for profit—equivalently, cost or utility—sharing. Starting with the classic work of Shapley and Shubik [25] on the assignment game, the cores of several natural games have been characterized using total unimodularity. The purpose of our paper is two-fold: 1. We give the first game for which total unimodularity does not hold and total dual integrality is needed for characterizing its core. 2. We demonstrate the versatility of the notion of core by proposing a completely different use: in a so-called investment management game, which is a game against nature rather than a cooperative game. Our game has only one agent, whose strategy set is all possible ways of distributing her money among investment firms. The agent wants to pick a strategy such that in each of exponentially many future “scenarios”, sufficient money is available in the “right” firms so she can buy an “optimal investment” for that scenario. Such a strategy constitutes a core imputation under a broad interpretation—though traditional mathematical framework—of the core. Our game is defined on perfect graphs, since the maximum stable set problem (which plays a key role in this game) can be solved in polynomial time for such graphs. We completely characterize the core of this game, analogous to Shapley and Shubik’s characterization of the core of the assignment game. A key difference is that whereas their characterization follows from total unimodularity, ours follows from total dual integrality.","Main_72:   We give new characterizations of core imputations for the following games:
  * The assignment game.
  * Concurrent games, i.e., general graph matching games having non-empty core.
  * The unconstrained bipartite $b$-matching game (edges can be matched
multiple times).
  * The constrained bipartite $b$-matching game (edges can be matched at most
once).
  The classic paper of Shapley and Shubik \cite{Shapley1971assignment} showed
that core imputations of the assignment game are precisely optimal solutions to
the dual of the LP-relaxation of the game. Building on this, Deng et al.
\cite{Deng1999algorithms} gave a general framework which yields analogous
characterizations for several fundamental combinatorial games. Interestingly
enough, their framework does not apply to the last two games stated above. In
turn, we show that some of the core imputations of these games correspond to
optimal dual solutions and others do not. This leads to the tantalizing
question of understanding the origins of the latter. We also present new
characterizations of the profits accrued by agents and teams in core
imputations of the first two games. Our characterization for the first game is
stronger than that for the second; the underlying reason is that the
characterization of vertices of the Birkhoff polytope is stronger than that of
the Balinski polytope.
",0.15
"Cite_73_1: Artificial Intelligence (AI) methods are powerful tools for various domains, including critical fields such as avionics, where certification is required to achieve and maintain an acceptable level of safety. General solutions for safety-critical systems must address three main questions: Is it suitable? What drives the system's decisions? Is it robust to errors/attacks? This is more complex in AI than in traditional methods. In this context, this paper presents a comprehensive mind map of formal AI certification in avionics. It highlights the challenges of certifying AI development with an example to emphasize the need for qualification beyond performance metrics.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_2: Audio-visual speech synthesis (AVSS) has garnered attention in recent years for its utility in the realm of audio-visual learning. AVSS transforms one speaker’s speech into another’s audio-visual stream while retaining linguistic content. This approach extends existing AVSS methods by first modifying vocal features from the source to the target speaker, akin to voice conversion (VC), and then synthesizing the audio-visual stream for the target speaker, termed audio-visual synthesis (AVS). In this work, a novel AVSS approach is proposed using vision transformer (ViT)-based Autoencoders (AEs), enriched with a combination of cycle consistency and reconstruction loss functions, with the aim of enhancing synthesis quality. Leveraging ViT’s attention mechanism, this method effectively captures spectral and temporal features from input speech. The combination of cycle consistency and reconstruction loss improves synthesis quality and aids in preserving essential information. The proposed framework is trained and tested on benchmark datasets, and compared extensively with state-of-the-art (SOTA) methods. The experimental results demonstrate the superiority of the proposed approach over existing SOTA models, in terms of quality and intelligibility for AVSS, indicating the potential for real-world applications.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_3: Deep Metric Learning (DML) loss functions traditionally aim to control the forces of separability and compactness within an embedding space so that the same class data points are pulled together and different class ones are pushed apart. Within the context of DML, a softmax operation will typically normalize distances into a probability for optimization, thus coupling all the push/pull forces together. This paper proposes a potential new class of loss functions that operate within a euclidean domain and aim to take full advantage of the coupled forces governing embedding space formation under a softmax. These forces of compactness and separability can be boosted or mitigated within controlled locations at will by using a warping function. In this work, we provide a simple example of a warping function and use it to achieve competitive, state-of-the-art results on various metric learning benchmarks.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_4: Automatic plant disease detection plays an important role in food security. Deep learning methods are able to detect precisely various types of plant diseases but at the expense of using huge amounts of resources (processors and data). Therefore, employing few-shot or zero-shot learning methods is unavoidable. Deep Metric Learning (DML) is a widely used technique for few/zero shot learning. Existing DML methods extract features from the last hidden layer of a pre-trained deep network, which increases the dependence of the specific features on the observed classes. In this paper, the general discriminative feature learning method is used to learn general features of plant leaves. Moreover, a proxy-based loss is utilized that learns the embedding without sampling phase while having a higher convergence rate. The network is trained on the Plant Village dataset where the images are split into 32 and 6 classes as source and target, respectively. The knowledge learned from the source domain is transferred to the target in a zero-shot setting. A few samples of the target domain are presented to the network as a gallery. The network is then evaluated on the target domain. The experimental results show that by presenting few or even only one sample of new classes to the network without fine-tuning step, our method can achieve a classification accuracy of 99%/80.64% for few/one image(s) per class.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_5: Currently, deep neural networks are widely used for analyzing temporal data. These networks can adapt their architecture to specific needs and deliver good performance. Researchers and developers frequently update their architecture to meet the requirements, but this process can be quite time-consuming. A crucial aspect of DNN architecture is the loss function, which plays a crucial role in calculating gradients. Most research and applications in time series analysis use the mean squared error (MSE) loss function. In this paper, we aim to explore existing loss functions to address the challenge of selecting the appropriate loss function for DNNs. We conduct experiments on time series datasets to evaluate the impact of different loss functions on DNN model performance. Our findings indicate that the Huber loss function outperforms other loss functions in time series analysis. Additionally, we discuss the potential for custom loss functions as future work, beyond the limitations of existing methods.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_6: This paper proposed an ILS (Intermittent Learning Strategy) for sketch-visible person re-identification. The ILS method performs intermittent epochs between self-supervised learning and person re-identification. It can be used as regularization for the training process and reduce the overfitting problem on person re-identification problems by disrupting the learning process for more simple tasks. We use a simple vertical jigsaw pre-task for self-supervised learning by dividing the person's image into several vertically cropped regions. The cropped regions were then labeled and used for self-supervised learning. We perform the experiments using two deep learning architectures: Swin Transformer and Swin Transformer V2 classifiers, along with the PKU-Sketch-ReID and Market-Sketch datasets. Experiments on two different sketch-visible person re-identification datasets show that the proposed ILS method can improve the classifier's performance with rank-1 of 94% along with mAP of 90.12% on PKU-Sketch-ReID and rank- 1 of 33.42% along with mAP of 29.72% on MaSk1K dataset. Further GradCAM analysis shows that the classifiers trained using ILS use more appropriate features compared with non-ILS classifiers.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_7: The global automobile market experiences quick changes in design preferences. In response to the demand shifts, manufacturers now try to apply new technologies to bring a novel design to market faster. In this paper, we introduce a novel application that performs a similarity verification task of wheel designs using an AI model and cloud computing technology. At Jan 2022, we successfully implemented the application to the wheel design process of Hyundai Motor Company’s design team and shortened the similarity verification time by 90% to a maximum of 10 minutes. We believe that this study is the first to build a wheel image database and empirically prove that the cross-entropy loss does similar tasks as the pairwise losses do in the embedding space. As a result, we successfully automated Hyundai Motor’s verification task of wheel design similarity. With a few clicks, the end-users in Hyundai Motor could take advantage of our application.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_8: The global automobile market experiences quick changes in design preferences. In response to the demand shifts, manufacturers now try to apply new technologies to bring a novel design to market faster. In this paper, we introduce a novel AI application that performs a similarity verification task of wheel designs that aims to solve the real-world problem. Through the deep metric learning approach, we empirically prove that the cross-entropy loss does similar tasks as the pairwise losses do in the embedding space. On Jan 2022, we successfully transitioned the verification system to the wheel design process of Hyundai Motor Company's design team and shortened the verification time by 90% to a maximum of 10 min. With a few clicks, the designers at Hyundai Motor could take advantage of our verification system.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_9: Recent studies show that the spatial distribution of the sentence representations generated from pre-trained language models is highly anisotropic. This results in a degradation in the performance of the models on the downstream task. Most methods improve the isotropy of the sentence embeddings by refining the corresponding contextual word representations, then deriving the sentence embeddings from these refined representations. In this study, we propose to improve the quality of the sentence embeddings extracted from the [CLS] token of the pre-trained language models by improving the isotropy of the embeddings. We add one feed-forward layer between the model and the downstream task layers, and we train it using a novel joint loss function. The proposed approach results in embeddings with better isotropy, that generalize better on the downstream task. Experimental results on 3 GLUE datasets with classification as the downstream task show that our proposed method is on par with the state-of-the-art, as it achieves performance gains of around 2–3% on the downstream tasks compared to the baseline.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_73_10: Image classification is a critical area of research in computer vision and deep learning, with applications in fields such as image segmentation and medical tumor detection. Despite significant advancements, challenges remain in improving classification accuracy and generalization. This paper introduces a novel deep metric learning-based image classification method to address these challenges. The model combines a CNN-based feature extraction module with a metric learning module that maps input vectors into a space where inter-class distances are maximized and intra-class distances are minimized. We propose a novel metric loss function that integrates contrast loss and triplet loss to improve accuracy and generalization. An adaptive sampling strategy dynamically selects contrast pairs based on sample difficulty, further boosting performance. Experiments with ResNet50 and GoogLeNet demonstrate the effectiveness of the proposed loss functions and sampling strategies. Additionally, a weight_decay training strategy mitigates overfitting and stabilizes training. Multiple evaluation metrics are used to assess model performance, showing that the proposed method achieves high accuracy and robust generalization.","Main_73: Deep Metric Learning (DML) learns a non-linear semantic embedding from input
data that brings similar pairs together while keeping dissimilar data away from
each other. To this end, many different methods are proposed in the last decade
with promising results in various applications. The success of a DML algorithm
greatly depends on its loss function. However, no loss function is perfect, and
it deals only with some aspects of an optimal similarity embedding. Besides,
the generalizability of the DML on unseen categories during the test stage is
an important matter that is not considered by existing loss functions. To
address these challenges, we propose novel approaches to combine different
losses built on top of a shared deep feature extractor. The proposed ensemble
of losses enforces the deep model to extract features that are consistent with
all losses. Since the selected losses are diverse and each emphasizes different
aspects of an optimal semantic embedding, our effective combining methods yield
a considerable improvement over any individual loss and generalize well on
unseen categories. Here, there is no limitation in choosing loss functions, and
our methods can work with any set of existing ones. Besides, they can optimize
each loss function as well as its weight in an end-to-end paradigm with no need
to adjust any hyper-parameter. We evaluate our methods on some popular datasets
from the machine vision domain in conventional Zero-Shot-Learning (ZSL)
settings. The results are very encouraging and show that our methods outperform
all baseline losses by a large margin in all datasets.
",0.5
"Cite_74_1: The power spectra estimated from the brain recordings are the mixed representation of aperiodic transient activity and periodic oscillations, i.e., aperiodic component (AC) and periodic component (PC). Quantitative neurophysiology requires precise decomposition preceding parameterizing each component. However, the shape, statistical distribution, scale, and mixing mechanism of AC and PCs are unclear, challenging the effectiveness of current popular parametric models such as FOOOF, IRASA, BOSC, etc. Here, ξ-π was proposed to decompose the neural spectra by embedding the nonparametric spectra estimation with penalized Whittle likelihood and the shape language modeling into the expectation maximization framework. ξ-π was validated on the synthesized spectra with loss statistics and on the sleep EEG and the large sample iEEG with evaluation metrics and neurophysiological evidence. Compared to FOOOF, both the simulation presenting shape irregularities and the batch simulation with multiple isolated peaks indicated that ξ-π improved the fit of AC and PCs with less loss and higher F1-score in recognizing the centering frequencies and the number of peaks; the sleep EEG revealed that ξ-π produced more distinguishable AC exponents and improved the sleep state classification accuracy; the iEEG showed that ξ-π approached the clinical findings in peak discovery. Overall, ξ-π offered good performance in the spectra decomposition, which allows flexible parameterization using descriptive statistics or kernel functions. ξ-π is a seminal tool for brain signal decoding in fields such as cognitive neuroscience, brain-computer interface, neurofeedback, and brain diseases.","Main_74:   Power spectra of awake resting state EEG recordings in humans typically have
an Alpha peak at around 10 Hz riding a decreasing background ""Xi process"".
Normal and pathological variations may have more than one peak or none. The
single channel Xi-Alpha model (Pascual-Marqui et al 1988,
https://doi.org/10.3109/00207458808985730) separated these two processes,
providing a low-dimensional parametric description of EEG spectra. Currently
lacking is a generative whole cortex model for activity spectra and
intracortical functional connectivity. Here we introduce the ""cortical Xi-Alpha
model"". The cross-spectral density matrices are modeled as additive components,
each one consisting of a scalar spectrum that multiplies a frequency invariant
Hermitian covariance matrix. This simple ""separation of variables"" form offers
a very rich repertoire of spatio-spectral properties, as well as diverse whole
cortex functional connectivity patterns. The scalp EEG model conserves the same
form, allowing simple estimation from scalp to cortex. Two independent
open-access, resting state eyes open and closed EEG data sets (203 participants
with 61 electrodes, and 47 participants with 26 electrodes) were used to
demonstrate, test, and validate the model. Results summary: - The average
dimension of cross-spectra lies between 1.7 and 2.6, indicating two processes.
- Non-negative matrix factorization of population power spectra sampled at 6239
cortical voxels with only two components (identified as Xi and Alpha) explains
99% of the variance. - The median value of explained variance was 95% for the
""cortical Xi-Alpha model"" across all datasets and conditions. - Alpha process
generators more occipital, Xi more frontal. - Xi cortical lagged connectivities
are isotropic with interdistance. - Laminar recordings suggest layers 2/3
pyramidal neurons generate Xi; layers 5/6 pyramidal neurons generate Alpha.
",0.1
"Cite_74_2: We formulate a new class of parametric, multivariate, and structurally informed spectral components model of the EEG, the ξ − α NET, that allows us to map the Lifespan of EEG source spectral dynamics across a large data set of EEG cross spectrum at high spatial resolution. This approach accurately estimates source spectral components and effective connectivity through the use of biophysical modeling while maintaining computational efficiency, as confirmed by simulation benchmarks. We are able to analyze source dynamics with a resolution of 8,003 voxels from the HarMNqEEG dataset, which includes scalp EEG cross-spectrum tensors collected from 1965 subjects across 9 countries, using various devices and accounting for different age groups. Our findings indicate that the Bayesian Model Inversion of the ξ − α NET allows to map Lifespan of conduction delays that follows a U-shaped trajectory, which contrasts with independently recorded myelin concentration measurements. Moreover, we assess the spatiotemporal distribution of spectral components, revealing that the aperiodic or fractal component has an isotropic spatial distribution on the cortical surface. While the generator’s spectral peak in the α band, i.e., α-rythms, is localized on the visual areas of the brain. Using a Zero Inflated Gaussian model, our findings indicate that the mode frequency that characterizes the α-rythms or Peak Alpha Frequency shows an inverted U-shaped trajectory for both hemispheres across the Lifespan and a spatial gradient of zero inflation in PAF across the cortex that flattens the trajectory from posterior to frontal areas. We provide both the code of the ξ − α NET and the source solution of the spectral dynamics for the HarMNqEEG.","Main_74:   Power spectra of awake resting state EEG recordings in humans typically have
an Alpha peak at around 10 Hz riding a decreasing background ""Xi process"".
Normal and pathological variations may have more than one peak or none. The
single channel Xi-Alpha model (Pascual-Marqui et al 1988,
https://doi.org/10.3109/00207458808985730) separated these two processes,
providing a low-dimensional parametric description of EEG spectra. Currently
lacking is a generative whole cortex model for activity spectra and
intracortical functional connectivity. Here we introduce the ""cortical Xi-Alpha
model"". The cross-spectral density matrices are modeled as additive components,
each one consisting of a scalar spectrum that multiplies a frequency invariant
Hermitian covariance matrix. This simple ""separation of variables"" form offers
a very rich repertoire of spatio-spectral properties, as well as diverse whole
cortex functional connectivity patterns. The scalp EEG model conserves the same
form, allowing simple estimation from scalp to cortex. Two independent
open-access, resting state eyes open and closed EEG data sets (203 participants
with 61 electrodes, and 47 participants with 26 electrodes) were used to
demonstrate, test, and validate the model. Results summary: - The average
dimension of cross-spectra lies between 1.7 and 2.6, indicating two processes.
- Non-negative matrix factorization of population power spectra sampled at 6239
cortical voxels with only two components (identified as Xi and Alpha) explains
99% of the variance. - The median value of explained variance was 95% for the
""cortical Xi-Alpha model"" across all datasets and conditions. - Alpha process
generators more occipital, Xi more frontal. - Xi cortical lagged connectivities
are isotropic with interdistance. - Laminar recordings suggest layers 2/3
pyramidal neurons generate Xi; layers 5/6 pyramidal neurons generate Alpha.
",0.1
"Cite_75_1: Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.","Main_75: Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the
Min Cut cost function by a cluster dependent factor (e.g., the size or the
degree of the clusters), in order to yield a more balanced partitioning. We,
instead, investigate adding such regularizations to the original cost function.
We first consider the case where the regularization term is the sum of the
squared size of the clusters, and then generalize it to adaptive regularization
of the pairwise similarities. This leads to shifting (adaptively) the pairwise
similarities which might make some of them negative. We then study the
connection of this method to Correlation Clustering and then propose an
efficient local search optimization algorithm with fast theoretical convergence
rate to solve the new clustering problem. In the following, we investigate the
shift of pairwise similarities on some common clustering methods, and finally,
we demonstrate the superior performance of the method by extensive experiments
on different datasets.
",0.2
Cite_75_2: We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then in the following we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose we first investigate embedding the respective hierarchy to be used for tree preserving embedding and feature extraction. Thereafter we study the extension of minimax distance measures to correlation clustering as another representation learning paradigm. Finally we demonstrate the performance of our methods on several datasets.,"Main_75: Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the
Min Cut cost function by a cluster dependent factor (e.g., the size or the
degree of the clusters), in order to yield a more balanced partitioning. We,
instead, investigate adding such regularizations to the original cost function.
We first consider the case where the regularization term is the sum of the
squared size of the clusters, and then generalize it to adaptive regularization
of the pairwise similarities. This leads to shifting (adaptively) the pairwise
similarities which might make some of them negative. We then study the
connection of this method to Correlation Clustering and then propose an
efficient local search optimization algorithm with fast theoretical convergence
rate to solve the new clustering problem. In the following, we investigate the
shift of pairwise similarities on some common clustering methods, and finally,
we demonstrate the superior performance of the method by extensive experiments
on different datasets.
",0.2
"Cite_75_3: We study correlation clustering where the pairwise similarities are not known in advance. For this purpose, we employ active learning to query pairwise similarities in a cost-efficient way. We propose a number of effective information-theoretic acquisition functions based on entropy and information gain. We extensively investigate the performance of our methods in different settings and demonstrate their superior performance compared to the alternatives.","Main_75: Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the
Min Cut cost function by a cluster dependent factor (e.g., the size or the
degree of the clusters), in order to yield a more balanced partitioning. We,
instead, investigate adding such regularizations to the original cost function.
We first consider the case where the regularization term is the sum of the
squared size of the clusters, and then generalize it to adaptive regularization
of the pairwise similarities. This leads to shifting (adaptively) the pairwise
similarities which might make some of them negative. We then study the
connection of this method to Correlation Clustering and then propose an
efficient local search optimization algorithm with fast theoretical convergence
rate to solve the new clustering problem. In the following, we investigate the
shift of pairwise similarities on some common clustering methods, and finally,
we demonstrate the superior performance of the method by extensive experiments
on different datasets.
",0.2
"Cite_75_4: Signed networks, where edges are labeled as positive or negative to represent friendly or antagonistic interactions, offer a natural framework for analyzing polarization, trust, and conflict in social systems. Detecting meaningful group structures in such networks is crucial for understanding online discourse, political divisions, and trust dynamics. A key challenge is to identify communities that are internally cohesive and externally antagonistic, while allowing for neutral or unaligned vertices. In this paper, we propose a method for identifying k polarized communities that addresses a major limitation of prior methods: their tendency to produce highly size-imbalanced solutions. We introduce a novel optimization objective that avoids such imbalance. In addition, it is well known that approximation algorithms based on local search are highly effective for clustering signed networks when neutral vertices are not allowed. We build on this idea and design the first local search algorithm that extends to the setting with neutral vertices while scaling to large networks. By connecting our approach to block-coordinate Frank-Wolfe optimization, we prove a linear convergence rate, enabled by the structure of our objective. Experiments on real-world and synthetic datasets demonstrate that our method consistently outperforms state-of-the-art baselines in solution quality, while remaining competitive in computational efficiency.","Main_75: Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the
Min Cut cost function by a cluster dependent factor (e.g., the size or the
degree of the clusters), in order to yield a more balanced partitioning. We,
instead, investigate adding such regularizations to the original cost function.
We first consider the case where the regularization term is the sum of the
squared size of the clusters, and then generalize it to adaptive regularization
of the pairwise similarities. This leads to shifting (adaptively) the pairwise
similarities which might make some of them negative. We then study the
connection of this method to Correlation Clustering and then propose an
efficient local search optimization algorithm with fast theoretical convergence
rate to solve the new clustering problem. In the following, we investigate the
shift of pairwise similarities on some common clustering methods, and finally,
we demonstrate the superior performance of the method by extensive experiments
on different datasets.
",0.2
"Cite_76_1: We propose that the recently observed quasi-periodic eruptions (QPEs) in galactic nuclei are produced by unstable mass transfer due to Roche lobe overflow of a low-mass main-sequence star in a mildly eccentric (e ∼ 0.5) orbit. We argue that the QPE emission is powered by circularization shocks, but not directly by black hole (BH) accretion. Our model predicts the presence of a time-steady accretion disc that is bolometrically brighter than the time-averaged QPE luminosity, but primarily emits in the extreme-ultraviolet. This is consistent with the quiescent soft X-ray emission detected in between the eruptions in eROSITA QPE1, QPE2, and GSN 069. Such accretion discs have an unusual νLν ∝ ν12/7 optical spectrum. The lifetime of the bright QPE phase, 102–103 yr, is set by mass-loss triggered by ram-pressure interaction between the star and the accretion disc fed by the star itself. We show that the stellar orbits needed to explain QPEs can be efficiently created by the Hills breakup of tight stellar binaries provided that (i) the stellar binary orbit is tidally hardened before the breakup due to diffusive growth of the f-mode amplitude and (ii) the captured star’s orbit decays by gravitational wave emission without significant orbital angular momentum diffusion (which is the case for low-mass BHs, MBH ≲ 106 M⊙). We conclude by discussing the implications of our model for hyper-velocity stars, extreme mass ratio inspirals, repeating partial TDEs, and related stellar phenomena in galactic nuclei.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_2: Type Ia and other peculiar supernovae (SNe) are thought to originate from the thermonuclear explosions of white dwarfs (WDs). Some of the proposed channels involve the ejection of a partly exploded WD (e.g. Iax SN remnant) or the companion of an exploding WD at extremely high velocities (>400 km s−1). Characterization of such hyper-runaway/hypervelocity (HVS) WDs might therefore shed light on the physics and origins of SNe. Here we analyse the Gaia DR3 data to search for HVS WDs candidates and peculiar sub-main-sequence (sub-MS) objects. We retrieve the previously identified HVSs and find 46 new HVS candidates. Among these we identify two new unbound WDs and two new unbound sub-MS candidates. The remaining stars are hyper-runaway WDs and hyper-runaway sub-MS stars. The numbers and properties of the HVS WD and sub-MS candidates suggest that extreme velocity ejections (>1000 km s−1) can accompany at most a small fraction of type Ia SNe, disfavouring a significant contribution of the D6-scenario to the origin of Ia SNe. The rate of HVS ejections following the hybrid WD reverse-detonation channel could be consistent with the identified HVSs. The numbers of lower-velocity HVS WDs could be consistent with type Iax SNe origin and/or contribution from dynamical encounters. We also searched for HVS WDs related to known SN remnants but identified only one such candidate.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_3: Hypervelocity stars (HVSs) are stars which have been ejected from the Galactic Centre (GC) at velocities of up to a few thousand ⁠. They are tracers of the Galactic potential and can be used to infer properties of the GC, such as the initial mass function and assembly history. HVSs are rare, however, with only about a dozen promising candidates discovered so far. In this work, we make use of a novel, highly efficient method to identify new HVS candidates in Gaia. This method uses the nearly radial trajectories of HVSs to infer their distances and velocities based on their position and Gaia proper motion alone. Through comparison of inferred distances with Gaia parallaxes and photometry, we identified 600 HVS candidates with G < 20 including the previously discovered S5-HVS1, out of which we obtained ground-based follow-up observations for 196 stars. As we found no new HVSs based on their radial velocity, we used detailed HVS ejection simulations to significantly improve previous HVS ejection rate constraints. In particular, the ejection rate of HVSs more massive than 1  cannot be higher than  significance. Additionally, we predict that there are 5–45 unbound HVSs in the complete Gaia catalogue (⁠ interval), most of which will be main-sequence stars of a few Mat heliocentric distances of tens to hundreds of kpc. By comparing our results to literature HVS candidates, we find an indication of either a time-dependent ejection rate of HVSs or a non-GC origin of previously identified HVS candidates.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_4: The dense environments in the cores of globular clusters (GCs) facilitate many strong dynamical encounters among stellar objects. These encounters have been shown to be capable of ejecting stars from the host GC, whereupon they become runaway stars, or hypervelocity stars (HVSs) if unbound to the galactic potential. We study high-speed stellar ejecta originating from GCs by using Monte Carlo N-body models, in particular focusing on binary–single encounters involving compact objects. We pair our model-discriminated populations with observational catalogs of Milky Way GCs (MWGCs) to compose a present-day Galactic population of stellar ejecta. We find that these kinds of encounters can accelerate stars to velocities in excess of 2000 km s−1, to speeds beyond the previously predicted limits for ejecta from star-only encounters and in the same regime of Galactic center ejections. However, the same ejections can only account for 1.5%–20% of the total population of stellar runaways, and only 0.0001%–1% of HVS, with similar relative rates found for runaway white dwarfs. We also provide credible regions for ejecta from 149 MWGCs, which we hope will be useful as supplementary evidence when pairing runaway stars with origin GCs.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_5: We assemble a large sample of 12,784 high-velocity stars with total velocity VGSR ≥ 300 km s−1, selected from RAVE DR5, SDSS DR12, LAMOST DR8, APOGEE DR16, GALAH DR2, and Gaia EDR3. In this sample, 52 are marginally hypervelocity star (HVS) candidates that have VGSR exceeding their local escape velocities within 2σ confidence levels, 40 of which are discovered for the first time. All of the candidates are metal-poor, late-type halo stars, which are significantly different from the previously identified HVSs, which are largely massive early-type stars, discovered by extreme radial velocity. This finding suggests that our newly identified HVS candidates are ejected by different mechanisms from the previous population. To investigate their origins, for 547 extreme velocity stars with VGSR ≥ 0.8Vesc, we reconstruct their backward-integrated trajectories in the Galactic potential. According to the orbital analysis, no candidates are found to be definitely ejected from the Galactic-center (GC), while eight metal-poor extreme velocity stars are found to have a closest distance to the GC within 1 kpc. Intriguingly, 15 extreme velocity stars (including 2 HVS candidates) are found to have experienced close encounters with the Sagittarius dSph, suggesting that they originated from this dSph. This hypothesis is supported by an analysis of the [α/Fe]–[Fe/H] diagram. From a preliminary analysis of all of the 547 extreme velocity stars, we propose a general picture–star ejection from Galactic subsystems such as dwarf galaxies and globular clusters can be an important channel to produce extreme velocity stars or even HVSs, particularly the metal-poor late-type halo population.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_6: Type Ia supernovae (SNe Ia) are securely understood to come from the thermonuclear explosion of a white dwarf as a result of binary interaction, but the nature of that binary interaction and the secondary object is uncertain. Recently, a double white dwarf model known as the dynamically driven double-degenerate double-detonation (D6) model has become a promising explanation for these events. One realization of this scenario predicts that the companion may survive the explosion and reside within the remnant as a fast moving (Vpeculiar > 1000 km s−1), overluminous (L > 0.1 L⊙) white dwarf. Recently, three objects that appear to have these unusual properties have been discovered in the Gaia survey. We obtained photometric observations of the SN Ia remnant SN 1006 with the Dark Energy Camera over four years to attempt to discover a similar star. We present a deep, high-precision astrometric proper-motion survey of the interior stellar population of the remnant. We rule out the existence of a high-proper-motion object consistent with our tested realization of the D6 scenario (Vtransverse > 600 km s−1 with mr < 21 corresponding to an intrinsic luminosity of L > 0.0176 L⊙). We conclude that such a star does not exist within the remnant or is hidden from detection by either strong localized dust or the unlikely possibility of ejection from the binary system almost parallel to the line of sight.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_7: We search for high-velocity stars in the inner region of the Galactic bulge using a selected sample of red clump stars. Some of those stars might be considered hypervelocity stars (HVSs). Even though the HVSs ejection relies on an interaction with the supermassive black hole (SMBH) at the centre of the Galaxy, there are no confirmed detections of HVSs in the inner region of our Galaxy. With the detection of HVSs, ejection mechanism models can be constrained by exploring the stellar dynamics in the Galactic centre through a recent stellar interaction with the SMBH. Based on a previously developed methodology by our group, we searched with a sample of preliminary data from version 2 of the Vista Variables in the Via Lactea (VVV) Infrared Astrometric Catalogue (VIRAC2) and Gaia DR3 data, including accurate optical and near-infrared proper motions. This search resulted in a sample of 46 stars with transverse velocities larger than the local escape velocity within the Galactic bulge, of which four are prime candidate HVSs with high-proper motions consistent with being ejections from the Galactic centre. Adding to that, we studied a sample of reddened stars without a Gaia DR3 counterpart and found 481 stars with transverse velocities larger than the local escape velocity, from which 65 stars have proper motions pointing out of the Galactic centre and are candidate HVSs. In total, we found 69 candidate HVSs pointing away from the Galactic centre with transverse velocities larger than the local escape velocity.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_8: Dense stellar clusters surround the supermassive black holes (SMBH) in galactic nuclei. Interactions within the cluster can alter the stellar orbits, occasionally driving a star into the SMBH’s tidal radius, where it becomes ruptured, or expelling a star from the nuclear cluster. This proof-of-concept study examines the orbital effects of stellar collisions using a semianalytic model. Both low- and high-speed collisions occur in the SMBH’s sphere of influence. We find that collisions can place stars on nearly radial orbits. Depositing stars within the tidal radius, collisions may drive the disruption of stars with unusual masses and structures: depending on the nature of the collision, the star could be the product of a recent merger, or it could have lost its outer layers in a previous high-speed impact, appearing as a stripped star. We also find that high-speed collisions near the periapsis of an eccentric orbit can unbind stars from the SMBH. However, dissipation during these high-speed collisions can substantially reduce the number of unbound stars achieved in our simulations. We conclude that tidal disruption events and ejected stars, even in the hypervelocity regime, are plausible outcomes of stellar collisions, though their frequency in a three-dimensional nuclear star cluster is uncertain. Future work will address the rates and properties of these events.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_9: When a binary of early-type stars from the young stellar populations in the Galactic center (GC) region is scattered to the vicinity of the supermassive black hole (SMBH) Sgr A*, one of the components would be tidally ejected as an early-type hypervelocity star (HVS) and the counterpart would be captured on a tight orbit around Sgr A*. Dozens of B-type HVSs moving faster than the Galactic escape speed have been discovered in the Galactic halo and are produced most likely by the SMBH Sgr A*. However, the velocity distribution and in particular the deficit of the HVSs above 700 km s−1 is seriously inconsistent with the expectations of the present models. Here we show that the high-velocity deficit is due to the deficiency in close interactions of stars with the SMBH Sgr A*, because an orbiting intermediate-mass black hole (IMBH) of about 15,000 Solar mass kicked away slowly approaching stars 50–250 million years ago. The SMBH–IMBH binary formed probably after the merger of the Milky Way with the Gaia-Sausage-Enceladus dwarf galaxy, and coalesced about 10 million years ago, leading to a gravitational recoil of Sgr A* at a velocity of 0.3–0.5 km s−1 and to a change of the HVS ejection scenarios. The SMBH–IMBH binary scenario predicts the formation of the S-star cluster at the GC with the distribution of the orbital size and stellar ages that are well consistent with the observations.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_10: Once per 10,000-100,000 years, an unlucky star may experience a close encounter with a supermassive black hole (SMBH), partially or fully tearing apart the star in an exceedingly brief, bright interaction called a tidal disruption event (TDE). Remnants of partial TDEs are expected to be plentiful in our Galactic Center, where at least six unexplained, diffuse, star-like 'G objects' have already been detected which may have formed via interactions between stars and the SMBH. Using numerical simulations, this work aims to identify the characteristics of TDE remnants. We take 3D hydrodynamic FLASH models of partially disrupted stars and map them into the 1D stellar evolution code MESA to examine the properties of these remnants from tens to billions of years after the TDE. The remnants initially exhibit a brief, highly luminous phase, followed by an extended cooling period as they return to stable hydrogen burning. During the initial stage (< 10,000 yr) their luminosities increase by orders of magnitude, making them intriguing candidates to explain a fraction of the mysterious G objects. Notably, mild TDEs are the most common and result in the brightest remnants during this initial phase. However, most remnants exist in a long-lived stage where they are only modestly offset in temperature and luminosity compared to main-sequence stars of equivalent mass. Nonetheless, our results indicate remnants will sustain abnormal, metal-enriched envelopes that may be discernible through spectroscopic analysis. Identifying TDE survivors within the Milky Way could further illuminate some of the most gravitationally intense encounters in the Universe.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_11: A common origin for a host of stellar phenomena in galactic centres is the tidal encounter between stellar binaries and a massive black hole (MBH), known as the ``Hills mechanism''. Following the encounter, binaries may disrupt into an ejected star and a captured one, they may merge, or survive to either fly away or come back for one or more subsequent encounters, until they are either disrupted or fly away. In this paper, we analyse how a binary's fate depends on its orbital parameters, by following its evolution through up to three subsequent pericentre passages. We choose an initial population of circular binaries on parabolic orbits. We present results from our restricted three-body formalism, whose strength lies in the ability to easily explore a multidimensional parameter space and make predictions independent of the binary physical properties. We find that fates depend strongly on orbital inclination, how deep the encounter is into the MBH tidal sphere and on the binary eccentricity, developed during encounters. Generally, non retrograde trajectories, high eccentricities or deep encounters produce disruptions preferentially. Disruption is the most common fate. A significant fraction of the surviving binaries fly away at velocities typically two orders of magnitude smaller than those of ejected stars. Multiple encounters boost disruptions by 20% or more. Finally, using an example system, we investigate the effect of finite stellar sizes and lifetimes, showing that mergers occur 31% of the time, and that disruptions are still boosted by 10% through subsequent passages.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_76_12: Type Ia and other peculiar supernovae (SNe) are thought to originate from the thermonuclear explosions of white dwarfs (WDs). Some of the proposed channels involve the ejection of a partly exploded WD (e.g. Iax SN remnant) or the companion of an exploding WD at extremely high velocities (>400 km,s^{-1}). Characterisation of such hyper-runaway/hypervelocity (HVS) WDs might therefore shed light on the physics and origins of SNe. Here we analyse the textit{Gaia} DR3 data to search for HVS WDs candidates, and peculiar sub-main-sequence (sub-MS) objects. We retrieve the previously identified HVSs, and find 46 new HVS candidates. Among these we identify two new unbound WDs and two new unbound sub-MS candidates. The remaining stars are hyper-runaway WDs and hyper-runaway sub-MS stars. The numbers and properties of the HVS WD and sub-MS candidates suggest that extreme velocity ejections (>1000 km,s^{-1}) can accompany at most a small fraction of type Ia SNe, disfavouring a significant contribution of the D6-scenario to the origin of Ia SNe. The rate of HVS ejections following the hybrid WD reverse-detonation channel could be consistent with the identified HVSs. The numbers of lower-velocity HVS WDs could be consistent with type Iax SNe origin and or contribution from dynamical encounters. We also searched for HVS WDs related to known SN remnants, but identified only one such candidate.","Main_76:   In recent years surveys have identified several dozen B stars in the Milky
Way halo moving faster than the local escape speed. The origin of most of these
hypervelocity stars (HVSs) is still poorly constrained. Here we show that the
velocity distribution, and in particular the deficiency in >700 km/s HVSs is
inconsistent with binary disruptions by the massive black hole (MBH) in the
Galactic Centre. This conclusion holds in the full and empty loss cone regime,
and for secular instabilities in eccentric disks. Accounting for multiple close
encounters between binaries and the MBH, does not qualitatively change the
results. Moreover, there is no observed counterpart population in the Galactic
Centre that is consistent with the HVSs. The star-formation history could be
tuned explain the HVS velocity distribution, but this tuning would produce a
mismatch with the observed HVS flight times. Frequent stellar collisions of the
binary components due to interactions with the MBH do not significantly impact
the velocity distribution in the Galactic halo. Such collisions, however, can
leave observable remnants in the Galactic Centre, and potentially explain the
origins of G2-like dust clouds.
",0.6
"Cite_77_1: In Evans and Francis (2022) and Hendel (2021) the authors investigated resistance distance in triangular grid graphs and observed several types of asymptotic behavior. This paper extends their work by studying the initial, non-asymptotic, behavior found when equivalent circuit transformations are performed, reducing the rows in the triangular grid graph one row at a time. The main conjecture characterizes, after reducing an arbitrary number of times an initial triangular grid all of whose edge resistances are identically one, when edge resistance values are less than, equal to, or greater than one. A special case of the conjecture is proven. The main theorem identifies patterns of repeating edge resistances arising in diagonals of a triangular grid reduced times provided the original grid has at least rows of triangles. This paper also improves upon the notation, concepts, and proof techniques introduced by the authors previously.","Main_77:   The literature presents several papers studying graphs whose edge-labels
correspond to electric resistances. A variety of combinatoric and computational
methods exist by which equivalent total resistance between two vertices in
these graphs may be computed. This paper studies the triangular grid $T(n)$
consisting of $n$ rows and $n^2$ cells (and hence $\frac{(n+1)(n+2)}{2}$
vertices and $\frac{3n(n+1)}{2}$ edges). The graph Laplacian is frequently used
to compute total resistances. This paper uses an alternative computational
method, a direct application of the four electric-circuit operations -- the
series, parallel, $\Delta-Y$, and $Y-\Delta$ transformations -- frequently used
to reduce circuits. This alternative method provides many intermediate
calculations; the additional numerical data in turn allows the discovery of
many interesting patterns several of which are explored in this paper. In
particular, as the number of rows in the triangular grid goes to infinity,
certain unexpected limiting behavior emerges, including for example the
mysterious appearance of $e$ the base of the natural logarithms. The paper
proves some theorems consistent with these patterns, provides additional
methods of studying these triangular grid graphs, and proposes a
straightforward proof method.
",0.1
"Cite_77_2: This paper extends our previous result on the circuit array, a two-dimensional array associated with the resistances in circuits whose underlying graph, when embedded in the Cartesian plane, has the form of a triangular grid. This paper extends the results of the prior paper by considering the circuit array in terms of polynomials instead of numbers as a means to facilitate finding patterns. The main conjecture of this paper states that the characteristic polynomials corresponding to the recursions of single or multivariable polynomial formulations of the circuit array exclusively have powers of 9 as roots. Several initial cases and one major sub-case are proven.","Main_77:   The literature presents several papers studying graphs whose edge-labels
correspond to electric resistances. A variety of combinatoric and computational
methods exist by which equivalent total resistance between two vertices in
these graphs may be computed. This paper studies the triangular grid $T(n)$
consisting of $n$ rows and $n^2$ cells (and hence $\frac{(n+1)(n+2)}{2}$
vertices and $\frac{3n(n+1)}{2}$ edges). The graph Laplacian is frequently used
to compute total resistances. This paper uses an alternative computational
method, a direct application of the four electric-circuit operations -- the
series, parallel, $\Delta-Y$, and $Y-\Delta$ transformations -- frequently used
to reduce circuits. This alternative method provides many intermediate
calculations; the additional numerical data in turn allows the discovery of
many interesting patterns several of which are explored in this paper. In
particular, as the number of rows in the triangular grid goes to infinity,
certain unexpected limiting behavior emerges, including for example the
mysterious appearance of $e$ the base of the natural logarithms. The paper
proves some theorems consistent with these patterns, provides additional
methods of studying these triangular grid graphs, and proposes a
straightforward proof method.
",0.1
"Cite_78_1: Understanding nonequilibrium dynamics of strongly interacting quantum systems represents one of the most challenging problems in many-body physics. Here, quantum thermalization dynamics are explored in real-time in an ultracold Fermi gas suddenly quenched to the BEC-BCS crossover. When quenched to unitarity, it is observed that the cloud size remains unchanged in the early evolution while the momentum distribution emerges two prethermal states with a lifetime difference of two orders of magnitude in the early and intermediate stage before very slowly evolving to the final stationary state. It is revealed that a crossover momentum, at which the momentum distribution remains nearly unchanged, is determined by the thermal wavelength at high temperatures and the Fermi momentum while at low temperatures. It is identified that the universal prethermal dynamics scaling where momentum distributions with different temperatures collapse onto one curve. When quenched to the BEC side, the thermalization rapidly relaxes into a prethermal state and exhibits the low energy oscillation related to the molecular bound states. This work provides benchmarks for the study of quantum thermalization in strongly interacting fermionic many-body systems.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_2: Prethermal discrete time crystals (PDTCs), an emergent non-equilibrium phase of matter, have been studied in two- and higher-dimensional lattices with nearest-neighbor (NN) interactions and one-dimensional (1D) lattices with long-range interactions. However, different from prethermalization that can be observed in 1D Floquet classical spin systems with NN interactions, classical PDTCs in Floquet 1D systems with only NN interactions have not been proposed before. Here, we demonstrate the emergence of disorder-free PDTCs in 1D Floquet classic spin systems with NN interactions. We show that the thermalization time grows exponentially as the driving frequency increases and depends on the energy density of the initial state, which are two key signatures of PDTCs. The robustness of 1D classical PDTC order is verified by introducing imperfect spin flip operations. The emergence of 1D classical PDTCs can be explained by mapping the 1D classical spin chain with NN interacting to a quantum spin-half system with effective long-range interactions. Our work provides an exploration of quantum characteristics, when considering the classical counterparts of quantum phenomena, and will be helpful for further investigations of both classical and quantum prethermal systems and discrete time-crystalline order.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_3: The last two decades has seen quantum thermodynamics become a well established field of research in its own right. In that time, it has demonstrated a remarkably broad applicability, ranging from providing foundational advances in the understanding of how thermodynamic principles apply at the nano-scale and in the presence of quantum coherence, to providing a guiding framework for the development of efficient quantum devices. Exquisite levels of control have allowed state-of-the-art experimental platforms to explore energetics and thermodynamics at the smallest scales which has in turn helped to drive theoretical advances. This Roadmap provides an overview of the recent developments across many of the field's sub-disciplines, assessing the key challenges and future prospects, providing a guide for its near term progress.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_4: Time-dependent drives hold the promise of realizing non-equilibrium many-body phenomena that are absent in undriven systems. Yet, drive-induced heating normally destabilizes the systems, which can be parametrically suppressed in the high-frequency regime by using periodic (Floquet) drives. It remains largely unknown to what extent highly controllable quantum simulators can suppress heating in non-periodically driven systems. Using the 78-qubit superconducting quantum processor, Chuang-tzu 2.0, we report the experimental observation of long-lived prethermal phases in many-body systems with tunable heating rates, driven by structured random protocols, characterized by n-multipolar temporal correlations. By measuring both the particle imbalance and subsystem entanglement entropy, we monitor the entire heating process over 1,000 driving cycles and observe the existence of the prethermal plateau. The prethermal lifetime is `doubly tunable': one way by driving frequency, the other by multipolar order; it grows algebraically with the frequency with the universal scaling exponent 2n{+}1. Using quantum state tomography on different subsystems, we demonstrate a non-uniform spatial entanglement distribution and observe a crossover from area-law to volume-law entanglement scaling. With 78 qubits and 137 couplers in a 2D configuration, the entire far-from-equilibrium heating dynamics are beyond the reach of simulation using tensor-network numerical techniques. Our work highlights superconducting quantum processors as a powerful platform for exploring universal scaling laws and non-equilibrium phases of matter in driven systems in regimes where classical simulation faces formidable challenges.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_5: Time crystals (TCs) and time quasicrystals (TQCs) represent novel phases of matter that arise from the breaking of time translational symmetry in periodically and quasiperiodically driven quantum systems. In this study, we explore the formation of a TQC phase within the disordered quantum Ising chain model under a transverse field (ITF). Notably, TQCs demonstrate robust subharmonic responses at multiple incommensurate frequencies, unlike traditional TCs which respond at a single frequency. Our analysis reveals that the TQC phase exhibits stable magnetization responses even in the presence of interaction perturbations and imperfections in the quasiperiodic driving fields. Employing exact diagonalization techniques, we find that increasing the chain length further stabilizes both TC and TQC phases. These results suggest promising pathways for experimental realization of TQCs in cold atomic systems and quantum simulators, opening avenues for deeper investigation into these intriguing dynamical phenomena.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_6: Driven systems offer the potential to realize a wide range of non-equilibrium phenomena that are inaccessible in static systems, such as the discrete time crystals. Time rondeau crystals with a partial temporal order have been proposed as a distinctive prethermal phase of matter in systems driven by structured random protocols. Yet, heating is inevitable in closed systems and time rondeau crystals eventually melt. We introduce dissipation to counteract heating and demonstrate stable time rondeau crystals, which persist indefinitely, in a many-body interacting system. A key ingredient is synchronization in the non-interacting limit, which allows for stable time rondeau order without generating excessive heating. The presence of many-body interaction competes with synchronization and a de-synchronization phase transition occurs at a finite interaction strength. This transition is well captured via a linear stability analysis of the underlying stochastic processes.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_7: Here, we calculate and study correlations of the Dicke model in the presence of qubit-qubit interaction. Whereasthe analysis of correlations among its subsystems is essential for the understanding of corresponding critical phenomena and for performing quantum information tasks, the majority of correlation measures are restricted to bipartitions due to the inherent challenges associated with handling multiple partitions. To circunvent this we employ the calculation of Genuine Multipartite Correlations (GMC) based on the invariance of our model under particle permutation. We then quantify the correlations within each subpart of the system, as well as the percentage contribution of each GMC of order $k$, highlighting the many-body behaviors for different regimes of parameters. Additionally, we show that GMC signal both first- and second-order quantum phase transitions present in the model. Furthermore, as GMC encompasses both classical and quantum correlations, we employ Quantum Fisher Information (QFI) to detect genuine multipartite entanglement. Ultimately, we map the Dicke model with interacting qubits to spin in solids interacting with a quantum field of magnons, thus demonstrating a potential experimental realization of this model.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_8: We investigate the electric-field driven power-law random banded matrix (PLRBM) model where a variation in the power-law exponent 𝛼 yields a delocalization-to-localization phase transition. We examine the periodically driven PLRBM model with the help of the Floquet operator. The level spacing ratio and the generalized participation ratio of the Floquet Hamiltonian reveal a drive-induced weak multifractal (fractal) phase accompanied by diffusive (subdiffusive) transport on the delocalized side of the undriven PLRBM model. On the localized side, the time-periodic model remains localized—the average level-spacing ratio corresponds to Poisson statistics and logarithmic transport is observed in the dynamics. Extending our analysis to the aperiodic Thue-Morse driven system, we find that the aperiodically driven clean long-range hopping model (clean counterpart of the PLRBM model) exhibits the phenomenon of exact dynamical localization on tuning the drive parameters at special points. The disordered time-aperiodic system shows diffusive transport followed by relaxation to the infinite-temperature state on the delocalized side, and a prethermal plateau with subdiffusion on the localized side. Additionally, we compare this with a quasiperiodically driven Aubry-André-Harper model that also undergoes a localization-delocalization transition. Unlike the disordered long-range model, it features a prolonged prethermal plateau followed by subdiffusion to the infinite temperature state, even on the delocalized side.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_9: Open systems that are weakly coupled to a thermal environment and driven by fast, periodically oscillating fields are commonly assumed to approach an equilibriumlike steady state with respect to a truncated Floquet-Magnus Hamiltonian. Using a general argument based on Fermi's golden rule, we show that such Floquet-Gibbs states emerge naturally in periodically modulated Rydberg atomic systems, whose laboratory-frame Hamiltonian is a quasiperiodic function of time. Our approach applies as long as the inherent Bohr frequencies of the system, the modulation frequency, and the frequency of the driving laser, which is necessary to uphold high-lying Rydberg excitations, are well separated. To corroborate our analytical results, we analyze a realistic model of up to five interacting Rydberg atoms with periodically changing detuning. We demonstrate numerically that the second-order Floquet-Gibbs state of this system is essentially indistinguishable from the steady state of the corresponding Redfield equation if the modulation and driving frequencies are sufficiently large.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_78_10: Diamonds with nitrogen-vacancy (NV) center ensembles are one of the most promising solid-state quantum platforms for various sensing applications. The combination of a long spin dephasing time (T_2^*) and a high NV center concentration is crucial for pushing the sensitivity limits. In this work, we propose a systematic measurement approach to quantify the electron spin dephasing in NV center ensembles and analyze the contributions of various sources to the dephasing time, including NV-NV interactions, strain and electric field distributions, ^{13}C nuclear spins, and P1 electron spins. Our method is validated using a series of high-performance diamond samples, providing a comprehensive understanding of dephasing mechanisms and revealing correlations between NV concentration and different dephasing sources. Based on these insights, we further evaluate and propose strategies to improve the achievable sensitivity limits for DC magnetic field measurements.","Main_78:   Floquet (periodic) driving has recently emerged as a powerful technique for
engineering quantum systems and realizing non-equilibrium phases of matter. A
central challenge to stabilizing quantum phenomena in such systems is the need
to prevent energy absorption from the driving field. Fortunately, when the
frequency of the drive is significantly larger than the local energy scales of
the many-body system, energy absorption is suppressed. The existence of this
so-called prethermal regime depends sensitively on the range of interactions
and the presence of multiple driving frequencies. Here, we report the
observation of Floquet prethermalization in a strongly interacting dipolar spin
ensemble in diamond, where the angular dependence of the dipolar coupling helps
to mitigate the long-ranged nature of the interaction. Moreover, we extend our
experimental observation to quasi-Floquet drives with multiple incommensurate
frequencies. In contrast to a single-frequency drive, we find that the
existence of prethermalization is extremely sensitive to the smoothness of the
applied field. Our results open the door to stabilizing and characterizing
non-equilibrium phenomena in quasi-periodically driven systems.
",0.5
"Cite_79_1: Detection of entanglement through partial knowledge of the quantum state is a challenge to implement efficiently. Here we propose a separability criterion for detecting bipartite entanglement in arbitrary-dimensional quantum states using partial moments of the realigned density matrix. Our approach enables the detection of both distillable and bound entangled states through a common framework. We illustrate the significance of our method through examples of states belonging to both the above categories, which are not detectable using comparable other schemes relying on partial state information. The formalism of employing partial realigned moments proposed here is further shown to be effective for two-qubit systems too, with a slight modification of our separability criterion","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_2: High-dimensional entanglement is a valuable resource for several quantum information processing tasks, and is often characterized by the Schmidt number and specific classes of entangled states beyond qubit-qubit and qubit-qutrit systems. We propose a criterion to detect high-dimensional entanglement, focusing on determining the Schmidt number of quantum states and identifying significant classes of PPT and NPT entangled states. Our approach relies on evaluating moments of generalized positive maps which can be efficiently simulated in real experiments without the requirement of full-state tomography. We demonstrate the effectiveness of our detection scheme through various illustrative examples. As a direct application, we explore the implications of our moment-based detection schemes in identifying useful quantum channels such as non-Schmidt number breaking channels and non-entanglement breaking channels. Finally, we present the operational implication of our proposed moment criterion through its manifestation in channel discrimination tasks.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_3: Efficiently detecting entanglement based on measurable quantities is a basic problem for quantum information processing. Recently, the measurable quantities called partial-transpose (PT) moments have been proposed to detect and characterize entanglement. Recently [L. Zhang et al., Ann. Phys. (Berlin) 534, 2200289 (2022)], we have already identified the two-dimensional region, comprised of the second and third PT moments, corresponding to two-qubit entangled states, and described the whole region for all two-qubit states. In the present paper, we visualize the three-dimensional (3D) region corresponding to all two-qubit states by further involving the fourth PT moment (the last one for two-qubit states). The characterization of this 3D region can finally be achieved by optimizing some polynomials. Furthermore, we identify the dividing surface which separates the two parts of the whole 3D region corresponding to entangled and separable states, respectively. Due to the measurability of PT moments, we obtain a complete and operational criterion for the detection of two-qubit entanglement.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_4: We have reexamined the moments of positive maps and the criterion based on these moments to detect entanglement. For two qubits, we observed that reduction map is equivalent to partial transpose map as the resulting matrices have the same set of eigenvalues although both matrices look different in same computational basis. Consequently, the detection power of both maps is same. For  systems, we find that moments of reduction map are capable to detect a family of bound entangled states. For qutrit–qutrit systems, we show that moments of reduction map can detect two well-known families of bound entangled states. The moments of another positive map can detect the complete range of entanglement for a specific family of quantum states, whereas the earlier criterion fails to detect a small range of entangled states. For three qubits system, we find that applying reduction map to one of the qubit is equivalent to partial transpose operation. In particularly, for GHZ state and W state mixed with white noise, all the moments of a reduction map are exactly the same as the moments of partial transpose map.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_5: Quantum entanglement plays a key role in quantum computation and quantum information processing. It is of great significance to find efficient and experimentally friend separability criteria to detect entanglement. In this paper, we firstly propose two easily used entanglement criteria based on matrix moments. The first entanglement criterion only uses the first two realignment moments of a density matrix. The second entanglement criterion is based on the moments related to the partially transposed matrix. By detailed examples we illustrate the effectiveness of these criteria in detecting entanglement. Moreover, we provide an experimentally measurable lower bound of concurrence based on these moments. Finally, we present both bipartite and genuine tripartite entanglement measures based on the moments of the reduced states. By detailed examples, we show that our entanglement measures characterize the quantum entanglement in a more fine ways than the existing measures.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_6: In this paper, we mainly investigate the detection of quantum states containing fewer than k unentangled particles in multipartite quantum systems. Based on inequalities of nonlinear operators, we derive two families of criteria for detecting N-partite quantum states containing fewer than k unentangled particles. By concrete examples, we point out that both families of criteria can identify some quantum states containing fewer than k unentangled particles that cannot be tested by known criteria. This demonstrates the effectiveness of our criteria.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_7: Entanglement is fundamental inasmuch because it rephrases the quest for the classical-quantum demarcation line, and it also has potentially enormous practical applications in modern information technology. In this work, employing the approach of matrix decomposition, we introduce and formulate a practicable criterion for separability based on the correlation tensor. It is interesting that this criterion unifies several relevant separability criteria proposed before, even stronger than some of them. Theoretical analysis and detailed examples demonstrate its availability and feasibility for entanglement detection. Furthermore we build a family of entanglement witnesses using the criterion according to its linearity in the density operator space.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_8: Quantum entanglement, a key component in quantum information theory, has attracted significant attention in quantum information processing tasks. Exact characterization of entanglement is essential for understanding and manipulating quantum systems. Recent advancements in this domain have been marked by a study that delves into the characterization of entanglement using partial transpose (PT) moments. In this work (Zhang et al. in Ann Phys (Berlin) 534:2200289, 2022, https://doi.org/10.1002/andp.202200289), researchers delineated a two-dimensional (2D) region using the second and third PT moments, specifically for two-qubit entangled states. This region provides insights into entanglement and moreover identifies the entire 2D space of two-qubit states. In this paper, we further explore another crucial 2D region, incorporating the second and fourth PT moments, for two-qubit entangled states. Additionally, we also determine the complete 2D region for all two-qubit states. This analysis represents one of three 2D projections of a comprehensive three-dimensional (3D) region encompassing the second, third, and fourth PT moments. Our extension not only offers additional insights into the entanglement characteristics of two-qubit states but also complements the previous work by providing a complementary perspective on the entanglement of two-qubit states using PT moments.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_9: Based on the realignment moments of density matrix, we study parameterized entanglement criteria for bipartite and multipartite states. By adjusting the different parameter values, our criterion can detect not only bound entangled states, but also non-positive partial transpose entangled states for bipartite quantum systems. Moreover, we propose the definition of multipartite realignment moments and generalize the result of bipartite systems to obtain a sufficient criterion to detect entanglement for multipartite quantum states in arbitrary dimensions. And we further improve the conclusion to obtain another new entanglement criterion. The new method can detect more entangled states than previous methods as backed by detailed examples.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_79_10: In recent years considerable progress has been made towards developing a general theory of quantum entanglement. In particular, criteria to decide whether a given quantum state is entangled are of high theoretical and practical interest. This problem is additionally complicated by the existence of bound entanglement, which are weak entangled states and hard to detect. In this thesis, we have worked on the characterization of bipartite and tripartite entanglement. We have established a few separability criteria that successfully detect Negative Partial Transpose (NPT) as well as Positive Partial Transpose (PPT) entangled states. Although the topic of detection of entanglement has been extensively studied in the literature through many approaches, the majority of these criteria are not physically realizable. This means that they are well accepted in the mathematical language but cannot be implemented in a laboratory setting. In this thesis, we propose some theoretical ideas to realize these entanglement detection criteria experimentally.","Main_79:   We introduce $\Lambda$-moments with respect to any positive map $\Lambda$. We
show that these $\Lambda$-moments can effectively characterize the entanglement
of unknown quantum states without theirs prior reconstructions. Based on
$\Lambda$-moments necessary and sufficient separability criteria, as well as
necessary optimized criteria are presented, which include the ones in
[\href{https://link.aps.org/doi/10.1103/PhysRevLett.127.060504}{Phys. Rev.
Lett. \textbf{127}, 060504 (2021)}] as special cases. Detailed example is given
to show that our criteria can detect bound entanglement that can not be
identified by positive partial transpose criterion, with the explicit
measurement operators to experimentally measure the corresponding
$\Lambda$-moments.
",0.5
"Cite_80_1: Change detection, a critical task in remote sensing and computer vision, aims to identify pixel-level differences between image pairs captured at the same geographic area but different times. It faces numerous challenges such as illumination variation, seasonal changes, background interference, and shooting angles, especially with a large time gap between images. While current methods have advanced, they often overlook temporal dependencies and overemphasize prominent changes while ignoring subtle but equally important changes. To address these limitations, we introduce 	extbf{CEBSNet}, a novel change-excited and background-suppressed network with temporal dependency modeling for change detection. During the feature extraction, we utilize a simple Channel Swap Module (CSM) to model temporal dependency, reducing differences and noise. The Feature Excitation and Suppression Module (FESM) is developed to capture both obvious and subtle changes, maintaining the integrity of change regions. Additionally, we design a Pyramid-Aware Spatial-Channel Attention module (PASCA) to enhance the ability to detect change regions at different sizes and focus on critical regions. We conduct extensive experiments on three common street view datasets and two remote sensing datasets, and our method achieves the state-of-the-art performance.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_2: The significance of building change detection (CD) lies in its contribution to understanding urban development and human activities. Convolutional neural networks have emerged as the predominant approach for CD, yet existing architectures still exhibit limitations in feature extraction and detection accuracy, with most state-of-the-art (SOTA) methods containing millions of parameters. To address this, we propose a dual-domain mixed lightweight U-Net (DML-UNet) for improved building CD in remote sensing imagery. Specifically, our method employs: (1) An optimized EfficientNet-B4 backbone using only its first three module slices for efficient feature extraction, achieving parameter reduction while enhancing performance; (2) A novel dual-domain mixed attention block (DMAB) that processes features through spatial/frequency-domain striping and cross-temporal feature mixing to boost representational capacity with lower computational costs; (3) Upsampling operations combined with a pixel to pixel classifier for precise change identification. Extensive experiments on LEVIR-CD and WHU-CD datasets demonstrate that DML-UNet achieves F1-scores of 91.21% and 91.81%, respectively, with merely 0.31M parameters, outperforming SOTA DSTAMNet by 1.4% and 1.78%, validating its superiority over existing CD methods.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_3: Optical change detection is limited by imaging conditions, hindering real-time applications. Synthetic Aperture Radar (SAR) overcomes these limitations by penetrating clouds and being unaffected by lighting, enabling all-weather monitoring when combined with optical data. However, existing heterogeneous change detection datasets lack complexity, focusing on single-scene targets. To address this gap, we introduce the XiongAn dataset, a novel urban architectural change dataset designed to advance heterogeneous change detection research. Furthermore, we propose HeteCD, a fully supervised heterogeneous change detection framework. HeteCD employs a Siamese Transformer architecture with non-shared weights to effectively model heterogeneous feature spaces and includes a Feature Consistency Alignment (FCA) loss to harmonize distributions and ensure class consistency across bi-temporal images. Additionally, a 3D Spatio-temporal Attention Difference module is incorporated to extract highly discriminative difference information from bi-temporal features. Extensive experiments on the XiongAn dataset demonstrate that HeteCD achieves a superior IoU of 67.50%, outperforming previous state-of-the-art methods by 1.31%. ","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_4: Building change detection (CD) is significant for understanding ground changes and human activities. Deep learning has become the mainstream approach for building CD. However, the detection accuracy remains insufficient due to limitations in feature extraction. Therefore, this paper proposes a feature enhancement network, FENET-UEVTS, to improve the accuracy of building detection, which combines a UNet encoder and a vision transformer structure to detect building changes. It can enhance the ability to detect irregular buildings and distinguish changes between adjacent buildings in different locations. The model combines a deep convolutional network with a part of vision transformer structure, which has a robust feature extraction ability for various types of buildings. We design a spatial-channel attention mechanism module (SCAM) that takes into account both the spatial and channel dimensions to enhance the detection ability of small-scale buildings. We also develop a u-shaped residual module (USRM) and a strengthened feature extraction module (SFEM) to improve the feature extraction capability for buildings with different shapes and edge details. A self-attention feature fusion module (SAFFM) is proposed to facilitate the full convergence and integration of different feature information. The SAFFM can better distinguish buildings of various shapes and sizes to prevent false detection and missed detection. To minimize information loss, a cross-channel context semantic aggregation module (CCSAM) is designed to perform information aggregation in the channel dimension. To evaluate the performance of our model, we conducted numerous experiments on three CD datasets. The results demonstrate that our proposed model outperforms eight other state-of-the-art (SOTA) algorithms in F1-score, overall accuracy, and KAPPA coefficient, achieving up to 91.83 %, 87.65 %, and 93.29 % F1-score on three widely used public datasets, i.e., LEVIR-CD, WHU-CD, and CDD dataset.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_5: In recent years, semantic change detection (SCD) has emerged as a pivotal field within the remote sensing (RS) research community, underscored by its essential contribution to various Earth observation undertakings. Conventional SCD methodologies typically adopt a multitask network architecture, fusing a binary change detection (BCD) sub-task with dual semantic segmentation (SS) sub-tasks. These strategies frequently rely on the encoder’s low-resolution yet semantically dense features, derived from multiple down-sampling stages, as the inputs for the decoding heads. Departing from this traditional path and targeting the nuanced characteristics of the multisubtasks, this study pioneers a novel methodology that harnesses the potential of the encoding phase’s high-resolution features. By integrating HRNet as the encoder structure, we introduce the BT-HRSCD framework, featuring two simple and effective modules. The first, bidirectional shallow and deep features aggregation module (BiFAM), seeks to imbue features with richer semantic insights through bidirectional feature fusion that spans from shallow-to-deep as well as deep-to-shallow layers. The second module, high-resolution difference extraction (HRDE), utilizes the encoder’s highest spatial resolution features, evaluating their differences to enhance the precision in identifying change areas. BiFAM is devised to boost the SS sub-tasks’ effectiveness, whereas HRDE aims to elevate the accuracy of the BCD sub-task. Experimental results reveal that our method outperforms state-of-the-art performances relative to previous SCD efforts. ","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_6: Semantic change detection (SCD) extends beyond binary change detection by not only discerning the locations of change areas, but also offering the alterations in land-cover/land-use types. This refined change information is concernful for various applications. While deep learning methods have made significant progress in SCD, accurately capturing the integrity of targets remains challenging in intricate scenarios. Therefore, this article proposes a deformable multiscale composite transformer network (DMCTNet). This network effectively models relevant semantic information and spatio-temporal dependencies. DMCTNet leverages a variant vision foundation models encoder to learn specific knowledge, facilitating effective visual representation in remote sensing images. A multiscale feature aggregator module is developed to discern both the “what” and “where” of changes by integrating features across different scales. Subsequently, a masked decoder through queries to convey rich semantic change information, guided by conspicuous change potential locations to decode. A substantial volume of experimental results consistently demonstrate that this model achieves more accurate and reliable results in change areas, improving the intersection over the union of change by 2.65% and 1.67% on the SECOND and Landsat-SCD datasets, respectively. Code will be made available.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_7: Data augmentation (DA) increases the diversity of training data to improve the model generalization ability. Most of the DA methods focus on image classification or object detection. Directly using the existing DA methods on change detection tasks not only falls short of fully exploring the specificity of the change image pairs but also leads to longer training times. In this article, we first propose a mask-guided mixing (MGM) DA for change detection, which mixes the change regions of the current training sample based on prediction results and labels to generate high-quality samples with more positive samples. We then propose a new reinforcement learning (RL)-based Adaptive DA method, AdaAug+, to adaptively select the optimal DA policy for the training samples. An actor selects the best augmentation operation from the operation set according to the image pair. The augmented image pairs make it easier for the change detector to learn the optimal parameters and improve the final detection performance. To reduce the training time, we identify and remove the redundant training samples during the training process by our redundancy searching policy. We have conducted various experiments on four remote sensing change detection datasets with different change detectors. The experimental results demonstrate that AdaAug+ achieves promising performance compared to the state-of-the-art DA methods and requires less training time.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_80_8: Remote sensing image change captioning (RSICC) aims to articulate the changes in objects of interest within bitemporal remote sensing images using natural language. Given the limitations of current RSICC methods in expressing general features across multitemporal and spatial scenarios, and their deficiency in providing granular, robust, and precise change descriptions, we introduce a novel change captioning (CC) method based on the foundational knowledge and semantic guidance, which we term Semantic-CC. Semantic-CC alleviates the dependency of high-generalization algorithms on extensive annotations by harnessing the latent knowledge of foundation models, and it generates more comprehensive and accurate change descriptions guided by pixel-level semantics from change detection (CD). Specifically, we propose a bitemporal SAM-based encoder for dual-image feature extraction; a multitask semantic aggregation neck for facilitating information interaction between heterogeneous tasks; a straightforward multiscale CD decoder to provide pixel-level semantic guidance; and a change caption decoder based on the large language model (LLM) to generate change description sentences. Moreover, to ensure the stability of the joint training of CD and CC, we propose a three-stage training strategy that supervises different tasks at various stages. We validate the proposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results corroborate the complementarity of CD and CC, demonstrating that Semantic-CC can generate more accurate change descriptions and achieve optimal performance across both tasks.","Main_80:   Change detection (CD) aims to detect change regions within an image pair
captured at different times, playing a significant role in diverse real-world
applications. Nevertheless, most of the existing works focus on designing
advanced network architectures to map the feature difference to the final
change map while ignoring the influence of the quality of the feature
difference. In this paper, we study the CD from a different perspective, i.e.,
how to optimize the feature difference to highlight changes and suppress
unchanged regions, and propose a novel module denoted as iterative
difference-enhanced transformers (IDET). IDET contains three transformers: two
transformers for extracting the long-range information of the two images and
one transformer for enhancing the feature difference. In contrast to the
previous transformers, the third transformer takes the outputs of the first two
transformers to guide the enhancement of the feature difference iteratively. To
achieve more effective refinement, we further propose the multi-scale
IDET-based change detection that uses multi-scale representations of the images
for multiple feature difference refinements and proposes a coarse-to-fine
fusion strategy to combine all refinements. Our final CD method outperforms
seven state-of-the-art methods on six large-scale datasets under diverse
application scenarios, which demonstrates the importance of feature difference
enhancements and the effectiveness of IDET.
",0.4
"Cite_81_1: Integrating a carbon dioxide energy storage system (CES) with an integrated energy system (IES) can significantly enhance renewable energy utilization, reduce carbon emissions, and improve both economic and environmental performance. This paper proposes an optimal scheduling strategy for a gas–liquid phase change CES coupled with wind and solar generation, considering multi-layer low-carbon benefits. At the system structural level, an IES operation framework integrating carbon capture and storage (CCS) and power-to-gas (P2G) technologies is developed to fully exploit the potential of CO2 capture and utilization. At the technical level, a mathematical model of gas–liquid phase change CES coupled with wind and solar is established to enhance renewable energy absorption. Based on a life cycle cost model of gas–liquid phase change CES, the economic and low-carbon advantages of CES are pointed out. By introducing a stepped carbon trading mechanism at the policy level, an IES scheduling model with the goal of considering the comprehensive economic optimization of low-carbon benefits is established. The results of the scene show that the proposed method reduces the system carbon emissions by 33.26%, increases the new energy consumption rate by 3.54%, and reduces the total operating cost of the system by 24.30%, which lays a theoretical foundation for the research of environmentally friendly IES.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_2: The integration of green energy sources such as hydrogen and natural gas plays a vital role in advancing low-carbon energy goals. Conventional integrated energy systems (IES) employing hydrogen-compressed natural gas (HCNG) typically operate with fixed mixing ratio, limiting flexibility and overlooking positive market feedback effects. This study proposes an optimal scheduling strategy for an IES with a dynamic hydrogen blending ratio within a joint carbon–green certificate trading framework. This model considers wind power penetration impacts on electrolysis efficiency using a dynamic electrolyzer efficiency model and incorporates operation and maintenance costs of HCNG pipelines. A trading strategy based on equivalence between green certificates and carbon quotas is developed to improve economic performance and emissions reduction. The influence of hydrogen production levels on carbon trading and market revenues is examined. A case study demonstrates that this method achieves a 25.16 % cost reduction, 17.21 % decrease in emissions, and 98.8 %a wind energy utilization.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_3: This study proposes an improved multi-objective black-winged kite algorithm (MOBKA-QL) integrating Q-learning with adaptive mutation strategies for optimizing multi-objective scheduling in integrated energy systems (IES). The algorithm dynamically selects mutation strategies through Q-learning to enhance solution diversity and accelerate convergence. First, an optimal scheduling model is established, incorporating a carbon capture system (CCS), power-to-gas (P2G), solar thermal, wind power, and energy storage to minimize economic costs and carbon emissions while maximizing energy efficiency. Second, the heat-to-power ratio of the cogeneration system is dynamically adjusted according to load demand, enabling flexible control of combined heat and power (CHP) output. The integration of CCS+P2G further reduces carbon emissions and wind curtailment, with the produced methane utilized in boilers and cogeneration systems. Hydrogen fuel cells (HFCs) are employed to mitigate cascading energy losses. Using forecasted load and renewable energy data from a specific region, dispatch experiments demonstrate that the proposed system reduces economic costs and CO2 emissions by 14.63% and 13.9%, respectively, while improving energy efficiency by 28.84%. Additionally, the adjustable heat-to-power ratio of CHP yields synergistic economic, energy, and environmental benefits.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_4: The increasing interconnection and digitalization of modern energy systems have intensified cybersecurity vulnerabilities in Power Cyber-Physical Systems (Power CPS). Traditional centralized defense approaches struggle to balance privacy preservation, scalability, and collaborative responsiveness across distributed infrastructures. Federated Learning (FL) emerges as a promising paradigm that enables distributed, privacy-preserving model training without sharing raw data. This review presents a comprehensive analysis of FL-based collaborative defense for Power CPS, spanning threat modeling, architectural taxonomies, privacy-preserving mechanisms, and realworld applications. We categorize FL techniques by learning structure, synchronization, and personalization, and examine privacy-enhancing technologies such as differential privacy, secure multiparty computation, homomorphic encryption, and trusted execution environments. Practical applications across substations, SCADA systems, WAMS, and EV infrastructures are reviewed alongside deployment challenges such as communication overhead, adversarial threats, and operational constraints. A roadmap is proposed for future research in cross-layer FL architectures, federated reinforcement learning, and regulatory standardization. The review concludes by advocating for cross-sector collaboration to operationalize federated defense as a cornerstone of resilient, secure, and privacy-compliant smart grids. ","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_5: The increasing complexity and digitalization of Power Cyber-Physical Systems (Power CPS) have amplified their exposure to persistent and adaptive cyber-physical threats. Static configurations across cyber, physical, and market layers offer adversaries stable attack surfaces for reconnaissance, manipulation, and disruption. In response, this review introduces Moving Target Defense (MTD) as a proactive and dynamic paradigm for enhancing Power CPS security. The paper systematically analyzes multi-domain MTD strategies—including IP randomization, topology reconfiguration, market rule variability, and adaptive control mode switching—and evaluates their effectiveness in disrupting attacker planning while preserving operational stability. Human-in-theloop orchestration frameworks, supported by AI-driven monitoring and explainable decision support, are proposed to coordinate safe and scalable MTD deployment. Additionally, we discuss digital twin-based validation platforms, resilience metrics, and real-world deployment challenges. The review concludes by outlining future research priorities and cross-sector collaboration pathways to operationalize adaptive MTD as a foundational pillar of resilient grid security. ","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_6: Existing researches predominantly focus on optimizing time-of-use tariffs for single-type consumers, overlooking the fact that an actual distribution system comprises multi-type consumers with distinct power consumption behaviors and preferences. To better cater to the diverse needs of different consumer types, this study proposes an optimization method for multi-type consumers of time-of-use tariffs using price-based demand response and a consumer classification model. Firstly, a joint classification model for multi-type consumers is constructed based on convolutional neural network autoencoder and hierarchical clustering algorithm. Then, by applying the constructed model to historical power consumption data, consumers are classified into several groups. Secondly, a fuzzy heuristic algorithm is introduced to optimize time-of-use tariffs for these consumer groups. Through fuzzy reasoning, the optimal tariffs during different time periods are determined for each group using the fuzzy heuristic algorithm. The effectiveness and adaptability of the proposed method is verified through an application example involving time-of-use tariff optimization for an actual distribution network. Simulation results demonstrated that the maximum power demands for consumers powered by the power company reduced by 6%~9%. The load rate for the studied power company increased from 0.77 to 0.81 ~ 0.84. In addition, compared with two latest methods, when the proposed method was used for ToU tariff optimization, reduction rate of the maximum power demand was improved by 0.75%~2.05% and 0.98%~2.52%, respectively. Load rate for the power company was improved by 0.01 ~ 0.03 and 0.02 ~ 0.03.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_7: The convergence of cyber, physical, and human elements in modern power systems brings both operational advantages and escalating security challenges. Beyond traditional False Data Injection Attack (FDIA) defenses, emerging threats now involve coordinated, multi-stage, and AIdriven cyber-physical attacks across technical, organizational, and market layers. This review proposes a mission-centric defense paradigm for Power Cyber-Physical Systems (Power CPS), integrating architectural resilience through redundancy, diversity, and trustworthy sensing with real-time, explainable, and human-in-the-loop defense mechanisms. It emphasizes the role of multiagent coordination, edge-cloud collaboration, and operator-centered situational awareness in ensuring effective response and mission continuity. Post-attack recovery strategies, including data reconstruction, adaptive reconfiguration, and resilience metric integration, are also explored. Finally, the review highlights future research and cross-sector collaboration needs to advance holistic cyberphysical-human resilience for secure energy system operations. ","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_8: Coordination between electric power systems (EPS) and natural gas systems (NGS) offers benefits, but faces problems concerning the nonlinear and non-convex Weymouth equation, privacy protection, and incentive compatibility. To address these issues, we propose a decentralized cooperation mechanism for integrated electricity and gas systems (IEGS). The Weymouth equation is linearized through state-space transition and the golden section method (GSM). Based on the highly precise linear Weymouth equation, the non-convex IEGS operation model is reformulated as convex quadratic programming (CQP), greatly improving the solution process and reducing the computational burden. Additionally, we design a distributed incentive approach that employs Benders decomposition for privacy protection and the Shapley value for benefit allocation. Experimental results demonstrate the proposed cooperation mechanism fosters a win-win situation for two energy sectors with limited information exchange and high accuracy.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_9: With the increasing integration of cyber and physical layers, power cyber-physical systems (CPS) face growing risks from advanced cyber threats, especially False Data Injection Attacks (FDIAs). Existing defense methods mainly focus on detection, leaving gaps against evolving, multistage attacks. This review advocates a shift toward Resilient-by-Design CPS, emphasizing systemlevel co-design of secure architectures, distributed detection, edge intelligence, and recovery mechanisms. It further discusses adaptive defense strategies using reinforcement learning, generative models, and explainable AI. Future directions include integrating resilience into grid planning, adversarial testing, and cross-sector policy coordination, aiming to enable secure and intelligent power systems. ","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_10: Reliable supply of electricity in isolated rural areas is challenging due to the uneconomical accessibility of the national grid. The issue can be resolved with an isolated microgrid. This study presents an optimal design and techno-economic analysis of an isolated microgrid based on hybrid renewable energy systems (HRES) for meeting the electricity demand of a rural area, 'Kanur,' Maharashtra, India. The proposed microgrid integrates the wind turbine (WT), solar photovoltaic (PV), biogas generator (BG), and battery energy storage system (BES). The HOMER software minimizes the net present cost (NPC) and cost of energy (COE) to offer a reasonable cost-optimal design at desired system reliability. At 0.0 % capacity shortage, the optimal sizing of PV, WT, BG, and BES units are 113 kW, 22 kW, 17 kW, and 362, respectively. At capacity shortages of 0.0 % and 2.5 %, the optimal WT/PV/BG/BES configuration achieves an NPC of $529,459 and $399,680 with COE of 0.146$/kWh and 0.112$/kWh, respectively. Excess energy generation is 23 % and 13.9 % of total annual generation, respectively. The proposed HRES is 71.2 % more cost-effective than diesel generator (DG) supply. The sensitivity analysis highlights the impact of system parameter variations on component sizing, NPC, and COE, aiding in the most cost-effective design selection.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_11: This paper provides a detailed review of reliability assessment methods for composite power systems, focusing on integrating renewable energy and advanced computational approaches. The study classifies existing research into three main areas: investigation studies, planning and optimization studies, and efficient evaluation studies. Findings indicate that machine learning techniques are increasingly important in improving accuracy and computational performance in reliability analysis. A comparative examination of conventional and Machine Learning (ML)-based methods demonstrates that deep learning models, such as Convolutional Neural Networks, offer substantial reductions in computational time while maintaining reliability assessment precision. The review also identifies key research gaps, such as the need for realistic test systems and enhanced hybrid ML strategies. Additionally, recommendations are proposed to address these challenges and strengthen the effectiveness of future reliability evaluation methodologies. The insights presented in this study aim to support researchers and industry professionals in developing more efficient and scalable reliability assessment models for evolving power systems.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_12: Efficient energy storage scheduling technology has become crucial for ensuring grid stability and enhancing system economy as the increasing proportion of renewable energy in the energy structure. This study proposes a data-driven dispatch strategy for compressed air energy storage (CAES), aimed at achieving the dual objectives of combined cooling, heating, and power (CCHP) and reducing operational costs. First, a dynamic modular approach is employed to model the CAES system, accurately representing the physical characteristics and coupling relationships of its components, thereby minimizing mutual interference among them. Secondly, to address the inadequacies of traditional control strategies in adapting to dynamic environments and their low efficiency in energy management, this research introduces an improved model-free deep reinforcement learning (DRL) algorithm—TD3-AC. This algorithm innovatively combines self-attention mechanisms and behavior cloning models on the basis of the traditional twin delayed deep deterministic policy gradient (TD3) algorithm, significantly enhancing the stability and robustness of energy dispatch. Experimental results demonstrate that, compared to traditional dispatch methods, the proposed algorithm successfully reduces operational costs by 8.3 % and improves dispatch accuracy by 29.13 %. This achievement greatly enhances the system's stability and economy, providing an innovative technical pathway for the intelligent management of large-scale energy storage systems.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_13: The increasing integration of Cyber-Physical Systems (CPS) into power grids has significantly enhanced their efficiency and flexibility. However, this integration has also exposed power grids to emerging cyber threats, particularly False Data Injection Attacks (FDIAs), which can disrupt the operation of Power CPS by manipulating system data. This paper reviews the challenges, detection methods, and resilience strategies related to FDIAs in Power CPS. It provides an overview of the types of FDIAs, their impact on system security and stability, and the evolving nature of these attacks. It further examines recent advances in detection techniques, including machine learning, deep learning, and hybrid detection methods, as well as data reconstruction and attack localization strategies. The paper concludes by highlighting future research directions, emphasizing the need for a multi-faceted approach combining technical, regulatory, and operational measures to secure Power CPS against the growing threat of FDIAs.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_14: The integration of Vehicle-to-Grid (V2G) systems into modern power grids has emerged as a promising solution to enhance grid stability, reduce energy costs, and promote renewable energy utilization. However, the optimal sizing of V2G systems remains a critical challenge due to the conflicting objectives of minimizing costs and maximizing reliability. This paper proposes a novel approach that combines the Grasshopper Optimization Algorithm (GOA) with a rule-based energy management scheme to address the sizing problem of V2G systems. The GOA is employed to optimize the system configuration, while the rule-based energy management scheme ensures efficient energy distribution and utilization. The proposed methodology is evaluated through a case study, demonstrating its effectiveness in achieving a balance between cost and reliability. The results indicate that the proposed approach outperforms traditional methods in terms of both economic and operational performance.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_15: False Data Injection Attacks (FDIAs) pose a significant threat to the security and stability of Power Cyber-Physical Systems (CPS). As these systems become more interconnected and automated, the risk of malicious data manipulation has increased, leading to potential grid instability, equipment damage, and large-scale outages. This review provides a comprehensive analysis of recent advancements in FDIA detection, defense mechanisms, and resilience strategies. It examines various detection methods, from traditional state estimation to machine learning-based approaches, and discusses the integration of hybrid techniques for enhanced accuracy. The review also explores emerging technologies such as federated learning, quantum computing, and IoT, highlighting their potential in strengthening FDIA defenses. Additionally, the importance of a multi-faceted approach, combining technical, regulatory, and operational solutions, is emphasized for ensuring the long-term security of power CPS. The review concludes with future research directions focused on dynamic attack modeling, adaptive defense systems, and the integration of emerging technologies to address the evolving landscape of cyber threats.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_16: Artificial Intelligence (AI) based energy management systems utilize sophisticated AI algorithms to improve and control the consumption of energy in various sectors, such as power utilities, industrial systems, and smart buildings. These systems support real-time analysis of data, predictive analytics, and automatic adjustments to improve energy efficiency, reduce expenses, and lower environmental footprints. This research introduces a new method of AI-based energy management through the creation of advanced mathematical aggregation operators under the theory of hesitant bipolar complex fuzzy sets (HBCFSs). The generalized HBCFS theory is a complete decision-making model that can efficiently deal with uncertainties, hesitancy, and bipolar information in a complex setting. To solve the intrinsic difficulties of energy management decision-making, we propose a series of new HBCF Hamacher power aggregation operators. These operators improve the precision and stability of multi-attribute decision-making (MADM) processes by using the Hamacher t-norm and power aggregation rules to represent intricate interactions among decision attributes. Further, a comparative study is conducted to highlight the strength and superiority of the proposed aggregation operators that significantly contribute to AI energy management systems. The results establish that the method developed significantly improves the accuracy and reliability of decisions, warranting application in energy distribution and usage optimization.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_17: To better consume new energy and make full use of new energy, an interval optimal scheduling method of integrated energy system considering wind power uncertainty is proposed. Interval mathematics is used to characterize the uncertainty of wind power output. Taking the system operation cost and wind curtailment cost as the optimization objectives, the interval optimization of integrated energy system and related optimization models are constructed. The model after uncertainty transformation is solved by allele quantum derivative algorithm to further improve the optimization accuracy. Combined with Copula distribution and K-means clustering, the typical scenario combination of source and load is generated. The simulation results show that the proposed interval optimization scheduling method can effectively reduce the operation cost of integrated energy system and improve the utilization of new energy.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_18: The global energy landscape is witnessing a transformational shift brought about by the adoption of renewable energy technologies along with power system modernisation. Distributed generation (DG), smart grids (SGs), microgrids (MGs), and advanced energy storage systems (AESSs) are key enablers of a sustainable and resilient energy future. This review deepens the analysis of the fulminating change in power systems, detailing the growth of power systems, wind and solar integration, and next-generation high-voltage direct current (HVDC) transmission systems. Moreover, we address important aspects such as power system monitoring, protection, and control, the dynamic modelling of transmission and distribution systems, and advanced metering infrastructure (AMI) development. Emphasis is laid on the involvement of artificial intelligence (AI) techniques in optimised grid operation, voltage control, stability, and the system integration of lifetime energy resources such as islanding and hosting capacities. This paper reviews the key aspects of current advancements in grid technologies and their applications, enabling the identification of opportunities and challenges to be addressed toward achieving a modern, intelligent, and efficient power system infrastructure. It wraps up with a perspective on future research paths as well as a discussion of potential hybrid models that integrate AI and machine learning (ML) with distributed energy systems (DESs) to improve the grid’s resilience and sustainability.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_19: Within the water-energy-food nexus, desalination and refrigeration cogeneration systems are able to address pressing sustainable development goals. Such systems exploit the synergy between the two processes to yield mutual benefits. This review evaluates existing cogeneration studies to recommend promising research directions for sustainable development applications such as simultaneous freshwater production and cold storage refrigeration. Compared to recent reviews of combined desalination and cooling systems, the novelty of this review is in its focus on refrigeration temperatures suitable for fresh food storage (between 0–15 °C), differentiating hybrid systems as Integrated or Coupled, and highlighting the potential technical and economic synergies between refrigeration and desalination systems for sustainable development applications. The key aspects examined included (1) capacity of desalination and refrigeration, (2) source of water and potential contaminants, (3) system design and synergies, (4) efficiency of energy and material resources, and (5) integration with renewable energy sources. The primary synergy in integrated systems was the simultaneous low-temperature evaporation of water and chilling of a cooling fluid. In coupled systems, the synergy comes from the usage of waste heat from refrigeration systems to drive thermal desalination processes. Regarding performance, the systems with the highest water recovery rates and thermal desalination efficiencies are hybrid multiple effect distillation systems. Promising directions for future research include experimentally validating numerically derived inferences, achieving deeper heat integration to improve system efficiency, and developing passive measures such as indirect evaporative cooling for simultaneous dehumidification, cooling, and water production.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_81_20: With the increasing penetration of distributed generators (DGs), the fault current of the connected distribution network (DN) becomes complex and variable. The overcurrent signals at the same location may be decided by the connected grid or solely by the DGs. As a result, the fault location methods based on intelligent optimization algorithms have low accuracy and poor fault tolerance, especially when the overcurrent signals are determined only by the DGs under simultaneous multi-area faults. Aimed at the aforementioned problems, a novel fault location method based on an improved seagull optimization algorithm is proposed for the distribution grid integrated with the DGs. The expression of the switch status function for DNs is firstly improved considering the impact of DGs on the overcurrent signals. An elite reverse learning strategy is introduced for the seagull optimization algorithm to the diversity of the initial seagull population. Both the Levy flight control and random walk strategies are used to increase the randomness of the optimization algorithm. It is good for avoiding the emergence of locally optimal results due to the variable overcurrent status of the feeder terminal units (FTUs). Finally, the proposed fault location method was validated using a simulation model of an active DN with photovoltaic DGs based on the IEEE 33 nodes. Based on the simulation results, it is verified that the proposed fault location method can identify single-point or multi-point faults in the case of distorted overcurrent signals. The proposed method is superior to the existing one in both high accuracy and high fault tolerance.","Main_81: Multi-uncertainties from power sources and loads have brought significant
challenges to the stable demand supply of various resources at islands. To
address these challenges, a comprehensive scheduling framework is proposed by
introducing a model-free deep reinforcement learning (DRL) approach based on
modeling an island integrated energy system (IES). In response to the shortage
of freshwater on islands, in addition to the introduction of seawater
desalination systems, a transmission structure of ""hydrothermal simultaneous
transmission"" (HST) is proposed. The essence of the IES scheduling problem is
the optimal combination of each unit's output, which is a typical timing
control problem and conforms to the Markov decision-making solution framework
of deep reinforcement learning. Deep reinforcement learning adapts to various
changes and timely adjusts strategies through the interaction of agents and the
environment, avoiding complicated modeling and prediction of
multi-uncertainties. The simulation results show that the proposed scheduling
framework properly handles multi-uncertainties from power sources and loads,
achieves a stable demand supply for various resources, and has better
performance than other real-time scheduling methods, especially in terms of
computational efficiency. In addition, the HST model constitutes an active
exploration to improve the utilization efficiency of island freshwater.
",1.0
"Cite_82_1: Long video understanding has become a critical task in computer vision, driving advancements across numerous applications from surveillance to content retrieval. Existing video understanding methods suffer from two challenges when dealing with long video understanding: intricate long-context relationship modeling and interference from redundancy. To tackle these challenges, we introduce Fine-Detailed Video Story generation (FDVS), which interprets long videos into detailed textual representations. Specifically, to achieve fine-grained modeling of long-temporal content, we propose a Bottom-up Video Interpretation Mechanism that progressively interprets video content from clips to video. To avoid interference from redundant information in videos, we introduce a Semantic Redundancy Reduction mechanism that removes redundancy at both the visual and textual levels. Our method transforms long videos into hierarchical textual representations that contain multi-granularity information of the video. With these representations, FDVS is applicable to various tasks without any fine-tuning. We evaluate the proposed method across eight datasets spanning three tasks. The performance demonstrates the effectiveness and versatility of our method.","Main_82:   Learning with noisy label (LNL) is a classic problem that has been
extensively studied for image tasks, but much less for video in the literature.
A straightforward migration from images to videos without considering the
properties of videos, such as computational cost and redundant information, is
not a sound choice. In this paper, we propose two new strategies for video
analysis with noisy labels: 1) A lightweight channel selection method dubbed as
Channel Truncation for feature-based label noise detection. This method selects
the most discriminative channels to split clean and noisy instances in each
category; 2) A novel contrastive strategy dubbed as Noise Contrastive Learning,
which constructs the relationship between clean and noisy instances to
regularize model training. Experiments on three well-known benchmark datasets
for video classification show that our proposed tru{\bf N}cat{\bf
E}-split-contr{\bf A}s{\bf T} (NEAT) significantly outperforms the existing
baselines. By reducing the dimension to 10\% of it, our method achieves over
0.4 noise detection F1-score and 5\% classification accuracy improvement on
Mini-Kinetics dataset under severe noise (symmetric-80\%). Thanks to Noise
Contrastive Learning, the average classification accuracy improvement on
Mini-Kinetics and Sth-Sth-V1 is over 1.6\%.
",0.2
"Cite_82_2: Numerous well-annotated human key-point datasets are publicly available to date. However, annotating human poses for newly collected images is still a costly and time-consuming progress. Pose distributions from different datasets share similar pose hinge-structure priors with different geometric transformations, such as pivot orientation, joint rotation, and bone length ratio. The difference between Pose distributions is essentially the difference between the transformation distributions. Inspired by this fact, we propose a method to calibrate a pre-trained pose generator in which the pose prior has already been learned to an adapted one following a new pose distribution. We treat the representation of human pose joint coordinates as skeleton image and transfer a pre-trained pose annotation generator with only a few annotation guidance. By fine-tuning a limited number of linear layers that closely related to the pose transformation, the adapted generator is able to produce any number of pose annotations that are similar to the target poses. We evaluate our proposed method, FlexPose, on several cross-dataset settings both qualitatively and quantitatively, which demonstrates that our approach achieves state-of-the-art performance compared to the existing generative-model-based transfer learning methods when given limited annotation guidance.","Main_82:   Learning with noisy label (LNL) is a classic problem that has been
extensively studied for image tasks, but much less for video in the literature.
A straightforward migration from images to videos without considering the
properties of videos, such as computational cost and redundant information, is
not a sound choice. In this paper, we propose two new strategies for video
analysis with noisy labels: 1) A lightweight channel selection method dubbed as
Channel Truncation for feature-based label noise detection. This method selects
the most discriminative channels to split clean and noisy instances in each
category; 2) A novel contrastive strategy dubbed as Noise Contrastive Learning,
which constructs the relationship between clean and noisy instances to
regularize model training. Experiments on three well-known benchmark datasets
for video classification show that our proposed tru{\bf N}cat{\bf
E}-split-contr{\bf A}s{\bf T} (NEAT) significantly outperforms the existing
baselines. By reducing the dimension to 10\% of it, our method achieves over
0.4 noise detection F1-score and 5\% classification accuracy improvement on
Mini-Kinetics dataset under severe noise (symmetric-80\%). Thanks to Noise
Contrastive Learning, the average classification accuracy improvement on
Mini-Kinetics and Sth-Sth-V1 is over 1.6\%.
",0.2
"Cite_82_3: A comprehensive and explicit understanding of surgical scenes plays a vital role in developing context-aware computer-assisted systems in the operating theatre. However, few works provide systematical analysis to enable hierarchical surgical scene understanding. In this work, we propose to represent the tasks set [phase recognition → step recognition → action and instrument detection] as multi-level semantic scene understanding (MSSU). For this target, we propose a novel hierarchical context transformer (HCT) network and thoroughly explore the relations across the different level tasks. Specifically, a hierarchical relation aggregation module (HRAM) is designed to concurrently relate entries inside multi-level interaction information and then augment task-specific features. To further boost the representation learning of the different tasks, inter-task contrastive learning (ICL) is presented to guide the model to learn task-wise features via absorbing complementary information from other tasks. Furthermore, considering the computational costs of the transformer, we propose HCT+ to integrate the spatial and temporal adapter to access competitive performance on substantially fewer tunable parameters. Extensive experiments on our cataract dataset and a publicly available endoscopic PSI-AVA dataset demonstrate the outstanding performance of our method, consistently exceeding the state-of-the-art methods by a large margin. The code is available at https://github.com/Aurora-hao/HCT.","Main_82:   Learning with noisy label (LNL) is a classic problem that has been
extensively studied for image tasks, but much less for video in the literature.
A straightforward migration from images to videos without considering the
properties of videos, such as computational cost and redundant information, is
not a sound choice. In this paper, we propose two new strategies for video
analysis with noisy labels: 1) A lightweight channel selection method dubbed as
Channel Truncation for feature-based label noise detection. This method selects
the most discriminative channels to split clean and noisy instances in each
category; 2) A novel contrastive strategy dubbed as Noise Contrastive Learning,
which constructs the relationship between clean and noisy instances to
regularize model training. Experiments on three well-known benchmark datasets
for video classification show that our proposed tru{\bf N}cat{\bf
E}-split-contr{\bf A}s{\bf T} (NEAT) significantly outperforms the existing
baselines. By reducing the dimension to 10\% of it, our method achieves over
0.4 noise detection F1-score and 5\% classification accuracy improvement on
Mini-Kinetics dataset under severe noise (symmetric-80\%). Thanks to Noise
Contrastive Learning, the average classification accuracy improvement on
Mini-Kinetics and Sth-Sth-V1 is over 1.6\%.
",0.2
"Cite_82_4: Recent advancements in layout pattern generation have been dominated by deep generative models. However, relying solely on neural networks for legality guarantees raises concerns in many practical applications. In this paper, we present 	ool{DiffPattern}-Flex, a novel approach designed to generate reliable layout patterns efficiently. 	ool{DiffPattern}-Flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. To ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. Furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. Experimental results across various benchmarks demonstrate that 	ool{DiffPattern}-Flex significantly outperforms existing methods and excels at producing reliable layout patterns.","Main_82:   Learning with noisy label (LNL) is a classic problem that has been
extensively studied for image tasks, but much less for video in the literature.
A straightforward migration from images to videos without considering the
properties of videos, such as computational cost and redundant information, is
not a sound choice. In this paper, we propose two new strategies for video
analysis with noisy labels: 1) A lightweight channel selection method dubbed as
Channel Truncation for feature-based label noise detection. This method selects
the most discriminative channels to split clean and noisy instances in each
category; 2) A novel contrastive strategy dubbed as Noise Contrastive Learning,
which constructs the relationship between clean and noisy instances to
regularize model training. Experiments on three well-known benchmark datasets
for video classification show that our proposed tru{\bf N}cat{\bf
E}-split-contr{\bf A}s{\bf T} (NEAT) significantly outperforms the existing
baselines. By reducing the dimension to 10\% of it, our method achieves over
0.4 noise detection F1-score and 5\% classification accuracy improvement on
Mini-Kinetics dataset under severe noise (symmetric-80\%). Thanks to Noise
Contrastive Learning, the average classification accuracy improvement on
Mini-Kinetics and Sth-Sth-V1 is over 1.6\%.
",0.2
"Cite_83_1: The advent of upcoming radio surveys, such as those enabled by the SKA Observatory, will provide the desired sensitivity and resolution required for weak-lensing studies. However, current shape and shear measurement methods are mostly tailored for optical surveys. The development of novel techniques to facilitate weak-lensing measurements in the radio waveband is thus needed.There are a few algorithms for shape measurement in the radio waveband. However, these are either computationally intensive or fail to achieve the accuracy required for future surveys. In this work, we present a supervised deep learning framework, dubbed DeepShape, that measures galaxy shapes with the necessary precision while minimizing computational expenses.  DeepShape is made of two modules. The first module is a plug-and-play (PnP) image reconstruction algorithm based on the half-quadratic splitting method (HQS), dubbed HQS-PnP, which reconstructs images of isolated radio galaxies. The second module is a measurement network that predicts galaxy shapes using the point spread function and reconstructed image pairs. We test our framework on a simulated radio data set based on the SKA-MID AA4 array configuration. The HQS-PnP algorithm outperforms the standard multiscale CLEAN algorithm across several tested metrics. DeepShape recovers galaxy shapes with an accuracy comparable to the leading radio shape measurement method, RadioLensfit, while significantly reducing the prediction time from ~4 minutes to ~220 milliseconds. We also demonstrate DeepShape’s applicability to shear measurements and recover shear estimates with an additive bias meeting SKA-MID requirements. Although the multiplicative shear bias is an order of magnitude higher than the required level, it can be mitigated using a shear measurement calibration technique, such as applying quality cuts.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_2: We performed individual weak-lensing (WL) mass measurements for 78 eROSITA's first All-Sky Survey (eRASS1) clusters in the footprint of Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) S19A. We did not adopt priors on the eRASS1 X-ray quantities or assumption of the mass and concentration relation. In the sample, we found three clusters are misassociated with optical counterparts and 12 clusters are poorly fitted with an NFW profile. The average mass for the 12 poor-fit clusters changes from sim 10^{14}h_{70}^{-1}M_odot to sim 2	imes 10^{13}h_{70}^{-1}M_odot when lensing contamination from surrounding mass structures is taken into account. The scaling relations between the true mass and cluster richness and X-ray count-rate agree well with the results of the eRASS1 western Galactic hemisphere region based on count-rate-inferred masses, which were calibrated with the HSC-SSP, DES, and KiDS surveys. We developed a Bayesian framework for inferring the mass-concentration relation of the cluster sample, explicitly incorporating the effects of weak-lensing mass calibration in the mass-concentration parameter space. The redshift-dependent mass and concentration relation is in excellent agreement with predictions of dark-matter-only numerical simulations and previous studies using X-ray-selected clusters. Based on the two-dimensional (2D) WL analysis, the offsets between the WL-determined centers and the X-ray centroids for 36 eRASS1 clusters with high WL S/N can be described by two Gaussian components. We find that the miscentering effect with X-ray centroids is smaller than that involving peaks in the galaxy maps. Stacked mass maps support a small miscentering effect, even for clusters with a low WL S/N. The projected halo ellipticity is langle varepsilon angle=0.45 at M_{200}sim 4	imes10^{14}h_{70}^{-1}M_odot.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_3: To date, galaxy image simulations for weak lensing surveys usually approximate the light profiles of all galaxies as a single or double Sérsic profile, neglecting the influence of galaxy substructures and morphologies deviating from such a simplified parametric characterisation. While this approximation may be sufficient for previous data sets, the stringent cosmic shear calibration requirements and the high quality of the data in the upcoming Euclid survey demand a consideration of the effects that realistic galaxy substructures and irregular shapes have on shear measurement biases. Here we present a novel deep learning-based method to create such simulated galaxies directly from Hubble Space Telescope (HST) data. We first build and validate a convolutional neural network based on the wavelet scattering transform to learn noise-free representations independent of the point-spread function (PSF) of HST galaxy images. These can be injected into simulations of images from Euclid’s optical instrument VIS without introducing noise correlations during PSF convolution or shearing. Then, we demonstrate the generation of new galaxy images by sampling from the model randomly as well as conditionally. In the latter case, we fine-tune the interpolation between latent space vectors of sample galaxies to directly obtain new realistic objects following a specific Sérsic index and half-light radius distribution. Furthermore, we show that the distribution of galaxy structural and morphological parameters of our generative model matches the distribution of the input HST training data, proving the capability of the model to produce realistic shapes. Next, we quantify the cosmic shear bias from complex galaxy shapes in Euclid-like simulations by comparing the shear measurement biases between a sample of model objects and their best-fit double-Sérsic counterparts, thereby creating two separate branches that only differ in the complexity of their shapes. Using the Kaiser, Squires, and Broadhurst shape measurement algorithm, we find a multiplicative bias difference between these branches with realistic morphologies and parametric profiles on the order of (6.9 ± 0.6) × 10−3 for a realistic magnitude-Sérsic index distribution. Moreover, we find clear detection bias differences between full image scenes simulated with parametric and realistic galaxies, leading to a bias difference of (4.0 ± 0.9) × 10−3 independent of the shape measurement method. This makes complex morphology relevant for stage IV weak lensing surveys, exceeding the full error budget of the Euclid Wide Survey (∆µ1,2 < 2 × 103).","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_4: Data from the Euclid space telescope will enable cosmic shear measurements to be carried out with very small statistical errors, necessitating a corresponding level of systematic error control. A common approach to correct for shear biases involves calibrating shape measurement methods using image simulations with known input shear. Given their high resolution, galaxies observed with the Hubble Space Telescope (HST) can, in principle, be utilised to emulate Euclid observations of sheared galaxy images with realistic morphologies. In this work, we employ a GalSim-based testing environment to investigate whether uncertainties in the HST point spread function (PSF) model or in data processing techniques introduce significant biases in weak-lensing (WL) shear calibration. We used single Sérsic galaxy models to simulate both HST and Euclid observations. We then ‘Euclidised’ our HST simulations and compared the results with the directly simulated Euclid-like images. For this comparison, we utilised a moment-based shape measurement algorithm and galaxy model fits. Through the Euclidisation procedure, we effectively reduced the residual multiplicative biases in shear measurements to sub-percent levels. This achievement was made possible by employing either the native pixel scales of the instruments, utilising the Lanczos15 interpolation kernel, correcting for noise correlations, and ensuring consistent galaxy signal-to-noise ratios between simulation branches. Alternatively, a finer pixel scale can be employed alongside deeper HST data. However, the Euclidisation procedure requires further analysis on the impact of the correlated noise, to estimate calibration bias. We found that additive biases can be mitigated by applying a post-deconvolution isotropisation in the Euclidisation set-up. Additionally, we conducted an in-depth analysis of the accuracy of TinyTim HST PSF models using star fields observed in the F606W and F814W filters. We observe that F606W images exhibit a broader scatter in the recovered best-fit focus, compared to those in the F814W filter. Estimating the focus value for the F606W filter in lower stellar density regimes has allowed us to reveal significant statistical uncertainties.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_5: Current and upcoming imaging galaxy surveys are pushing galaxy samples to higher and higher redshifts. This push will be more pronounced for lens galaxies, for which we only need to measure galaxy positions, not shapes. As a result, we will increasingly often have lens galaxy samples at redshifts higher than those of source galaxies, changing the traditional configuration of galaxy-galaxy lensing (GGL). In this paper, we explore this situation, where lens galaxies are behind source galaxies, which we call inverse galaxy-galaxy lensing (IGGL). We take projected lens and source sample specifications from the Vera Rubin Observatory Large Synoptic Survey Telescope Dark Energy Science Collaboration to compare astrophysical and cosmological constraints between traditional GGL and IGGL. We find IGGL to behave in a different way than GGL, being especially sensitive to lensing magnification, intrinsic alignments and cosmology, but largely independent of galaxy bias (as opposed to traditional GGL). In this way, we find IGGL can provide independent and robust cosmological constraints without combination with galaxy clustering and can also probe IA at high redshift and baryonic effects at small scales without being entwined with the effects of nonlinear galaxy bias. When combined with cosmic shear, we find IGGL to improve 𝑆8 constraints by 25% compared to cosmic shear alone, while also providing tighter and more robust constraints on IA and baryons.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_6: We present a weak lensing analysis of the galaxy cluster A2390 at z = 0.23 using second moment shape measurements made in 411 short 60 s exposures. The exposures are obtained in three broadband photometric filters (g, r, and i) using WIYN-ODI. Shape measurement in individual exposures is done using a moment-matching algorithm. Forced measurement is used when the moment-matching algorithm fails to converge at low signal-to-noise ratio. The measurements made in individual images are combined using inverse error weighting to obtain accurate shapes for the sources and hence recover shear. We use PhoSim simulations to validate the shear measurements recovered by our pipeline. We find the mass of A2390 is in agreement with previously published results. We also find the E-mode maps show filamentary structures consistent with baryonic structures and recover most clusters/groups of galaxies found using optical and X-ray data. Thus, we demonstrate the feasibility of using weak lensing to map large-scale structure of the Universe. We also find the central portion of the cluster has a bimodal mass distribution and the relative orientation of the peaks is similar to X-ray. We discuss earlier research on this galaxy cluster, and show that a late-stage merger accounts for all the observed data.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_7: To date, galaxy image simulations for weak lensing surveys usually approximate the light profiles of all galaxies as a single or double Sérsic profile, neglecting the influence of galaxy substructures and morphologies deviating from such a simplified parametric characterisation. While this approximation may be sufficient for previous data sets, the stringent cosmic shear calibration requirements and the high quality of the data in the upcoming Euclid survey demand a consideration of the effects that realistic galaxy substructures and irregular shapes have on shear measurement biases. Here we present a novel deep learning-based method to create such simulated galaxies directly from Hubble Space Telescope (HST) data. We first build and validate a convolutional neural network based on the wavelet scattering transform to learn noise-free representations independent of the point-spread function (PSF) of HST galaxy images. These can be injected into simulations of images from Euclid’s optical instrument VIS without introducing noise correlations during PSF convolution or shearing. Then, we demonstrate the generation of new galaxy images by sampling from the model randomly as well as conditionally. In the latter case, we fine-tune the interpolation between latent space vectors of sample galaxies to directly obtain new realistic objects following a specific Sérsic index and half-light radius distribution. Furthermore, we show that the distribution of galaxy structural and morphological parameters of our generative model matches the distribution of the input HST training data, proving the capability of the model to produce realistic shapes. Next, we quantify the cosmic shear bias from complex galaxy shapes in Euclid-like simulations by comparing the shear measurement biases between a sample of model objects and their best-fit double-Sérsic counterparts, thereby creating two separate branches that only differ in the complexity of their shapes. Using the Kaiser, Squires, and Broadhurst shape measurement algorithm, we find a multiplicative bias difference between these branches with realistic morphologies and parametric profiles on the order of (6.9 ± 0.6)×10−3 for a realistic magnitude-Sérsic index distribution. Moreover, we find clear detection bias differences between full image scenes simulated with parametric and realistic galaxies, leading to a bias difference of (4.0 ± 0.9)×10−3 independent of the shape measurement method. This makes complex morphology relevant for stage IV weak lensing surveys, exceeding the full error budget of the Euclid Wide Survey (Δμ1, 2 < 2 × 103).","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_83_8: To date, galaxy image simulations for weak lensing surveys usually approximate the light profiles of all galaxies as a single or double S'ersic profile, neglecting the influence of galaxy substructures and morphologies deviating from such a simplified parametric characterization. While this approximation may be sufficient for previous data sets, the stringent cosmic shear calibration requirements and the high quality of the data in the upcoming Euclid survey demand a consideration of the effects that realistic galaxy substructures have on shear measurement biases. Here we present a novel deep learning-based method to create such simulated galaxies directly from HST data. We first build and validate a convolutional neural network based on the wavelet scattering transform to learn noise-free representations independent of the point-spread function of HST galaxy images that can be injected into simulations of images from Euclid's optical instrument VIS without introducing noise correlations during PSF convolution or shearing. Then, we demonstrate the generation of new galaxy images by sampling from the model randomly and conditionally. Next, we quantify the cosmic shear bias from complex galaxy shapes in Euclid-like simulations by comparing the shear measurement biases between a sample of model objects and their best-fit double-S'ersic counterparts. Using the KSB shape measurement algorithm, we find a multiplicative bias difference between these branches with realistic morphologies and parametric profiles on the order of $6.9	imes 10^{-3}$ for a realistic magnitude-S'ersic index distribution. Moreover, we find clear detection bias differences between full image scenes simulated with parametric and realistic galaxies, leading to a bias difference of $4.0	imes 10^{-3}$ independent of the shape measurement method. This makes it relevant for stage IV weak lensing surveys such as Euclid.","Main_83:   The GRavitational lEnsing Accuracy Testing 3 (GREAT3) challenge is the third
in a series of image analysis challenges, with a goal of testing and
facilitating the development of methods for analyzing astronomical images that
will be used to measure weak gravitational lensing. This measurement requires
extremely precise estimation of very small galaxy shape distortions, in the
presence of far larger intrinsic galaxy shapes and distortions due to the
blurring kernel caused by the atmosphere, telescope optics, and instrumental
effects. The GREAT3 challenge is posed to the astronomy, machine learning, and
statistics communities, and includes tests of three specific effects that are
of immediate relevance to upcoming weak lensing surveys, two of which have
never been tested in a community challenge before. These effects include
realistically complex galaxy models based on high-resolution imaging from
space; spatially varying, physically-motivated blurring kernel; and combination
of multiple different exposures. To facilitate entry by people new to the
field, and for use as a diagnostic tool, the simulation software for the
challenge is publicly available, though the exact parameters used for the
challenge are blinded. Sample scripts to analyze the challenge data using
existing methods will also be provided. See http://great3challenge.info and
http://great3.projects.phys.ucl.ac.uk/leaderboard/ for more information.
",0.4
"Cite_84_1: We analyze the ground state energy of N fermions in a two-dimensional box interacting with an impurity particle via two-body point interactions. We show that for weak coupling, the ground state energy is asymptotically described by the polaron energy, as proposed by F. Chevy in the physics literature. The polaron energy is the solution of a nonlinear equation involving the Green’s function of the free Fermi gas and the binding energy of the two-body point interaction. We provide quantitative error estimates that are uniform in the thermodynamic limit.","Main_84:   Ultracold quantum gases of equal spin fermions with short range interactions
are often considered free even in the presence of strongly binding
spin-up-spin-down pairs. We describe a large class of many-particle
Schr\""odinger operators with short-range pair interactions, where this
approximation can be justified rigorously.
",0.15
"Cite_84_2: We study the low-energy spectrum of large Bose-Fermi mixtures. In the chosen scaling, the fermions induce an effective attraction among the bosons, which competes with their intrinsic repulsive interaction. Our main result demonstrates the convergence of the eigenvalues towards those of an effective Bose Hamiltonian. For short-range potentials, we apply this result to derive a stability-instability transition in the bosonic subsystem, driven by the Bose-Fermi coupling strength g. For small |g|, the bosons form a stable Bose-Einstein condensate with the energy per particle uniformly bounded from below. For large |g|, the energy per particle is no longer uniformly bounded from below, signalling the collapse of the condensate.","Main_84:   Ultracold quantum gases of equal spin fermions with short range interactions
are often considered free even in the presence of strongly binding
spin-up-spin-down pairs. We describe a large class of many-particle
Schr\""odinger operators with short-range pair interactions, where this
approximation can be justified rigorously.
",0.15
"Cite_84_3: In this article, we consider the d-dimensional mollified stochastic heat equation (SHE) when the mollification parameter is turned off. Here, we concentrate on the high-dimensional case d geq 3. Recently, the limiting higher moments of the two-dimensional mollified SHE have been established. However, this problem in high dimensions remains unexplored to date. The main theorems of this article aim to answer this question and prove some related properties: (1) Our first main result, based on the spectral theorem for the unbounded operator, proves the divergence of the higher moments of the high-dimensional mollified SHE even when the system is strictly inside the L^{2}-regime. This phenomenon is completely opposite to its two-dimensional counterpart; (2) To further differentiate the nature of the high-dimensional case from the case in two dimensions, our second main result proves the unboundedness of the sub-limiting higher moments of the three-dimensional mollified SHE at the L^{2}-criticality. Here, the sub-limiting higher moment is a natural limit of the higher moment of the three-dimensional mollified SHE at the L^{2}-criticality; (3) As an application, we provide partial results for the conjecture about the high-order critical regimes of the continuous directed polymer. The other byproduct of the above results gives a proper estimate for the critical exponent of the polymer in the L^{2}-regime.","Main_84:   Ultracold quantum gases of equal spin fermions with short range interactions
are often considered free even in the presence of strongly binding
spin-up-spin-down pairs. We describe a large class of many-particle
Schr\""odinger operators with short-range pair interactions, where this
approximation can be justified rigorously.
",0.15
"Cite_85_1: Within the extensive array of image generative models, two models are particularly notable: Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN). Generative Adversarial Networks (GANs) can generate realistic images; nevertheless, they are prone to mode collapse and lack straightforward methods for obtaining the latent representation of an image. Conversely, VAEs do not encounter these issues; yet, they frequently produce images that are less realistic than those generated by GANs. This article elucidates that the absence of realism is partly attributable to a prevalent overestimate of the dimensionality of the natural image manifold. To address this issue, we propose a new framework that integrates VAE with GAN in a unique and complementary manner, resulting in an auto-encoding model that retains the features of VAEs while creating images of GAN quality. We assess our methodology using both qualitative and quantitative analyses across five image datasets. We introduce a comprehensive learning system that integrates a deep convolutional GAN network with a variational autoencoder network. Initially, we identified a technique that addresses the issue of images generated by GANs typically being unclear and distorted. In this scenario, the integration of GAN with VAE may be a more advantageous option.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_2: This study explores the integrationof artificial intelligence (AI) and modern data analytics for accurately predicting and classifying three distinct periods of volcanic activity. By leveraging previously dated volcanic samples, we assess whether existing age and geochemical data can reliably group and predict volcanic episodes. Our study focuses on the Kula Volcanic Province (Turkey). We compare the effectiveness of two analytical techniques—Electron Microprobe Analysis (EPMA) and Laser Ablation Inductively Coupled Plasma Mass Spectrometry (LA-ICP-MS)—in producing high-quality datasets for training deep learning models. While EPMA provides major and minor elemental compositions, LA-ICP-MS offers a broader range of trace elements, which may improve classification accuracy. Two experiments were conducted to evaluate the feasibility of AI-based volcanic rock age estimation. In the first experiment, an autoencoder and unsupervised clustering were applied to reduce dimensionality and group samples based on their elemental composition. The results revealed that EPMA data lacked sufficient detail to form well-defined clusters, whereas LA-ICP-MS data produced clusters that closely aligned with true age classes due to their higher sensitivity to trace elements. In the second experiment, a deep neural network (DNN) was trained to classify rock ages. The LA-ICP-MS-based model achieved a classification accuracy of 95 %, significantly outperforming the EPMA-based model (72 %). These findings underscore the importance of data quality and analytical technique selection in AI-powered geochronology, demonstrating that high-quality trace element data enhances AI model performance for volcanic rock age estimation.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_3: Can machines truly think, reason and act in domains like humans? This enduring question continues to shape the pursuit of Artificial General Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5, DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal fluency and partial reasoning, these systems remain fundamentally limited by their reliance on token-level prediction and lack of grounded agency. This paper offers a cross-disciplinary synthesis of AGI development, spanning artificial intelligence, cognitive neuroscience, psychology, generative models, and agent-based systems. We analyze the architectural and cognitive foundations of general intelligence, highlighting the role of modular reasoning, persistent memory, and multi-agent coordination. In particular, we emphasize the rise of Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use to enable more adaptive behavior. We discuss generalization strategies, including information compression, test-time adaptation, and training-free methods, as critical pathways toward flexible, domain-agnostic intelligence. Vision-Language Models (VLMs) are reexamined not just as perception modules but as evolving interfaces for embodied understanding and collaborative task completion. We also argue that true intelligence arises not from scale alone but from the integration of memory and reasoning: an orchestration of modular, interactive, and self-improving components where compression enables adaptive behavior. Drawing on advances in neurosymbolic systems, reinforcement learning, and cognitive scaffolding, we explore how recent architectures begin to bridge the gap between statistical learning and goal-directed cognition. Finally, we identify key scientific, technical, and ethical challenges on the path to AGI.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_4: Artificial intelligence (AI) has been increasingly influencing both the general public and scientific research, yet its adoption in agricultural sciences remains unclear. While recent reviews suggest that AI has already permeated agricultural research, no systematic study has examined this phenomenon. This study employs a bibliometric analysis to assess AI-related publication trends using metadata from 14 agricultural and applied biology journals, along with a reference methodological journal (Computers and Electronics in Agriculture), covering the period from 2010 to 2023. The findings reveal a significant rise in AI-related studies in methodological quantitative research for agricultural sciences, with over 60% of recent articles in Computers and Electronics in Agriculture incorporating AI. This trend, however, has not yet extended to applied agricultural research, where AI-related publications remain a small fraction of the total output. These results indicate that while AI is transforming methodological studies related to data science for agriculture, its broader adoption in applied agricultural research is still in its early stages.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_5: This article delves into the groundbreaking convergence of traditional artistry and modern technology through a singular event: the orchestration of a transformative concert at the Istanbul Harbiye Cemil Topuzlu Open Air Theater on July 14, 2022. Featuring artworks generated by Generative Adversarial Networks (GAN) models alongside live orchestration, this extraordinary event underscores the unparalleled and singular nature of the artistic experience. It illuminates the profound synergy between traditional artistic sensibilities and the boundless possibilities afforded by technological advancement, marking a significant milestone in the evolution of art in the digital age.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_6: As generative artificial intelligence tools like ChatGPT become increasingly integrated into educational environments, understanding their impact on critical thinking is crucial. Despite growing concerns about AI's potential to diminish students' independent reasoning, there is a lack of research tools specifically designed to evaluate students' perceptions of AI's cognitive capabilities. To address this gap, this study introduces the Perceived Critical Thinking Disposition of Generative Artificial Intelligence (PCTD-GAI) scale, aimed at measuring how students perceive generative AI’s (GAI) six critical thinking dispositions (reasoning, access to justice, search for evidence, search for truth, open-mindedness, and systematicity). While this study validates the scale using ChatGPT, the instrument is adaptable for evaluating other generative AI tools, supporting broader research in AI-driven learning environments, to assess not only how students engage with AI, but also how their reliance on AI may affect their cognitive development and self-regulated learning skills in digital education. To develop and validate the PCTD-GAI scale, the Marmara Critical Thinking Dispositions Scale (MCTDS) was adapted, ensuring relevance to AI assessment while maintaining conceptual robustness. A quantitative cross-sectional study was conducted with 931 university students from Portugal and Poland, employing exploratory and confirmatory factor analyses (EFA & CFA) to assess the scale’s validity and reliability. The results demonstrate that the PCTD-GAI effectively captures students’ perceptions of ChatGPT’s critical thinking dispositions across six key dimensions. Findings indicate moderately positive perceptions across both countries, with Portuguese students consistently rating ChatGPT marginally higher across domains and showing less response variability, suggesting greater consensus. Notably, perceptions were most neutral in the truth-seeking domain, while systematicity received the highest ratings, reflecting ChatGPT’s perceived systematic capabilities among students. These findings have significant implications for e-learning and AI-driven education. The PCTD-GAI scale enables educators to track students’ evolving AI literacy and develop targeted interventions that promote critical AI engagement rather than passive reliance on AI-generated content. Moreover, this research advances the field of e-learning by offering an empirical basis for integrating AI assessment into digital learning strategies, ensuring that AI serves as a cognitive tool rather than a substitute for independent reasoning. The validated PCTD-GAI scale provides a reliable, scalable method for assessing students’ perceptions of AI's cognitive capabilities, supporting evidence-based AI pedagogy, and guiding institutional policies on AI integration in education.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_7: This document summarizes the panel discussion titled 'AI Foundations and Applications,' held at Loyola University Chicago as part of the 'Forum on Global Affairs: Artificial Intelligence in a Globalized World' series. The panel brought together interdisciplinary experts to discuss the foundational aspects of artificial intelligence (AI), its applications, ethical considerations, and implications for education and society.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_8: Natural products and their derivatives have been important for treating diseases in humans, animals, and plants. However, discovering new structures from natural sources is still challenging. In recent years, artificial intelligence (AI) has greatly aided the discovery and development of natural products and drugs. AI facilitates to: connect genetic data to chemical structures or vice-versa, repurpose known natural products, predict metabolic pathways, and design and optimize metabolites biosynthesis. More recently, the emergence and improvement in neural networks such as deep learning and ensemble automated web based bioinformatics platforms have sped up the discovery process. Meanwhile, AI also improves the identification and structure elucidation of unknown compounds from raw data like mass spectrometry and nuclear magnetic resonance. This article reviews these AI-driven methods and tools, highlighting their practical applications and guide for efficient natural product discovery and drug development.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_9: Neural networks have proven to be remarkable function approximators, being able to learn models that both fit the training data and being able to generalize very well to unseen data. Deep learning techniques have pushed their learning capabilities even further, enabling neural networks to learn more abstract, higher-level concepts. However, neural networks still remain black-box classifiers, their decision-making process remains mostly incomprehensible. To address this issue, rule extraction techniques aim to derive symbolic rule sets from trained neural networks, offering a more interpretable representation of their learned functions. In this thesis, we present one such method for creating rule sets from feedforward neural networks. However, most conventional rule extraction techniques aim to generate shallow rule sets from neural networks, that is, rules that map the input features directly to class label predictions. This does not lend itself well for cases where complex functions rely on intermediate representations of abstract concepts. For this reason, the decompositonal rule extraction technique that we present will rather create a deep rule set, i.e. one where production rules can reference each other. We evaluate our method on a range of handcrafted datasets, some that can be described well by both shallow and deep rule sets alike, as well as an extreme example of a dataset which can only be learned by models that make predictions through the use of intermediate concepts. We show that, while the models tend to increase in size very quickly, our rule extraction method can indeed learn accurate deep rule sets on a variety of tasks, including problems where conventional rule learning algorithms like RIPPER might struggle.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_10: One of the main challenges in NeuroSymbolic AI is the integration of logical constraints into Neural Networks. In this work, we focus on the extension of deep learning architectures and learning methods that attempt to address this. Firstly, we investigate the capabilities of certain neural networks to learn propositional formulas. Then, we propose a neural architecture that is capable of learning the functional representation of any propositional formula. We explore which neural networks can learn which propositional formulas and investigate how we can optimize their training times and spatial complexity. Inspired by Minsky and Papert's parallel machines, we developed a new set of neural architectures, the Signal Perceptrons. We have proven both analytically and experimentally that these architectures are efficient in terms of learning parameters when used to learn propositional formulas. Furthermore, we show experimentally that such architectures can learn arbitrary functions defined over discrete domains, and their applicability may extend to mainstream Deep Learning tasks such as classification. Next, we look at how to embed logical constraints into neural architectures. We extended current semantic regularization methods to provide a new set of loss functions. These loss functions can inject the semantic information of propositional and finite-variable first-order logic formulas into the network. Initially, we build upon the work of citep{semanticLossXu2018} A Semantic Loss Function for Deep Learning with Symbolic Knowledge, by introducing a new set of Semantic Loss functions. These extensions enable neural networks to learn a probability distribution that is consistent with the models of a logic formula. This is a more robust approach for formula learning, as it ensures not only the satisfaction of formulas, but also the learning of a probability distribution that samples all the models that satisfy a given formula. Furthermore, we have expanded the domain of the Semantic Loss to accommodate formulas of a first-order logic language with a finite set of variables. This is achieved by constructing a constraint distribution out of external knowledge or logic constraints. Then, we defined the total loss function as a linear combination of the deep learning task loss and our proposed semantic loss, which is the Fisher-Rao distance or Kullback-Leibler divergence of the constraint distribution.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_11: This paper proposes that Artificial Intelligence (AI) progresses through several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI), AI 3.0 (Physical AI), and a speculative AI 4.0 (Conscious AI). Each AI generation is driven by shifting priorities among algorithms, computing power, and data. AI 1.0 accompanied breakthroughs in pattern recognition and information processing, fueling advances in computer vision, natural language processing, and recommendation systems. AI 2.0 is built on these foundations through real-time decision-making in digital environments, leveraging reinforcement learning and adaptive planning for agentic AI applications. AI 3.0 extended intelligence into physical contexts, integrating robotics, autonomous vehicles, and sensor-fused control systems to act in uncertain real-world settings. Building on these developments, the proposed AI 4.0 puts forward the bold vision of self-directed AI capable of setting its own goals, orchestrating complex training regimens, and possibly exhibiting elements of machine consciousness. This paper traces the historical foundations of AI across roughly 70 years, mapping how changes in technological bottlenecks from algorithmic innovation to high-performance computing to specialized data have stimulated each generational leap. It further highlights the ongoing synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the ethical, regulatory, and philosophical challenges that arise when artificial systems approach (or aspire to) human-like autonomy. Ultimately, understanding these evolutions and their interdependencies is pivotal for guiding future research, crafting responsible governance, and ensuring that AI’s transformative potential benefits society.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_12: We present ExoMiner++, an enhanced deep learning model that builds on the success of ExoMiner to improve transit signal classification in 2-minute TESS data. ExoMiner++ incorporates additional diagnostic inputs, including periodogram, flux trend, difference image, unfolded flux, and spacecraft attitude control data, all of which are crucial for effectively distinguishing transit signals from more challenging sources of false positives. To further enhance performance, we leverage multi-source training by combining high-quality labeled data from the Kepler space telescope with TESS data. This approach mitigates the impact of TESS's noisier and more ambiguous labels. ExoMiner++ achieves high accuracy across various classification and ranking metrics, significantly narrowing the search space for follow-up investigations to confirm new planets. To serve the exoplanet community, we introduce new TESS catalog containing ExoMiner++ classifications and confidence scores for each transit signal. Among the 147,568 unlabeled TCEs, ExoMiner++ identifies 7,330 as planet candidates, with the remainder classified as false positives. These 7,330 planet candidates correspond to 1,868 existing TESS Objects of Interest (TOIs), 69 Community TESS Objects of Interest (CTOIs), and 50 newly introduced CTOIs. 1,797 out of the 2,506 TOIs previously labeled as planet candidates in ExoFOP are classified as planet candidates by ExoMiner++. This reduction in plausible candidates combined with the excellent ranking quality of ExoMiner++ allows the follow-up efforts to be focused on the most likely candidates, increasing the overall planet yield.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_13: n this thesis, we study a deep-learning-based approach to the compression of multi-scale partial differential operators characterized by highly heterogeneous coefficients. Such operators naturally appear in many science and engineering disciplines when modeling processes in heterogeneous media, that are marked by the interaction of effects on multiple scales. In order to simulate the effective behaviour of the operator on a macroscopic target scale of interest without having to resolve all microscopic features in the model with a computational mesh, many numerical homogenization methods that compress such operators reliably to macroscopic surrogate models suitable for this task, have been developed over the years.  We propose to approximate these surrogates with a neural network in a hybrid offline-online algorithm, that aims at combining the advantages of classical model-based numerical homogenization with those of the data-driven regime of deep learning. In the offline phase, a neural network is trained to approximate the coefficient-to-surrogate map from a dataset consisting of coefficient-surrogate-pairs, which is computed with classical numerical homogenization algorithms. This has the advantage that in the subsequent online phase, compressing previously unseen coefficients via forward passes through the trained network is significantly accelerated compared to the classical homogenization algorithm used in the offline phase. This makes multi-query applications where online efficiency is crucial, for example the simulation of evolution equations with time-dependent multi-scale coefficients, computationally feasible. We apply this hybrid framework to a prototypical elliptic homogenization problem in connection with a representative modern numerical homogenization method, the Localized Orthogonal Decomposition. To justify our approach mathematically, we rigorously analyze it from the viewpoint of approximation theory and prove that the surrogates produced by the Localized Orthogonal Decomposition can be approximated up to arbitrary accuracy by a feedforward neural network. We provide upper bounds on the depth and number of non-zero parameters for such a network and discuss the required fine-scale discretization level to obtain optimal error bounds for a neural-network-based surrogate model. Furthermore, we perform numerical experiments to demonstrate the feasibility of our method, using a very high-dimensional class of elliptic multi-scale coefficients as a demonstrating example, and test its limitations by means of diffusion coefficients based on realizations of high-contrast random fields. Finally, we numerically investigate the performance of our method in a time-dependent setting by considering heterogeneous heat and wave equations with time-dependent multi-scale coefficients.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_85_14: A graph is used to represent complex systems where both entities and their interconnections are equally important. Real-life situations, e.g., social networks, biological networks, recommender systems, etc., are better modeled in terms of graphical structures, as the information about individual entities is not enough to understand the whole system. Due to the existence of non-uniformity in graphical data, traditional machine learning algorithms that perform tasks like prediction, classification, etc., can not be applied directly to such data. Graph Neural Networks (GNNs) are robust variants of deep neural network models that are typically designed to learn from such graphical data. GNN involves transforming graph data into Euclidean representations that various machine-learning algorithms can utilize.","Main_85:   Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.
",0.7
"Cite_86_1: Topological invariants are crucial for characterizing topological systems. However, experimentally measuring them presents a significant challenge, especially in non-Hermitian systems where the biorthogonal eigenvectors are often necessary. We propose a general approach for measuring the topological invariants of one-dimensional non-Hermitian systems, which can be derived from the spin textures of right eigenstates. By utilizing a dilation method, we realize a non-Hermitian system without chiral symmetry on a two-qubit nuclear magnetic resonance system and measure the winding number associated with the eigenstates. In addition to examining the topology of the eigenstates, our experiment also reveals the topological structure of the energy band, which differs from that in chiral systems. Our work paves the way for further exploration of complex topological properties in non-Hermitian systems without chiral symmetry.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_2:  We consider a generalization of the Su-Schrieffer-Heeger (SSH) model by including next-nearest-neighbour (NNN) interaction and balanced loss-gain (BLG), and subjecting the whole system to an external uniform magnetic field. We study the band structure, edge states, and persistent current in this extended SSH model under general boundary condition (GBC) of which the periodic, antiperiodic, and open boundary conditions appear as special cases. It is shown that the point band gap decreases with increasing strength of the NNN interaction and vanishes beyond a critical value for both topologically trivial and nontrivial phases. Further, the line gap exhibits closed-loop-like structures for nonvanishing NNN interaction under the periodic boundary condition (PBC). The Zak phase receives no contribution from the NNN interaction under the PBC. We show that the NNN interaction has no effect on the persistent current in the half-filled limit for the case of PBC, while for other fillings less than the half-filling, it enhances the magnitude of the current significantly. We numerically study the variation of the persistent current with respect to the system parameters under the GBC, for the case in which the Hamiltonian admits entirely real spectra, and show that its magnitude increases with the increasing strength of the BLG. We show that the model without the NNN interaction is exactly solvable for a class of GBC, of which PBC, antiperiodic boundary condition (APBC) and anti-Hermitian boundary condition (AHBC) arise as special cases. We obtain analytic expressions for the edge states in the case of open boundary condition (OBC) and AHBC for vanishing NNN interaction. We show numerically for OBC that edge states in the topologically trivial phase appear for nonvanishing NNN interaction only when the strength of the loss-gain term is greater than the modulus of the difference between the intercell and intracell hopping strengths. In the topologically nontrivial phase, the edge states under OBC exist only up to a critical value of the NNN strength and vanish beyond this critical value. The bulk-boundary correspondence (BBC) for unbroken 𝒫𝒯 phase is similar to Hermitian SSH model, while the non-Hermitian skin effect (NHSE) is observed for broken 𝒫𝒯 phase.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_3: Nonreciprocal hopping induces a synthetic magnetic flux which leads to the non-Hermitian Aharonov-Bohm effect. Since non-Hermitian Hamiltonians possess both real and imaginary eigenvalues, this effect allows the observation of real and imaginary persistent currents in a ring threaded by the synthetic flux. Motivated by this, we investigate the behavior of persistent currents in a disordered Hatano-Nelson ring with anti-Hermitian intradimer hopping. The disorder is diagonal and we explore three distinct models, namely, the Aubry-André-Harper model, the Fibonacci model, both representing correlated disorder, and an uncorrelated (random) model. We conduct a detailed analysis of the energy spectrum and examine the real and imaginary parts of the persistent current under various conditions such as different ring sizes and filling factors. Interestingly, we find that real and imaginary persistent currents exhibit amplification in the presence of correlated disorder. This amplification is also observed in certain individual random configurations but vanishes after configuration averaging. Additionally, we observe both diamagnetic and paramagnetic responses in the current behavior and investigate aspects of persistent currents in the absence of disorder that have not been previously explored. Interestingly, we find that the intradimer bonds host only imaginary currents, while the interdimer bonds carry only real currents. The bulk-boundary correspondence is investigated by analyzing the existence of localized edge states under the open boundary condition.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_4: We numerically verify and analytically prove a winding number invariant that correctly predicts the number of edge states in one-dimensional, nearest-neighbor (between unit cells), two-band models with any complex couplings and open boundaries. Our winding number uses analytical continuation of the wave-vector into the complex plane and involves two special points on the full Riemann surface band structure that correspond to bulk eigenvector degeneracies. Our winding number is invariant under unitary or similarity transforms. We emphasize that the topological criteria we propose here differ from what is traditionally defined as a topological or trivial phase in symmetry-protected classification studies. It is a broader invariant for our model that supports non-zero energy edge states and its transition does not coincide with the gap closing condition. When the relevant symmetries are applied, our invariant reduces to well-known Hermitian and non-Hermitian symmetry-protected topological invariants.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_5: We study the topological properties of the one-dimensional generalized quasiperiodic modulated Su-Schrieffer-Heeger model. The results reveal that the topological re-entrant phase transition emerges. Through the analysis of a real-space winding number, we divide the emergent topological re-entrant phase transitions into two types. The first is the re-entrant phase transition from the traditional topological insulator phase into the topological Anderson insulator phase, and the second is the re-entrant phenomenon from one topological Anderson insulator phase into another topological Anderson insulator phase. These two types of re-entrant phase transitions correspond to bounded and unbounded cases of quasiperiodic modulation, respectively. Furthermore, we verify the above topological re-entrant phase transitions by analyzing the Lyapunov exponent and bulk gap. Since Su-Schrieffer-Heeger models have been realized in various artificial systems (such as cold atoms, optical waveguide arrays, ion traps, Rydberg atom arrays, etc.), the two types of topological re-entrant phase transitions predicted in this paper are expected to be realized in the near future.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_6: This article studies a non-Hermitian Su–Schrieffer–Heeger model which has periodically staggered Hermitian and non-Hermitian dimers. The changes in topological phases of the considered chiral symmetric model with respect to the introduced non-Hermiticity are studied where we find that the system supports only complex eigenspectra for all values of u ≠ 0 and it stabilizes only non-trivial insulating phase for higher loss-gain strength. Even if the system acts as a trivial insulator in the Hermitian limit, the increase in loss-gain strength induces phase transition to non-trivial insulating phase through a (gapless) semi-metallic phase. Interesting phenomenon is observed in the case where Hermitian system acts as a non-trivial insulator. In such a situation, the introduced non-Hermiticity neither leaves the non-trivial phase undisturbed nor induces switching to trivial phase. Rather, it shows transition from non-trivial insulating phase to the same where it is mediated by the stabilization of (non-trivial) semi-metallic phase. This unusual transition between the non-trivial insulating phases through non-trivial semi-metallic phase gives rise to a question regarding the topological states of the system under open boundary conditions. So, we analyze the possibility of stable edge states in these two non-trivial insulating phases and check the characteristic difference between them. In addition, we study the nature of topological states in the case of non-trivial gapless (semi-metallic) region.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_7: We consider non-Hermitian tight-binding one-dimensional Hamiltonians and show that imposing a certain symmetry causes all eigenvalues to pair up and the corresponding eigenstates to coalesce in pairs. This Pairwise Coalescence (PC) is an enhanced version of an exceptional point -- the complete spectrum pairs up, not just one pair of eigenstates. The symmetry is that of reflection excluding the central two sites, and allowing flipping of non-reciprocal hoppings (``generalized off-center reflection symmetry''). Two simple examples of PC exist in the literature -- our construction encompasses these examples and extends them to a vast class of Hamiltonians. We study several families of such Hamiltonians, extend to cases of full-spectrum higher-order coalescences, and show how the PC point corresponds to amplified non-orthogonality of the eigenstates and enhanced loss of norm in time evolution.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_8: The non-Hermitian systems exhibit extreme sensitivity to the boundary conditions. The change in the eigenspectrum with tuning boundary parameter is intimately connected to the non-Hermitian skin effect. The single-particle systems are affected by the boundary perturbations; however, the interplay of a random disorder potential and nonreciprocal hopping under boundary perturbations of an interacting many-body system is not yet clear. In this work, we examine the boundary sensitivity of a non-Hermitian interacting fermionic system in the presence of a random disorder potential. A nonzero boundary parameter results in real-complex spectral transitions with nonreciprocal (or unidirectional) hopping at weak disorder, while the many-body localization at strong disorder washes away real-complex transitions leading to dynamical stability and real eigenvalue spectrum. We show that the boundary-driven real-complex spectral transitions of the non-Hermitian chain are accompanied by the corresponding changes in the level statistics and nearest level-spacing distributions. The intriguing features of nonreciprocity and boundary sensitivity are further revealed using the averaged inverse participation ratios. Finally, we find distinct behavior in the quench dynamics of local particle density, population imbalance, and entanglement entropy of charge-density-wave ordered state that corroborate the real-complex and localization transitions. Our results provide a route to understanding disordered many-body systems under a generalized boundary.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_9: In this paper, we study a non-Hermitian (NH) ladder subjected to a variety of driving protocols. The driven system looses chiral symmetry (CS), whose presence is indispensable for its topological characterization. Further, the bulk boundary correspondence (BBC) gets adversely affected due to the presence of the non-Hermitian skin effect (NHSE). Here, we present a formalism that retrieves the lost CS and subsequently restores the BBC via the construction of a generalized Brillouin zone (GBZ). Specifically, we employ delta and step drives to compare and contrast between them with regard to their impact on NHSE. Further, a widely studied harmonic drive is invoked in this context, not only for the sake of completeness, but its distinct computational framework offers valuable insights on the properties of out-of-equilibrium systems. While the delta and harmonic drives exhibit unidirectional skin effect in the system, the step drive may show a bidirectional skin effect. Also, there are specific points in the parameter space that are devoid of skin effect. These act as critical points that distinguish the skin modes to be localized at one boundary or the other. Moreover, for the computation of the non-Bloch invariants, we employ GBZ via a pair of symmetric time frames corresponding to the delta and step drives, while a high-frequency expansion was carried out to deal with the harmonic drive. Finally, we present phase boundary diagrams that demarcate distinct NH phases obtained via tracking the trajectories of the exceptional points. These diagrams demonstrate a coexistence of the zero and 𝜋 energy modes in the strong NH limit and thus may be relevant for studies of Floquet time crystals.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_10: It is well-known that the standard bulk-boundary correspondence does not hold for non-Hermitian systems in which also new phenomena such as exceptional points do occur. Here we study by analytical and numerical means a paradigmatic one-dimensional non-Hermitian model with dimerization, asymmetric hopping, and imaginary staggered potentials. We present analytical solutions for the singular-value and the eigenspectrum of this model with both open and closed boundary conditions. We explicitly demonstrate that the proper bulk-boundary correspondence is between topological winding numbers in the periodic case and singular values, {it not eigenvalues}, in the open case. These protected singular values are connected to hidden edge modes which only become exact zero-energy eigenmodes in the semi-infinite chain limit. We also show that a non-trivial topology leads to protected eigenvalues in the entanglement spectrum. In the mathcal{PT}-symmetric case, we find that the model has a so far overlooked phase where exceptional points become dense in the thermodynamic limit.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_11: A quench in a Su-Schrieffer-Heeger lattice across the topological boundary initialized with an edge state leads to transport across the chain. We consider such a quench in an effective model in which non-Hermitian components are embedded in an SSH lattice. We find that the transport arising as a result of quench is asymmetric in the sense that there is imbalance in reflection in transport from left and right and this imbalance switches from higher right reflection to higher left reflection as the configuration to which the system is quenched varies in parameter space. We discuss the switching as emergence from the underlying phenomenon of a partial reorganization of bulk states in terms of localization and energy, the intricacies of which depends upon the configuration of the system and the symmetries present.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_12: The development of non-Hermitian (NH) physics in nonconservative systems has led to various interesting wave phenomena, including the emergence of the non-Hermitian skin effect (NHSE). NHSE is a topological effect caused by the nontrivial winding number of the energy spectrum, which results in many bulk eigenmodes localized on low-dimensional boundaries under open boundary conditions. Although this phenomenon is striking, the flexible and dynamic tuning of NHSE has not been reviewed in optics systems. As an ideal NH platform, an optical system can introduce artificially constructed gains and losses and flexibly design optical structures to control the transmission and regulation of light. This exploration based on NH optical systems deepens the understanding of NHSE and facilitates technological breakthroughs in high-energy directional localized optics applications. In this review, the focus is on the latest progress and applications of NHSE in NH optical systems, including the basic theory and local property modulation methods. Finally, possible future research directions for NHSE based on NH optical systems are discussed.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_86_13: The coalescence of eigenstates is a unique phenomenon in non-Hermitian systems. Remarkably, it has been noticed in some non-Hermitian systems under open boundary conditions that the whole set of eigenstates can coalesce to only a few eigenstates. In the parameter space, the point at which such a coalescence of macroscopic eigenstates occurs is dubbed as an infernal point. In this Letter, based on the non-Bloch band theory and amoeba formulation, we establish the criteria for the presence of infernal points in one-dimensional and higher-dimensional open boundary non-Hermitian systems. In addition, we find an explanation for the extreme localization of the wave functions and unveil the mechanism for the coalescence of enormous eigenstates at the infernal points. Our work provides a general theory for infernal points in open boundary non-Hermitian systems in arbitrary dimensions, and hence paves the way to study the intriguing infernal points systematically.","Main_86:   The present work addresses the distinction between the topological properties
of PT symmetric and non-PT symmetric scenarios for the non-Hermitian
Su-Schrieffer-Heeger (SSH) model. The non-PT symmetric case is represented by
non-reciprocity in both the inter- and the intra-cell hopping amplitudes, while
the one with PT symmetry is modeled by a complex on-site staggered potential.
In particular, we study the loci of the exceptional points, the winding
numbers, band structures, and explore the breakdown of bulk-boundary
correspondence (BBC). We further study the interplay of the dimerization
strengths on the observables for these cases. The non-PT symmetric case denotes
a more familiar situation, where the winding number abruptly changes by a
half-integer through tuning of the non-reciprocity parameters, and demonstrates
a complete breakdown of BBC, thereby showing the non-Hermitian skin effect. The
topological nature of the PT symmetric case appears to follow closely to its
Hermitian analogue, except that it shows unbroken (broken) regions with complex
(purely real) energy spectra, while another variant of the winding number
exhibits a continuous behavior as a function of the strength of the potential,
while the conventional BBC is preserved.
",0.65
"Cite_88_1: We study several combinatorial properties of finite groups that are related to the notions of sequenceability, R-sequenceability, and harmonious sequences. In particular, we show that in every abelian group G with a unique involution imath_G there exists a permutation g_0, ldots, g_{m} of elements of G back such that the consecutive sums g_0+g_1, g_1+g_2,ldots, g_{m}+g_0 also form a permutation of elements of G backslash {imath_G}. We also show that in every abelian group of order at least 4 there exists a sequence containing each non-identity element of G exactly twice such that the consecutive sums also contain each non-identity element of G twice. We apply several results to the existence of transversals in Latin squares.","Main_88:   Given a sequence ${\bf g}: g_0,\ldots, g_{m}$, in a finite group $G$ with
$g_0=1_G$, let ${\bf \bar g}: \bar g_0,\ldots, \bar g_{m}$, be the sequence
defined by $\bar g_0=1_G$ and $\bar g_i=g_{i-1}^{-1}g_i$ for $1\leq i \leq m$.
We say that $G$ is doubly sequenceable if there exists a sequence ${\bf g}$ in
$G$ such that every element of $G$ appears exactly twice in each of ${\bf g}$
and ${\bf \bar g}$. If a group $G$ is abelian, odd, sequenceable,
R-sequenceable, or terraceable, then $G$ is doubly sequenceable. In this paper,
we show that if $N$ is an odd or sequenceable group and $H$ is an abelian
group, then $N \times H$ is doubly sequenceable.
",0.05
"Cite_89_1: The Matérn model has been a cornerstone of spatial statistics for more than half a century. More recently, the Matérn model has been exploited in disciplines as diverse as numerical analysis, approximation theory, computational statistics, machine learning, and probability theory. In this article, we take a Matérn-based journey across these disciplines. First, we reflect on the importance of the Matérn model for estimation and prediction in spatial statistics, establishing also connections to other disciplines in which the Matérn model has been influential. Then, we position the Matérn model within the literature on big data and scalable computation: the SPDE approach, the Vecchia likelihood approximation, and recent applications in Bayesian computation are all discussed. Finally, we review recent devlopments, including flexible alternatives to the Matérn model, whose performance we compare in terms of estimation, prediction, screening effect, computation, and Sobolev regularity properties.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_89_2: This paper tackles efficient methods for Bayesian inverse problems with priors based on Whittle–Matérn Gaussian random fields. The Whittle–Matérn prior is characterized by a mean function and a covariance operator that is taken as a negative power of an elliptic differential operator. This approach is flexible in that it can incorporate a wide range of prior information including nonstationary effects, but it is currently computationally advantageous only for integer values of the exponent. In this paper, we derive an efficient method for handling all admissible noninteger values of the exponent. The method first discretizes the covariance operator using finite elements and quadrature, and uses preconditioned Krylov subspace solvers for shifted linear systems to efficiently apply the resulting covariance matrix to a vector. This approach can be used for generating samples from the distribution in two different ways: by solving a stochastic partial differential equation, and by using a truncated Karhunen–Loève expansion. We show how to incorporate this prior representation into the infinite-dimensional Bayesian formulation, and show how to efficiently compute the maximum a posteriori estimate, and approximate the posterior variance. Although the focus of this paper is on Bayesian inverse problems, the techniques developed here are applicable to solving systems with fractional Laplacians and Gaussian random fields. Numerical experiments demonstrate the performance and scalability of the solvers and their applicability to model and real-data inverse problems in tomography and a time-dependent heat equation.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_89_3: We consider the numerical approximation of Gaussian random fields on closed surfaces defined as the solution to a fractional stochastic partial differential equation (SPDE) with additive white noise. The SPDE involves two parameters controlling the smoothness and the correlation length of the Gaussian random field. The proposed numerical method relies on the Balakrishnan integral representation of the solution and does not require the approximation of eigenpairs. Rather, it consists of a sinc quadrature coupled with a standard surface finite element method. We provide a complete error analysis of the method and illustrate its performances in several numerical experiments.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_89_4: Bayesian models based on Gaussian processes (GPs) offer a flexible framework to predict spatially distributed variables with uncertainty. But the use of nonstationary priors, often necessary for capturing complex spatial patterns, makes sampling from the predictive posterior distribution (PPD) computationally intractable. In this paper, we propose a two-step approach based on diffusion generative models (DGMs) to mimic PPDs associated with non-stationary GP priors: we replace the GP prior by a DGM surrogate, and leverage recent advances on training-free guidance algorithms for DGMs to sample from the desired posterior distribution. We apply our approach to a rich non-stationary GP prior from which exact posterior sampling is untractable and validate that the issuing distributions are close to their GP counterpart using several statistical metrics. We also demonstrate how one can fine-tune the trained DGMs to target specific parts of the GP prior. Finally we apply the proposed approach to solve inverse problems arising in environmental sciences, thus yielding state-of-the-art predictions.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_89_5: The stochastic heat equation on the sphere driven by additive Lévy random field is approximated by a spectral method in space and forward and backward Euler-Maruyama schemes in time, in analogy to the Wiener case. New regularity results are proven for the stochastic heat equation. The spectral approximation is based on a truncation of the series expansion with respect to the spherical harmonic functions. To do so, we restrict to square-integrable random field and optimal strong convergence rates for a given regularity of the initial condition and two different settings of regularity for the driving noise are derived for the Euler-Maruyama methods. Besides strong convergence, convergence of the expectation and second moment is shown. Weak rates for the spectral approximation are discussed. Numerical simulations confirm the theoretical results.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
Cite_89_6:  flexible model for non-stationary Gaussian random fields on hypersurfaces is this http URL class of random fields on curves and surfaces is characterized by an amplitude spectral density of a second order elliptic differential this http URL is done by a Galerkin--Chebyshev approximation based on the surface finite element method and Chebyshev polynomials. Strong error bounds are shown with convergence rates depending on the smoothness of the approximated random field. Numerical experiments that confirm the convergence rates are presented.,"Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_89_7: The aim of this work is to propose a statistical model for spatio-temporal data on meshed surfaces based on the SPDE modeling approach. To do so, we consider a class of advection-diffusion SPDEs defined on smooth compact orientable closed Riemannian manifolds of dimension 2, and their discretization using a Galerkin approach. We show how this approach allows to easily propose scalable algorithms for the simulation and prediction of Gaussian random fields that are solutions to the discretized SPDE.","Main_89:   A new numerical approximation method for a class of Gaussian random fields on
compact connected oriented Riemannian manifolds is introduced. This class of
random fields is characterized by the Laplace--Beltrami operator on the
manifold. A Galerkin approximation is combined with a polynomial approximation
using Chebyshev series. This so-called Galerkin--Chebyshev approximation scheme
yields efficient and generic sampling algorithms for Gaussian random fields on
manifolds. Strong and weak orders of convergence for the Galerkin approximation
and strong convergence orders for the Galerkin--Chebyshev approximation are
shown and confirmed through numerical experiments.
",0.35
"Cite_90_1: Blockchain-based decentralized applications (DApps) promise trust and transparency as their execution can be verified publicly and do not rely on trusted third parties. To this end, DApps leverage smart contracts and immutable blockchain storage that allow all participants to verify their correct execution. This makes it possible to verify all DApp interactions and corresponding blockchain transactions, including past ones, and thus prevents subsequent manipulations without being noticed. Unlike traditional legal contracts and centralized web applications, a smart contract ensures that agreements must be executed as implemented, and invalid interactions will be rejected. In particular, there are no designated third parties or other intermediary instances that can violate a smart contract agreement without being explicitly permitted to do so by its implementation. Moreover, smart contracts will automatically execute agreements once their terms and conditions are met.","Main_90:   Modern computer systems tend to rely on large trusted computing bases (TCBs)
for operations. To address the TCB bloating problem, hardware vendors have
developed mechanisms to enable or facilitate the creation of a trusted
execution environment (TEE) in which critical software applications can execute
securely in an isolated environment. Even under the circumstance that a host OS
is compromised by an adversary, key security properties such as confidentiality
and integrity of the software inside the TEEs can be guaranteed. The promise of
integrity and security has driven developers to adopt it for use cases
involving access control, PKS, IoT among other things.
  Among these applications include blockchain-related use cases. The usage of
the TEEs doesn't come without its own implementation challenges and potential
pitfalls. In this paper, we examine the assumptions, security models, and
operational environments of the proposed TEE use cases of blockchain-based
applications. The exercise and analysis help the hardware TEE research
community to identify some open challenges and opportunities for research and
rethink the design of hardware TEEs in general.
",0.05
"Cite_92_1: Linear integrators are well known for their ability to counter static forces and improve low-frequency disturbance rejection properties in control systems. However, linear integrators introduce phase lag, which is a frequencydependent time shift or delay. Since the early introduction of the Clegg integrator, nonlinear integrators have held the promise of providing phase advantages over linear integrators when evaluated from the perspective of a describing function. This could potentially reduce delay and therefore provide the means to surpass linear design limitations; for example, overshoot and settling times can be reduced or even avoided. In addition, loop gains can be increased, which improves the low-frequency disturbance rejection properties. For five nonlinear integrato of recent developments in stage motion control. Benchmark examples are taken from the industrial practice of wafer scanners, which form the pivotal machines used in the manufacturing of computer chips.","Main_92:   According to the well-known loop shaping method for the design of
controllers, the performance of the controllers in terms of step response,
steady-state disturbance rejection and noise attenuation and robustness can be
improved by increasing the gain at lower frequencies and decreasing it at
higher frequencies and increasing the phase margin as much as possible.
However, the inherent properties of linear controllers, the Bode's phase-gain
relation, create a limitation. In theory, a complex-order transfer function can
break the Bode's gain-phase relation; however, such transfer function cannot be
directly implemented and should be approximated. This paper proposes a reset
element and a tuning method to approximate a Complex-Order Controller (CLOC)
and, through a simulation example, shows the benefits of using such a
controller.
",0.2
"Cite_92_2: This research focuses on the fractional complex order plant (FCOP). The significant contribution is the role of complex plant models in system stability and robustness and associated physical phenomena. A general transfer function is studied in the paper. Other plant models may be built with this structure since the FCOP is a general mathematical form covering integer order plant (IOP) and fractional order plant (FOP). Using the equations produced with the proposed technique and the recommended integer order proportional derivative (IOPD controller, physical changes in integer, fractional and complex coefficients, and orders are observed within this paper. Analysis of the plant controlled with an IOPD controller is done by applying an integrator to reveal the differences. The effects of the parameters are discussed together with the visuals, supported by simulations. The aim is to tune the controller parameters to achieve the phase and specifications as the researcher desired. It is observed that the integrator greatly takes part in reducing the steady-state error. The IOP with the integrator showed the lowest steady-state error, and also, the settling and overshoot time were enhanced. Increase in the phase margin also caused an increase in the phase crossover frequency. It is also observed that the fractional order affected the phase crossover frequency comparing with the IOP, and the complex order modification also had an effect comparing to the fractional order version. The complex order of the system is considered with its conjugate components in the imaginary part thus, the results are found separately for each case.","Main_92:   According to the well-known loop shaping method for the design of
controllers, the performance of the controllers in terms of step response,
steady-state disturbance rejection and noise attenuation and robustness can be
improved by increasing the gain at lower frequencies and decreasing it at
higher frequencies and increasing the phase margin as much as possible.
However, the inherent properties of linear controllers, the Bode's phase-gain
relation, create a limitation. In theory, a complex-order transfer function can
break the Bode's gain-phase relation; however, such transfer function cannot be
directly implemented and should be approximated. This paper proposes a reset
element and a tuning method to approximate a Complex-Order Controller (CLOC)
and, through a simulation example, shows the benefits of using such a
controller.
",0.2
"Cite_92_3: This study presents a shaped reset feedback control strategy to enhance the performance of precision motion systems. The approach utilizes a phase-lead compensator as a shaping filter to tune the phase of reset instants, thereby shaping the nonlinearity in the first-order reset control. {The design achieves either an increased phase margin while maintaining gain properties or improved gain without sacrificing phase margin, compared to reset control without the shaping filter.} Then, frequency-domain design procedures are provided for both Clegg Integrator (CI)-based and First-Order Reset Element (FORE)-based reset control systems. Finally, the effectiveness of the proposed strategy is demonstrated through two experimental case studies on a precision motion stage. In the first case, the shaped reset control leverages phase-lead benefits to achieve zero overshoot in the transient response. In the second case, the shaped reset control strategy enhances the gain advantages of the previous reset element, resulting in improved steady-state performance, including better tracking precision and disturbance rejection, while reducing overshoot for an improved transient response.","Main_92:   According to the well-known loop shaping method for the design of
controllers, the performance of the controllers in terms of step response,
steady-state disturbance rejection and noise attenuation and robustness can be
improved by increasing the gain at lower frequencies and decreasing it at
higher frequencies and increasing the phase margin as much as possible.
However, the inherent properties of linear controllers, the Bode's phase-gain
relation, create a limitation. In theory, a complex-order transfer function can
break the Bode's gain-phase relation; however, such transfer function cannot be
directly implemented and should be approximated. This paper proposes a reset
element and a tuning method to approximate a Complex-Order Controller (CLOC)
and, through a simulation example, shows the benefits of using such a
controller.
",0.2
"Cite_92_4: Fractional-order calculus has been widely employed in recent years in many areas of engineering. This however has been with real-valued orders; i.e. using the operator α in the frequency domain (s=jω) with α being a real number. Mathematically, the order of an integral/derivative is not restricted to real-value orders and can generally be a complex number. In this work, we derive the time-domain expression for the integral/derivative of the simple linear function f(t)=t and verify this expression experimentally using a Field Programmable Analog Array (FPAA) after employing a second-order approximation of the operator s±(α+jβ).","Main_92:   According to the well-known loop shaping method for the design of
controllers, the performance of the controllers in terms of step response,
steady-state disturbance rejection and noise attenuation and robustness can be
improved by increasing the gain at lower frequencies and decreasing it at
higher frequencies and increasing the phase margin as much as possible.
However, the inherent properties of linear controllers, the Bode's phase-gain
relation, create a limitation. In theory, a complex-order transfer function can
break the Bode's gain-phase relation; however, such transfer function cannot be
directly implemented and should be approximated. This paper proposes a reset
element and a tuning method to approximate a Complex-Order Controller (CLOC)
and, through a simulation example, shows the benefits of using such a
controller.
",0.2
"Cite_93_1: We consider a class of stochastic optimal transport, SOT for short, with given two endpoint marginals in the case where a cost function exhibits at most quadratic growth. We first study the upper and lower estimates, the short-time asymptotics, the zero-noise limits, and the explosion rate as time goes to infinity of SOT. We also show that the value function of SOT is equal to zero or infinity in the case where a cost function exhibits less than linear growth. As a by-product, we characterize the finiteness of the value function of SOT by that of the Monge–Kantorovich problem. As an application, we show the existence of a continuous semimartingale, with given initial and terminal distributions, of which the drift vector is rth integrable for We also consider the same problem for Schrödinger’s problem where . This paper is a continuation of our previous work.","Main_93:   We study the Lagrangian formulation of a class of the Monge-Kantorovich
optimal transportation problem. It can be considered a stochastic optimal
transportation problem for absolutely continuous stochastic processes. A cost
function and stochastic processes under consideration is not convex and have
essentially bounded time derivatives almost surely, respectively. This paper is
a continuation of the second author's master thesis.
",0.05
"Cite_94_1: This work is concerned with a coupled system of focusing nonlinear Schrödinger equations involving general power-type nonlinearities in the energy-critical setting for dimensions 3 leq d leq 5 in the radial setting. Our aim is to demonstrate the scattering versus blow-up dichotomy in the radial case. To achieve this, we first prove the existence of ground state solutions using the concentration-compactness method combined with variational techniques. We then establish finite-time blow-up through a convexity argument and prove scattering by applying the concentration-compactness and rigidity method.","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_94_2: In this paper, we consider a nonlinear Schrödinger system with quadratic interaction. We extend the recent results of Fukaya et al. (Math. Ann. 2024) and show that the system has a ground state in when the mass parameter is larger than .","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_94_3: In this paper, we consider a nonlinear Schrödinger system with fractional derivative and quadratic interaction. Using constrained variational methods, we prove that this problem has a prescribed solution. Our contribution is that we can deal with a case in the supercritical range.","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_94_4: In this paper, we study the solutions to the energy-critical quadratic nonlinear Schrödinger system in, where the sign of its potential energy can not be determined directly. If the initial data is radial or non-radial but satisfies the mass-resonance condition, and its energy is below that of the ground state, using the compactness/rigidity method, we give a complete classification of scattering versus blowing-up dichotomies depending on whether the kinetic energy of is below or above that of the ground state.","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_94_5: In this paper, we consider three components system of nonlinear Schrödinger equations related to the Raman amplification in a plasma. By using variational method, a new result on the existence of traveling wave solutions are obtained under the non-mass resonance condition. We also study the new global existence result for oscillating data. Both of our results essentially due to the absence of Galilean symmetry in the system.","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_94_6: We consider the Cauchy problem of the system of nonlinear Schrödinger equations with derivative nonlinearlity. This system was introduced by Colin and Colin (Differ Int Equ 17:297–330, 2004) as a model of laser-plasma interactions. We study existence of ground state solutions and the global well-posedness of this system by using the variational methods. We also consider the stability of traveling waves for this system. These problems are proposed by Colin–Colin as the open problems. We give a subset of the ground-states set which satisfies the condition of stability. In particular, we prove the stability of the set of traveling waves with small speed for 1-dimension.","Main_94:   We study traveling wave solutions for a nonlinear Schrodinger system with
quadratic interaction. For the non mass resonance case, the system has no
Galilean symmetry, which is of particular interest in this paper. We construct
traveling wave solutions by variational methods and see that for the non mass
resonance case there exist specific traveling wave solutions which correspond
to the solutions for ``zero mass"" case in nonlinear elliptic equations. We also
establish the new global existence result for oscillating data as an
application. Both of our results essentially come from the lack of Galilean
invariance in the system.
",0.3
"Cite_95_1: Image-to-image translation (I2I) transforms an image from a source domain to a target domain while preserving source content. Most computer vision applications are in the field of image-to-image translation, such as style transfer, image segmentation, and photo enhancement. The degree of preservation of the content of the source images in the translation process can be different according to the problem and the intended application. From this point of view, in this paper, we divide the different tasks in the field of image-to-image translation into three categories: Fully Content preserving, Partially Content preserving, and Non-Content preserving. We present different tasks, datasets, methods, results of methods for these three categories in this paper. We make a categorization for I2I methods based on the architecture of different models and study each category separately. In addition, we introduce well-known evaluation criteria in the I2I translation field. Specifically, nearly 70 different I2I models were analyzed, and more than 10 quantitative evaluation metrics and 30 distinct tasks and datasets relevant to the I2I translation problem were both introduced and assessed. Translating from simulation to real images could be well viewed as an application of fully content preserving or partially content preserving unsupervised image-to-image translation methods. So, we provide a benchmark for Sim-to-Real translation, which can be used to evaluate different methods. In general, we conclude that because of the different extent of the obligation to preserving content in various applications, it is better to consider this issue in choosing a suitable I2I model for a specific application.","Main_95:   Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate
a source image to a target image domain with the style of a target image
exemplar, without ground-truth input-translation pairs. Existing UEI2I methods
represent style using either a global, image-level feature vector, or one
vector per object instance/class but requiring knowledge of the scene
semantics. Here, by contrast, we propose to represent style as a dense feature
map, allowing for a finer-grained transfer to the source image without
requiring any external semantic information. We then rely on perceptual and
adversarial losses to disentangle our dense style and content representations,
and exploit unsupervised cross-domain semantic correspondences to warp the
exemplar style to the source content. We demonstrate the effectiveness of our
method on two datasets using standard metrics together with a new localized
style metric measuring style similarity in a class-wise manner. Our results
evidence that the translations produced by our approach are more diverse and
closer to the exemplars than those of the state-of-the-art methods while
nonetheless preserving the source content.
",0.15
"Cite_95_2: Stain normalization is a key preprocessing step that has been shown to significantly improve the segmentation and classification performance of computer-aided diagnosis (CAD) systems. In recent advancements, numerous approaches have demonstrated significant progress in the domain of stain normalization; however, the most of these approaches are based on Generative Adversarial Networks. In this paper, we propose a novel vision transformer-based model, termed as StainSWIN, that combines the strengths of swin transformer and the architecture of super resolution to achieve improved performance in stain normalization task. The key concept behind the StainSWIN is the utilization of swin transformer blocks that exploit content-based interactions to capture long-range dependencies. The proposed model is equipped with two key blocks, including residual stain swin block (ResStainSWIN) and swin transformer block (STB). The StainSWIN has a residual super resolution architecture, in which the high-level features, extracted by STB, are combined to ResStainSWIN block. The performance of the StainSWIN model was compared with other state-of-the-art methods on a commonly used MITOS-ATYPIA14 histopathology dataset. The StainSWIN outperformed other methods in stain normalization with a large margin in terms of PSNR, SSIM, and RMSE metrics. The StainSWIN model achieved PSNR of 26.667 ± 3.492, SSIM of 0.943 ± 0.037, and RMSE of 6.206 ± 1.973. Additionally, we evaluated the model’s impact to the segmentation performance of the MICCAI GlaS’16 dataset. The results demonstrates a 4.3% improvement in segmentation accuracy, attributed to a reduction in stain color variation. The proposed method has the ability to greatly assist CAD systems in maintaining consistent performance despite color variations.","Main_95:   Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate
a source image to a target image domain with the style of a target image
exemplar, without ground-truth input-translation pairs. Existing UEI2I methods
represent style using either a global, image-level feature vector, or one
vector per object instance/class but requiring knowledge of the scene
semantics. Here, by contrast, we propose to represent style as a dense feature
map, allowing for a finer-grained transfer to the source image without
requiring any external semantic information. We then rely on perceptual and
adversarial losses to disentangle our dense style and content representations,
and exploit unsupervised cross-domain semantic correspondences to warp the
exemplar style to the source content. We demonstrate the effectiveness of our
method on two datasets using standard metrics together with a new localized
style metric measuring style similarity in a class-wise manner. Our results
evidence that the translations produced by our approach are more diverse and
closer to the exemplars than those of the state-of-the-art methods while
nonetheless preserving the source content.
",0.15
"Cite_95_3: Unsupervised Semantic Segmentation (USS) involves segmenting images without relying on predefined labels, aiming to alleviate the burden of extensive human labeling. Existing methods utilize features generated by self-supervised models and specific priors for clustering. However, their clustering objectives are not involved in the optimization of the features during training. Additionally, due to the lack of clear class definitions in USS, the resulting segments may not align well with the clustering objective. In this paper, we introduce a novel approach called Optimally Matched Hierarchy (OMH) to simultaneously address the above issues. The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity. The structure of this sparsity stems from our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy among parallel clusters through Optimal Transport. Our OMH yields better unsupervised segmentation performance compared to existing USS methods. Our extensive experiments demonstrate the benefits of OMH when utilizing our differentiable paradigm. We will make our code publicly available.","Main_95:   Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate
a source image to a target image domain with the style of a target image
exemplar, without ground-truth input-translation pairs. Existing UEI2I methods
represent style using either a global, image-level feature vector, or one
vector per object instance/class but requiring knowledge of the scene
semantics. Here, by contrast, we propose to represent style as a dense feature
map, allowing for a finer-grained transfer to the source image without
requiring any external semantic information. We then rely on perceptual and
adversarial losses to disentangle our dense style and content representations,
and exploit unsupervised cross-domain semantic correspondences to warp the
exemplar style to the source content. We demonstrate the effectiveness of our
method on two datasets using standard metrics together with a new localized
style metric measuring style similarity in a class-wise manner. Our results
evidence that the translations produced by our approach are more diverse and
closer to the exemplars than those of the state-of-the-art methods while
nonetheless preserving the source content.
",0.15
"Cite_96_1: In a V2X environment, the target vehicle is capable of acquiring motion information from multiple vehicles ahead, and this information plays a crucial role in predicting the target vehicle's motion behavior. To better understand how leading vehicles affect the car-following behavior of the target vehicle, we have developed an improved car-following model that considers the effects of multiple leading vehicles. This model constructs distance-based and field-based models to describe the influence weights of different vehicles on the target vehicle. The stability conditions of the model were obtained through linear stability analysis, and the mKdV equation, which describes the evolution characteristics of traffic density waves in congested areas, was determined through nonlinear analysis. Numerical simulations were conducted to discuss the multi-vehicle effect, different vehicle influence weights (distance-based and field-based models), and the number of leading vehicles considered. The study found that in congested areas, initial perturbations evolve in the form of kink-antikink waves, moving rearward, with the amplitude of the headway curve decreasing as the multi-vehicle effect coefficient value increases. For different vehicle influence weights, the field-based model outperforms the distance-based model. Moreover, as the number of leading vehicles considered increases, the stability of traffic flow gradually improves. The numerical results are consistent with the theoretical findings, and it is noted that the model successfully enhances vehicular movement efficiency, reduces congestion, and improves road safety. To minimize collision incidents, the improved model can be implemented as an active safety technology.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_2: To optimize the mixed car-following behavior of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), an adaptive backstepping sliding mode control (ABSMC) strategy for longitudinal velocity and distance control model (namely, MCF-ABSMCM) is put forth. First, the variable desired headway model (VDHM) is designed based on the vehicle-to-vehicle and vehicle-to-infrastructure information. Also, an asymmetric stochastic car-following model (ASCM) is designed to realistically describe the stochastic factor as well as vehicle acceleration and braking behaviors in mixed car-following. Second, an adaptive cruise sliding mode control law with a backstepping approach is devised to precisely track the desired following distance based on the VDHM. After creating the basic diagram of the two-model traffic flow, the intelligent driver model (IDM) is used as a baseline to compare and analyze the traffic capacity. Lastly, the effectiveness of the MCF-ABSMCM approach is confirmed in terms of both the mixed traffic flow density wave evolution and the vehicle control strategy, and the MCF-ABSMCM is validated under various adversary inputs. The simulation results demonstrate that the mixed-flow queue can finish headway tracking and keep a safe distance when using the MCF-ABSMCM described. This control strategy has been validated in VISSIM to significantly improve the efficiency of traffic operations compared to IDM and ASCM.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_3: Connected automated vehicles (CAVs) have the potential to improve the efficiency of vehicular traffic. In this paper, we discuss how CAVs can positively impact the dynamic behavior of mixed traffic systems on highways through the lens of nonlinear dynamics theory. First, we show that human-driven traffic exhibits a bistability phenomenon, in which the same drivers can both drive smoothly or cause congestion, depending on perturbations like a braking of an individual driver. As such, bistability can lead to unexpected phantom traffic jams, which are undesired. By analyzing the corresponding nonlinear dynamical model, we explain the mechanism of bistability and identify which human driver parameters may cause it. Second, we study mixed traffic that includes both human drivers and CAVs, and we analyze how CAVs affect the nonlinear dynamic behavior. We show that a large-enough penetration of CAVs in the traffic flow can eliminate bistability, and we identify the controller parameters of CAVs that are able to do so. Ultimately, this helps to achieve stable and smooth mobility on highways.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_4: Recent research has paid little attention to complex driving behaviors, namely merging car-following and lanechanging behavior, and how lane-changing affects algorithms designed to model and control a car-following vehicle. During the merging behavior, the Follower Vehicle (FV) might significantly diverge from typical car-following models. Thus, this paper aims to control the FV witnessing lane-changing behavior based on anticipation, perception, preparation, and relaxation states defined by a novel measurable human perception index. Data from human drivers are utilized to create a perception-based fuzzy controller for the behavior vehicle's route guidance, taking into account the opacity of human driving judgments. We illustrate the efficacy of the established technique using simulated trials and data from actual drivers, focusing on the benefits of the increased comfort, safety, and uniformity of traffic flow and the decreased of wait time and motion sickness this brings about.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_5: In the process of autonomous vehicle lane changing, a reliable decision-making system is crucial for driving safety and comfort. However, traditional decision-making systems have short-term characteristics, which makes them susceptible to real-time inference from surrounding vehicles. Usually, system sacrifices driving comfort to ensure the safety of the lane change. Balancing driving safety and comfort has always been a research challenge. Long-term trajectory prediction can provide accurate future trajectories of target vehicles, providing reliable long-term information to compensate for the short-term variability of decision systems. This paper proposes a novel decision-making model with long-term trajectory prediction for lane-changing. First, we constructed a long-term trajectory prediction model to predict the trajectories of surrounding vehicles. Besides, we built a lane change decision-making model based on fuzzy inferencing, considering the predicted trajectories to infer the relative relationship between other vehicles and the self-driving car. The establishment of the fuzzy rule library considered the vehicle speed, acceleration, system delay time, driver delay time and the distance between vehicles. Finally, we created a dataset for training and testing the trajectory prediction model, and we built 4 cases simulation environments, for two or three vehicles on a straight road or curved road, respectively, to test the decision-making model. Experimental results show that our proposed model can ensure driving safety and improve driving comfort.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_6: Driving risk field (DRF) emerges as an effective way to assess the driving safety of connected and automated vehicles (CAVs). Most existing DRF models are established from the so-called birds-eye-view (BEV), which limits their accuracy for distributed vehicle-level tasks such as trajectory planning since the interactions between ego vehicle (EV) and its surrounding traffic environment have not been fully considered. To fill this research gap, we establish a novel DRF model from ego-vehicle-view (EVV) and apply it in trajectory planning in this paper. Firstly, the collision boundary between EV and its surrounding obstacles is defined by introducing the elliptical model to fully consider the geometry characteristics of vehicles. Secondly, the relative motion influence coefficient is designed to accurately characterize the relative motion between EV and obstacles, instead of using only basic driving state information such as location and velocity. On this basis, the unified DRF is established from EVV for driving safety assessment, which contains vehicle risk field (VRF) and lane marking risk field (LMRF). Based on the established DRF model, we then design a rolling trajectory planning method (RTPM) with a rolling horizon strategy, which not only ensures a long prediction horizon but also effectively reduces the computational complexity. Multiple simulation results under different traffic scenarios jointly verify the accuracy and applicability of the proposed RTPM and DRF model established from this new perspective.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_7: Vehicle-following control is a crucial component of adaptive cruise control systems, playing a pivotal role in ensuring safe autonomous driving. Maintaining a reasonable distance between vehicles (DBV) is a prerequisite for safe vehicle following, with time headway (TH) being a key factor in ensuring this safe distance. To enhance the performance of vehicle following in diverse driving scenarios, this paper proposes a model-predictive-control-based (MPC) vehicle-following control scheme utilizing the continuous synthesis variable time headway (CSVTH) model. First, this paper introduces a novel CSVTH model based on existing TH models, and the sigmoid function is used as the transition function to realize the unique capability of seamlessly transitioning between different TH models according to the DBV, ensuring optimal performance in various driving scenarios. Second, the MPC algorithm based on incremental control is employed as the upper controller. By utilizing fuzzy reasoning to adjust the weight of the MPC algorithm, the adaptability of the controller to the different working conditions is enhanced. Finally, the lower controller is constructed within the simulation environment. The rationality of the proposed CSVTH model is verified through simulation experiments across three distinct driving conditions. Compared with the existing three TH models, the proposed CSVTH model improves economy by 22.7%, vehicle-following performance by 38.8%, and comfort by 15.3%.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_8: Recent research has paid little attention to complex driving behaviors, namely merging car-following and lane-changing behavior, and how lane-changing affects algorithms designed to model and control a car-following vehicle. During the merging behavior, the Follower Vehicle (FV) might significantly diverge from typical car-following models. Thus, this paper aims to control the FV witnessing lane-changing behavior based on anticipation, perception, preparation, and relaxation states defined by a novel measurable human perception index. Data from human drivers are utilized to create a perception-based fuzzy controller for the behavior vehicle's route guidance, taking into account the opacity of human driving judgments. We illustrate the efficacy of the established technique using simulated trials and data from actual drivers, focusing on the benefits of the increased comfort, safety, and uniformity of traffic flow and the decreased of wait time and motion sickness this brings about.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
"Cite_96_9: This paper presents a novel consensus control algorithm for a vehicle platoon, taking into account human physiological-psychological comfort. To this end, this paper incorporates state constraints into the optimal velocity-based platoon controller, which can effectively improve the comfort and consensus. Moreover, this paper employs the barrier Lyapunov function (BLF) to prove the stability of the proposed controller, providing more strict stability guarantees. According to theoretical analysis, numerical experiments are conducted to demonstrate the performance of the proposed control algorithm and the benchmark algorithm. The comparative results show that the proposed control algorithm can produce better-uniformed states and bounded state errors.","Main_96:   This paper investigates the car-following problem and proposes a nonlinear
controller that considers driving comfort, safety concerns, steady-state
response and transient response. This controller is designed based on the
demands of lower cost, faster response, increased comfort, enhanced safety and
elevated extendability from the automotive industry. Design insights and
intuitions are provided in detail. Also, theoretical analysis are performed on
plant stability, string stability and tracking performance of the closed-loop
system. Conditions and guidelines are provided on the selection of control
parameters. Comprehensive simulations are conducted to demonstrate the efficacy
of the proposed controller in different driving scenarios.
",0.45
Cite_97_1: ,"Main_97:   Parkinson's disease is marked by altered and increased firing characteristics
of pathological oscillations in the brain. In other words, it causes abnormal
synchronous oscillations and suppression during neurological processing. In
order to examine and regulate the synchronization and pathological oscillations
in motor circuits, deep brain stimulators (DBS) are used. Although machine
learning methods have been applied for the investigation of suppression, these
models require large amounts of training data and computational power, both of
which pose challenges to resource-constrained DBS. This research proposes a
novel reinforcement learning (RL) framework for suppressing the synchronization
in neuronal activity during episodes of neurological disorders with less power
consumption. The proposed RL algorithm comprises an ensemble of a temporal
representation of stimuli and a twin-delayed deep deterministic (TD3) policy
gradient algorithm. We quantify the stability of the proposed framework to
noise and reduced synchrony using RL for three pathological signaling regimes:
regular, chaotic, and bursting, and further eliminate the undesirable
oscillations. Furthermore, metrics such as evaluation rewards, energy supplied
to the ensemble, and the mean point of convergence were used and compared to
other RL algorithms, specifically the Advantage actor critic (A2C), the Actor
critic with Kronecker-featured trust region (ACKTR), and the Proximal policy
optimization (PPO).
",0.15
"Cite_97_2: Deep Brain Stimulation (DBS) is effective for movement disorders, particularly Parkinson’s disease (PD). However, a closed-loop DBS system using reinforcement learning (RL) for automatic parameter tuning, offering enhanced energy efficiency and the effect of thalamus restoration, is yet to be developed for clinical and commercial applications. In this research, we instantiate a basal ganglia-thalamic (BGT) model and design it as an interactive environment suitable for RL models. Four finely tuned RL agents based on different frameworks, namely Soft Actor-Critic (SAC), Twin Delayed Deep Deterministic Policy Gradient (TD3), Proximal Policy Optimization (PPO), and Advantage Actor-Critic (A2C), are established for further comparison. Within the implemented RL architectures, the optimized TD3 demonstrates a significant 67% reduction in average power dissipation when compared to the open-loop system while preserving the normal response of the simulated BGT circuitry. As a result, our method mitigates thalamic error responses under pathological conditions and prevents overstimulation. In summary, this study introduces a novel approach to implementing an adaptive parameter-tuning closed-loop DBS system. Leveraging the advantages of TD3, our proposed approach holds significant promise for advancing the integration of RL applications into DBS systems, ultimately optimizing therapeutic effects in future clinical trials.","Main_97:   Parkinson's disease is marked by altered and increased firing characteristics
of pathological oscillations in the brain. In other words, it causes abnormal
synchronous oscillations and suppression during neurological processing. In
order to examine and regulate the synchronization and pathological oscillations
in motor circuits, deep brain stimulators (DBS) are used. Although machine
learning methods have been applied for the investigation of suppression, these
models require large amounts of training data and computational power, both of
which pose challenges to resource-constrained DBS. This research proposes a
novel reinforcement learning (RL) framework for suppressing the synchronization
in neuronal activity during episodes of neurological disorders with less power
consumption. The proposed RL algorithm comprises an ensemble of a temporal
representation of stimuli and a twin-delayed deep deterministic (TD3) policy
gradient algorithm. We quantify the stability of the proposed framework to
noise and reduced synchrony using RL for three pathological signaling regimes:
regular, chaotic, and bursting, and further eliminate the undesirable
oscillations. Furthermore, metrics such as evaluation rewards, energy supplied
to the ensemble, and the mean point of convergence were used and compared to
other RL algorithms, specifically the Advantage actor critic (A2C), the Actor
critic with Kronecker-featured trust region (ACKTR), and the Proximal policy
optimization (PPO).
",0.15
"Cite_97_3: Adaptive deep brain stimulation (aDBS) has emerged as a promising treatment for Parkinson disease (PD). In aDBS, a surgically placed electrode sends dynamically altered stimuli to the brain based on neurophysiological feedback: an invasive gadget that limits the amount of data one could collect for optimizing the control offline. As a consequence, a plethora of synthetic models of PD and those of the control algorithms have been proposed. Herein, we introduce the first neurophysiologically realistic benchmark for comparing said models. Specifically, our methodology covers not only conventional basal ganglia circuit dynamics and pathological oscillations, but also captures 15 previously dismissed physiological attributes, such as signal instabilities and noise, neural drift, electrode conductance changes and individual variability - all modeled as spatially distributed and temporally registered features via beta-band activity in the brain and a feedback. Furthermore, we purposely built our framework as a structured environment for training and evaluating deep reinforcement learning (RL) algorithms, opening new possibilities for optimizing aDBS control strategies and inviting the machine learning community to contribute to the emerging field of intelligent neurostimulation interfaces.","Main_97:   Parkinson's disease is marked by altered and increased firing characteristics
of pathological oscillations in the brain. In other words, it causes abnormal
synchronous oscillations and suppression during neurological processing. In
order to examine and regulate the synchronization and pathological oscillations
in motor circuits, deep brain stimulators (DBS) are used. Although machine
learning methods have been applied for the investigation of suppression, these
models require large amounts of training data and computational power, both of
which pose challenges to resource-constrained DBS. This research proposes a
novel reinforcement learning (RL) framework for suppressing the synchronization
in neuronal activity during episodes of neurological disorders with less power
consumption. The proposed RL algorithm comprises an ensemble of a temporal
representation of stimuli and a twin-delayed deep deterministic (TD3) policy
gradient algorithm. We quantify the stability of the proposed framework to
noise and reduced synchrony using RL for three pathological signaling regimes:
regular, chaotic, and bursting, and further eliminate the undesirable
oscillations. Furthermore, metrics such as evaluation rewards, energy supplied
to the ensemble, and the mean point of convergence were used and compared to
other RL algorithms, specifically the Advantage actor critic (A2C), the Actor
critic with Kronecker-featured trust region (ACKTR), and the Proximal policy
optimization (PPO).
",0.15
"Cite_99_1: The expansion of renewable energy sources (RESs) in European Union countries has given rise to the development of Renewable Energy Communities (RECs), which are made up of locally generated energy by these RESs controlled by individuals, businesses, enterprises, and public administrations. There are several advantages for creating these RECs and participating in them, which include social, environmental, and financial. Nonetheless, according to the Renewable Energy Directive (RED II), the idea of RECs has given opportunities for researchers to investigate the behavior from all aspects. These RECs are characterized by energy fluxes corresponding to self-consumption, energy sales, and energy sharing. Our work focuses on a mathematical time-dependent model on an hourly basis that considers the optimization of photovoltaic-based RECs to maximize profit based on the number of prosumers and consumers, as well as the impact of load profiles on the community’s technical and financial aspects using MATLAB software. In this work, REC’s users can install their plant and become prosumers or vice versa, and users could change their consumption habits until the optimum configuration of REC is obtained. Moreover, this work also focuses on the financial analysis of the plant by comparing the Net Present Value (NPV) as a function of plant size, highlighting the advantage of creating a REC. Numerical results have been obtained investigating the case studies of RECs as per the Italian framework, which shows an optimal distribution of prosumers and consumers and an optimal load profile in which the maximum profitability is obtained. Optimization has been performed by considering different load profiles. Moreover, starting from the optimized configurations, an analysis based on the plant size is also made to maximize the NPV. This work has shown positive outcomes and would be helpful for the researchers and stakeholders while designing the RECs.","Main_99:   In smart energy communities, households of a particular geographical location
make a cooperative group to achieve the community's social welfare. Prosumers
are the users that both consume and produce energy. In this paper, we develop
stochastic and distributed algorithms to regulate the number of consumers and
the number of prosumers with heterogeneous energy sources in the smart energy
community. In the community, each prosumer has one of the heterogeneous energy
sources such as solar photovoltaic panels or wind turbines installed in their
household. The prosumers and consumers decide in a probabilistic way when to be
active. They keep their information private and do not need to share it with
other prosumers or consumers in the community. Moreover, we consider a central
server that keeps track of the total number of active prosumers and consumers
and sends feedback signals in the community at each time step; the prosumers
and consumers use these signals to calculate their probabilistic intent. We
present experimental results to check the efficacy of the algorithms. We
observe that the average number of times prosumers and consumers are active
reaches the optimal value over time, and the community asymptotically achieves
the social optimum value.
",0.2
"Cite_99_2: We develop an iterative differentially private algorithm for client selection in federated settings. We consider a federated network wherein clients coordinate with a central server to complete a task; however, the clients decide whether to participate or not at a time step based on their preferences—local computation and probabilistic intent. The algorithm does not require client-to-client information exchange. The developed algorithm provides near-optimal values to the clients over long-term average participation with a certain differential privacy guarantee. Finally, we present the experimental results to check the algorithm’s efficacy.","Main_99:   In smart energy communities, households of a particular geographical location
make a cooperative group to achieve the community's social welfare. Prosumers
are the users that both consume and produce energy. In this paper, we develop
stochastic and distributed algorithms to regulate the number of consumers and
the number of prosumers with heterogeneous energy sources in the smart energy
community. In the community, each prosumer has one of the heterogeneous energy
sources such as solar photovoltaic panels or wind turbines installed in their
household. The prosumers and consumers decide in a probabilistic way when to be
active. They keep their information private and do not need to share it with
other prosumers or consumers in the community. Moreover, we consider a central
server that keeps track of the total number of active prosumers and consumers
and sends feedback signals in the community at each time step; the prosumers
and consumers use these signals to calculate their probabilistic intent. We
present experimental results to check the efficacy of the algorithms. We
observe that the average number of times prosumers and consumers are active
reaches the optimal value over time, and the community asymptotically achieves
the social optimum value.
",0.2
"Cite_99_3: The transition away from fossil fuels towards a carbon-neutral, clean and circular economy is one of the greatest challenges of our time. Energy communities are one of the tools to re-structure our energy systems by harnessing the energy and allowing citizens to participate actively in the energy transition and thereby enjoy greater benefits. The definition of Renewable Energy Communities (RECs) given by the European Commission, places the REC as an association that produces and shares renewable energy, generating and managing cost-effective green energy autonomously, reducing CO2 emissions and energy waste. Observing this definition, the new Italian regulations concerning RECs boost distributed generation, encouraging the development of ‘zero-mile’ local energy production and smart grids. This research aims to evaluate the conditions to propose a REC in the Tor Sapienza district, as an Italian prototype, assessing the possibilities and advantages of transforming it into a large-scale sustainable infrastructure by means of a deep energy transition and the active role of local citizens, public administrations and small and medium-sized enterprises. Thanks to an in-depth technological, environmental and demographic survey of the neighbourhood, the study focuses on the retrofitting of a social housing complex in Tor Sapienza, as the main prosumer of the REC.","Main_99:   In smart energy communities, households of a particular geographical location
make a cooperative group to achieve the community's social welfare. Prosumers
are the users that both consume and produce energy. In this paper, we develop
stochastic and distributed algorithms to regulate the number of consumers and
the number of prosumers with heterogeneous energy sources in the smart energy
community. In the community, each prosumer has one of the heterogeneous energy
sources such as solar photovoltaic panels or wind turbines installed in their
household. The prosumers and consumers decide in a probabilistic way when to be
active. They keep their information private and do not need to share it with
other prosumers or consumers in the community. Moreover, we consider a central
server that keeps track of the total number of active prosumers and consumers
and sends feedback signals in the community at each time step; the prosumers
and consumers use these signals to calculate their probabilistic intent. We
present experimental results to check the efficacy of the algorithms. We
observe that the average number of times prosumers and consumers are active
reaches the optimal value over time, and the community asymptotically achieves
the social optimum value.
",0.2
"Cite_99_4: A federated multi-agent system is a multi-agent system wherein agents collaborate with a central server to optimize system goals without sharing their private information. We develop a communication-efficient solution to resource allocation problems for a population of agents coupled through multiple indivisible shared resources in a federated multi-agent system. The agents demand resources in a probabilistic way based on their local computation and preferences, and the agents receive either one unit of a resource or do not receive it. The agents are not required to share their cost functions or derivatives of cost functions with other agents or the central server. Optimal control of a population of such agents, subject to capacity constraints, is widely found in many application domains, such as smart energy systems, intelligent transportation systems, and edge computing, to name a few. We present convergence results using multi-time scale stochastic approximation techniques and an example of electric vehicle charging point allocation illustrating the efficacy of the developed solution.","Main_99:   In smart energy communities, households of a particular geographical location
make a cooperative group to achieve the community's social welfare. Prosumers
are the users that both consume and produce energy. In this paper, we develop
stochastic and distributed algorithms to regulate the number of consumers and
the number of prosumers with heterogeneous energy sources in the smart energy
community. In the community, each prosumer has one of the heterogeneous energy
sources such as solar photovoltaic panels or wind turbines installed in their
household. The prosumers and consumers decide in a probabilistic way when to be
active. They keep their information private and do not need to share it with
other prosumers or consumers in the community. Moreover, we consider a central
server that keeps track of the total number of active prosumers and consumers
and sends feedback signals in the community at each time step; the prosumers
and consumers use these signals to calculate their probabilistic intent. We
present experimental results to check the efficacy of the algorithms. We
observe that the average number of times prosumers and consumers are active
reaches the optimal value over time, and the community asymptotically achieves
the social optimum value.
",0.2
"Cite_101_1: Let G=(V,E) be a simple graph. A set D subseteq V is a strong dominating set of G, if for every vertex x in V setminus D there is a vertex y in D with xy in E(G) and deg(x)leq deg(y). The strong domination number gamma_{st}(G) is defined as the minimum cardinality of a strong dominating set. In this paper, we study the strong domination number of Hajós sum and vertex-sum of two graphs.","Main_101:   A vertex coloring of a graph $G$ is distinguishing if non-identity
automorphisms do not preserve it. The distinguishing number, $D(G)$, is the
minimum number of colors required for such a coloring and the distinguishing
threshold, $\theta(G)$, is the minimum number of colors~$k$ such that any
arbitrary $k$-coloring is distinguishing. Moreover, $\Phi_k (G)$ is the number
of distinguishing coloring of $G$ using at most $k$ colors. In this paper, for
some graph operations, namely, vertex-sum, rooted product, corona product and
lexicographic product, we find formulae of the distinguishing number and
threshold using $\Phi_k (G)$.
",0.1
"Cite_101_2: A vertex coloring is called distinguishing if the identity is the only automorphism that can preserve it. The distinguishing threshold θ(G) of a graph G is the minimum number of colors k required that any arbitrary k-coloring of G is distinguishing. In this paper, we calculate the distinguishing threshold of a Cartesian product graph. Moreover, we calculate the number of non-equivalent distinguishing colorings of grids.","Main_101:   A vertex coloring of a graph $G$ is distinguishing if non-identity
automorphisms do not preserve it. The distinguishing number, $D(G)$, is the
minimum number of colors required for such a coloring and the distinguishing
threshold, $\theta(G)$, is the minimum number of colors~$k$ such that any
arbitrary $k$-coloring is distinguishing. Moreover, $\Phi_k (G)$ is the number
of distinguishing coloring of $G$ using at most $k$ colors. In this paper, for
some graph operations, namely, vertex-sum, rooted product, corona product and
lexicographic product, we find formulae of the distinguishing number and
threshold using $\Phi_k (G)$.
",0.1
